Deep Learning for Computer Vision with
Python
Practitioner Bundle
Dr. Adrian Rosebrock
1st Edition (1.1.0)Copyright c 2017 Adrian Rosebrock, PyImageSearch.com
P UBLISHED BY P Y I MAGE S EARCH
PYIMAGESEARCH . COM
The contents of this book, unless otherwise indicated, are Copyright c 2017 Adrian Rosebrock,
PyimageSearch.com. All rights reserved. Books like this are made possible by the time invested by
the authors. If you received this book and did not purchase it, please consider making future books
possible by buying a copy at https://www.pyimagesearch.com/deep-learning-computer-vision-
python-book/ today.
First printing, September 2017To my father, Joe; my wife, Trisha;
and the family beagles, Josie and Jemma.
Without their constant love and support,
this book would not be possible.Contents
1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2 Data Augmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.1 What Is Data Augmentation? 13
2.2 Visualizing Data Augmentation 14
2.3 Comparing Training With and Without Data Augmentation 17
2.3.1
2.3.2
2.3.3
2.3.4 The Flowers-17 Dataset . . . . . . . . . . .
Aspect-aware Preprocessing . . . . . . .
Flowers-17: No Data Augmentation .
Flowers-17: With Data Augmentation 2.4 Summary 3 Networks as Feature Extractors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
3.1 Extracting Features with a Pre-trained CNN 3.1.1
3.1.2 What Is HDF5? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
Writing Features to an HDF5 Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
3.2 The Feature Extraction Process 3.2.1
3.2.2
3.2.3 Extracting Features From Animals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
Extracting Features From CALTECH-101 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
Extracting Features From Flowers-17 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
3.3 Training a Classifier on Extracted Features 3.3.1
3.3.2
3.3.3 Results on Animals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
Results on CALTECH-101 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
Results on Flowers-17 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
3.4 Summary
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
17
18
21
25
29
32
37
43
464 Understanding rank-1 & rank-5 Accuracies . . . . . . . . . . . . . . . . . . . . 49
4.1 Ranked Accuracy
4.1.1
4.1.2
4.1.3
4.1.4 Measuring rank-1 and rank-5 Accuracies
Implementing Ranked Accuracy . . . . . .
Ranked Accuracy on Flowers-17 . . . . . . .
Ranked Accuracy on CALTECH-101 . . . .
4.2 Summary
5 Fine-tuning Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
5.1 Transfer Learning and Fine-tuning
5.1.1
5.1.2
5.1.3 Indexes and Layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
Network Surgery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
Fine-tuning, from Start to Finish . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
5.2 Summary
6 Improving Accuracy with Network Ensembles . . . . . . . . . . . . . . . . . . 71
6.1 Ensemble Methods
6.1.1
6.1.2
6.1.3 Jensen’s Inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
Constructing an Ensemble of CNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
Evaluating an Ensemble . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
6.2 Summary
7 Advanced Optimization Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
7.1 Adaptive Learning Rate Methods
7.1.1
7.1.2
7.1.3
7.1.4
7.1.5 Adagrad
Adadelta
RMSprop
Adam . . .
Nadam .
7.2 Choosing an Optimization Method
7.2.1 Three Methods You Should Learn how to Drive: SGD, Adam, and RMSprop . . 86
7.3 Summary
8 Optimal Pathway to Apply Deep Learning . . . . . . . . . . . . . . . . . . . . . 89
8.1 A Recipe for Training
89
8.2 Transfer Learning or Train from Scratch
93
8.3 Summary
94
9 Working with HDF5 and Large Datasets . . . . . . . . . . . . . . . . . . . . . . . . . 95
9.1 Downloading Kaggle: Dogs vs. Cats
95
9.2 Creating a Configuration File
96
9.2.1 Your First Configuration File . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
9.3 Building the Dataset
9.4 Summary
49
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
51
52
54
54
54
57
69
71
80
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
83
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
84
84
85
85
86
86
87
98
10210 Competing in Kaggle: Dogs vs. Cats . . . . . . . . . . . . . . . . . . . . . . . . . . 103
10.1 Additional Image Preprocessors
103
10.1.1 Mean Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
10.1.2 Patch Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
10.1.3 Crop Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
10.2 HDF5 Dataset Generators 109
10.3 Implementing AlexNet 112
10.4 Training AlexNet on Kaggle: Dogs vs. Cats 117
10.5 Evaluating AlexNet 120
10.6 Obtaining a Top-5 Spot on the Kaggle Leaderboard 123
10.6.1 Extracting Features Using ResNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
10.6.2 Training a Logistic Regression Classifier . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
10.7 Summary
128
11 GoogLeNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
11.1 The Inception Module (and its Variants)
132
11.1.1 Inception . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
11.1.2 Miniception . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
11.2 MiniGoogLeNet on CIFAR-10
134
11.2.1
11.2.2
11.2.3
11.2.4
11.2.5 Implementing MiniGoogLeNet . . . . . . . . . . . . . . . . .
Training and Evaluating MiniGoogLeNet on CIFAR-10
MiniGoogLeNet: Experiment #1 . . . . . . . . . . . . . . . . .
MiniGoogLeNet: Experiment #2 . . . . . . . . . . . . . . . . .
MiniGoogLeNet: Experiment #3 . . . . . . . . . . . . . . . . .
11.3 The Tiny ImageNet Challenge
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
135
140
143
144
145
146
11.3.1 Downloading Tiny ImageNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
11.3.2 The Tiny ImageNet Directory Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
11.3.3 Building the Tiny ImageNet Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
11.4 DeeperGoogLeNet on Tiny ImageNet
11.4.1
11.4.2
11.4.3
11.4.4
11.4.5 Implementing DeeperGoogLeNet . . . . . . . . .
Training DeeperGoogLeNet on Tiny ImageNet
Creating the Training Script . . . . . . . . . . . . . . .
Creating the Evaluation Script . . . . . . . . . . . . .
DeeperGoogLeNet Experiments . . . . . . . . . . .
153
11.5 Summary
12 ResNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
12.1 ResNet and the Residual Module
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
153
161
161
163
165
168
171
12.1.1 Going Deeper: Residual Modules and Bottlenecks . . . . . . . . . . . . . . . . . . . . . 172
12.1.2 Rethinking the Residual Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
12.2 Implementing ResNet 175
12.3 ResNet on CIFAR-10 180
12.3.1 Training ResNet on CIFAR-10 With the ctrl + c Method . . . . . . . . . . . . . . . . . . 181
12.3.2 ResNet on CIFAR-10: Experiment #2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18512.4 Training ResNet on CIFAR-10 with Learning Rate Decay 188
12.5 ResNet on Tiny ImageNet 192
12.5.1 Updating the ResNet Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
12.5.2 Training ResNet on Tiny ImageNet With the ctrl + c Method . . . . . . . . . . . . . . 194
12.5.3 Training ResNet on Tiny ImageNet with Learning Rate Decay . . . . . . . . . . . . . 198
12.6
Summary
202Companion Website
Thank you for picking up a copy of Deep Learning for Computer Vision with Python! To accompany
this book I have created a companion website which includes:
• Up-to-date installation instructions on how to configure your development environment
• Instructions on how to use the pre-configured Ubuntu VirtualBox virtual machine and
Amazon Machine Image (AMI)
• Supplementary material that I could not fit inside this book
Additionally, you can use the “Issues” feature inside the companion website to report any bugs,
typos, or problems you encounter when working through the book. I don’t expect many problems;
however, this is a brand new book so myself and other readers would appreciate reporting any
issues you run into. From there, I can keep the book updated and bug free.
To create your companion website account, just use this link:
http://pyimg.co/fnkxk
Take a second to create your account now so you’ll have access to the supplementary materials
as you work through the book.1. Introduction
Welcome to the Practitioner Bundle of Deep Learning for Computer Vision with Python! This
volume is meant to be the next logical step in your deep learning for computer vision education
after completing the Starter Bundle.
At this point, you should have a strong understanding of the fundamentals of parameterized
learning, neural networks, and Convolutional Neural Networks (CNNs). You should also feel
relatively comfortable using the Keras library and the Python programming language to train your
own custom deep learning networks.
The purpose of the Practitioner Bundle is to build on your knowledge gained from the Starter
Bundle and introduce more advanced algorithms, concepts, and tricks of the trade — these tech-
niques will be covered in three distinct parts of the book.
The first part will focus on methods that are used to boost your classification accuracy in one
way or another. One way to increase your classification accuracy is to apply transfer learning
methods such as fine-tuning or treating your network as a feature extractor.
We’ll also explore ensemble methods (i.e., training multiple networks and combining the
results) and how these methods can give you a nice classification boost with little extra effort.
Regularization methods such as data augmentation are used to generate additional training data
– in nearly all situations, data augmentation improves your model’s ability to generalize. More
advanced optimization algorithms such as Adam [1], RMSprop [2], and others can also be used on
some datasets to help you obtain lower loss. After we review these techniques, we’ll look at the
optimal pathway to apply these methods to ensure you obtain the maximum amount of benefit with
the least amount of effort.
We then move on to the second part of the Practitioner Bundle which focuses on larger
datasets and more exotic network architectures. Thus far we have only worked with datasets that
have fit into the main memory of our system – but what if our dataset is too large to fit into RAM?
What do we do then? We’ll address this question in Chapter 9 when we work with HDF5.
Given that we’ll be working with larger datasets, we’ll also be able to discuss more advanced
network architectures using AlexNet, GoogLeNet, ResNet, and deeper variants of VGGNet. These
network architectures will be applied to more challenging datasets and competitions, including the12
Chapter 1. Introduction
Kaggle: Dogs vs. Cats recognition challenge [3] as well as the cs231n Tiny ImageNet challenge
[4], the exact same task Stanford CNN students compete in. As we’ll find out, we’ll be able to
obtain a top-25 position on the Kaggle Dogs vs. Cats leaderboard and top the cs231n challenge for
our technique type.
The final part of this book covers applications of deep learning for computer vision outside of
image classification, including basic object detection, deep dreaming and neural style, Generative
Adversarial Networks (GANs), and Image Super Resolution. Again, the techniques covered in this
volume are meant to be much more advanced than the Starter Bundle – this is where you’ll start to
separate yourself from a deep learning novice and transform into a true deep learning practitioner.
To start your transformation to deep learning expert, just flip the page.2. Data Augmentation
According to Goodfellow et al., regularization is “any modification we make to a learning algo-
rithm that is intended to reduce its generalization error, but not its training error” [5]. In short,
regularization seeks to reduce our testing error perhaps at the expense of increasing training error
slightly.
We’ve already looked at different forms of regularization in Chapter 9 of the Starter Bundle;
however, these were parameterized forms of regularization, requiring us to update our loss/update
function. In fact, there exist other types of regularization that either:
1. Modify the network architecture itself.
2. Augment the data passed into the network for training.
Dropout is a great example of modifying a network architecture by achieving greater general-
izability. Here we insert a layer that randomly disconnects nodes from the previous layer to the
next layer, thereby ensuring that no single node is responsible for learning how to represent a given
class.
In the remainder of this chapter, we’ll be discussing another type of regularization called
data augmentation. This method purposely perturbs training examples, changing their appearance
slightly, before passing them into the network for training. The end result is that a network
consistently sees “new” training data points generated from the original training data, partially
alleviating the need for us to gather more training data (though in general, gathering more training
data will rarely hurt your algorithm).
2.1
What Is Data Augmentation?
Data augmentation encompasses a wide range of techniques used to generate new training samples
from the original ones by applying random jitters and perturbations such that the classes labels are
not changed. Our goal when applying data augmentation is to increase the generalizability of the
model. Given that our network is constantly seeing new, slightly modified versions of the input data
points, it’s able to learn more robust features. At testing time, we do not apply data augmentation
and evaluate our trained network – in most cases, you’ll see an increase in testing accuracy, perhaps
at the expense at a slight dip in training accuracy.14
Chapter 2. Data Augmentation
Figure 2.1: Left: A sample of 250 data points that follow a normal distribution exactly. Right:
Adding a small amount of random “jitter” to the distribution. This type of data augmentation can
increase the generalizability of our networks.
Let’s consider the Figure 2.1 (left) of a normal distribution with zero mean and unit variance.
Training a machine learning model on this data may result in us modeling the distribution exactly –
however, in real-world applications, data rarely follows such a neat distribution.
Instead, to increase the generalizability of our classifier, we may first randomly jitter points
along the distribution by adding some values ε drawn from a random distribution (right). Our plot
still follows an approximately normal distribution, but it’s not a perfect distribution as on the left. A
model trained on this data is more likely to generalize to example data points not included in the
training set.
In the context of computer vision, data augmentation lends itself naturally. For example, we
can obtain additional training data from the original images by apply simple geometric transforms
such as random:
1. Translations
2. Rotations
3. Changes in scale
4. Shearing
5. Horizontal (and in some cases, vertical) flips
Applying a (small) amount of these transformations to an input image will change its appearance
slightly, but it does not change the class label – thereby making data augmentation a very natural,
easy method to apply to deep learning for computer vision tasks. More advanced techniques for
data augmentation applied to computer vision include random perturbation of colors in a given
color space [6] and nonlinear geometric distortions [7].
2.2
Visualizing Data Augmentation
The best way to understand data augmentation applied to computer tasks is to simply visualize a
given input being augmented and distorted. To accomplish this visualization, let’s build a simple
Python script that uses the built-in power of Keras to perform data augmentation. Create a new file,
name it augmentation_demo.py. and insert the following code:2.2 Visualizing Data Augmentation
1
2
3
4
5
6
15
# import the necessary packages
from keras.preprocessing.image import ImageDataGenerator
from keras.preprocessing.image import img_to_array
from keras.preprocessing.image import load_img
import numpy as np
import argparse
Lines 2-6 import our required Python packages. Take note of Line 2 where we import the
ImageDataGenerator class from Keras – this code will be used for data augmentation and includes
all relevant methods to help us transform our input image.
Next, we parse our command line arguments:
8
9
10
11
12
13
14
15
16
# construct the argument parse and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-i", "--image", required=True,
help="path to the input image")
ap.add_argument("-o", "--output", required=True,
help="path to output directory to store augmentation examples")
ap.add_argument("-p", "--prefix", type=str, default="image",
help="output filename prefix")
args = vars(ap.parse_args())
Our script requires three command line arguments, each detailed below:
• --image: This is the path to the input image that we want to apply data augmentation to and
visualize the results.
• --output: After applying data augmentation to a given image, we would like to store the
result on disk so we can inspect it – this switch controls the output directory.
• --prefix: A string that will be prepended to the output image filename.
Now that our command line arguments are parsed, let’s load our input image, convert it to a
Keras-compatible array, and add an extra dimension to the image, just as we would do if we were
preparing our image for classification:
18
19
20
21
22
23
# load the input image, convert it to a NumPy array, and then
# reshape it to have an extra dimension
print("[INFO] loading example image...")
image = load_img(args["image"])
image = img_to_array(image)
image = np.expand_dims(image, axis=0)
We are now ready to initialize our ImageDataGenerator:
25
26
27
28
29
30
# construct the image generator for data augmentation then
# initialize the total number of images generated thus far
aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,
height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,
horizontal_flip=True, fill_mode="nearest")
total = 0
The ImageDataGenerator class has a number of parameters, too many to enumerate in
this book. For a full review of the parameters, please refer to the official Keras documentation
(http://pyimg.co/j8ad8).Chapter 2. Data Augmentation
16
Instead, we’ll be focusing on the augmentation parameters you will most likely use in your
own applications. The rotation_range parameter controls the degree range of the random
rotations. Here we’ll allow our input image to be randomly rotated ±30 degrees. Both the
width_shift_range and height_shift_range are used for horizontal and vertical shifts, re-
spectively. The parameter value is a fraction of the given dimension, in this case, 10%.
The shear_range controls the angle in counterclockwise direction as radians in which our
image will allowed to be sheared. We then have the zoom_range, a floating point value that allows
the image to be “zoomed in” or “zoomed out” according to the following uniform distribution of
values: [1 - zoom_range, 1 + zoom_range].
Finally, the horizontal_flip boolean controls whether or not a given input is allowed to be
flipped horizontally during the training process. For most computer vision applications a horizontal
flip of an image does not change the resulting class label – but there are applications where a
horizontal (or vertical) flip does change the semantic meaning of the image. Take care when
applying this type of data augmentation as our goal is to slightly modify the input image, thereby
generating a new training sample, without changing the class label itself. For a more detailed review
of image transformations, please refer to Module #1 in PyImageSearch Gurus ([8], PyImageSearch
Gurus) as well as Szeliski [9].
Once ImageDataGenerator is initialized, we can actually generate new training examples:
32
33
34
35
# construct the actual Python generator
print("[INFO] generating images...")
imageGen = aug.flow(image, batch_size=1, save_to_dir=args["output"],
save_prefix=args["prefix"], save_format="jpg")
36
37
38
39
40
# loop over examples from our image data augmentation generator
for image in imageGen:
# increment our counter
total += 1
41
42
43
44
# if we have reached 10 examples, break from the loop
if total == 10:
break
Lines 34 and 35 initialize a Python generator used to construct our augmented images. We’ll
pass in our input image, a batch_size of 1 (since we are only augmenting one image), along with
a few additional parameters to specify the output image file paths, the prefix for each file path,
and the image file format. Line 38 then starts looping over each image in the imageGen generator.
Internally, imageGen is automatically generating a new training sample each time one is requested
via the loop. We then increment the total number of data augmentation examples written to disk
and stop the script from executing once we’ve reached ten examples.
To visualize data augmentation in action, we’ll be using Figure 2.2 (left), an image of Jemma,
my family beagle. To generate new training example images of Jemma, just execute the following
command:
$ python augmentation_demo.py --image jemma.png --output output
After the script executes you should see ten images in the output directory:
$ ls output/
image_0_1227.jpg
image_0_2358.jpg
image_0_4205.jpg
image_0_4770.jpg2.3 Comparing Training With and Without Data Augmentation
17
Figure 2.2: Left: The input image we are going to apply data augmentation to. Right: A montage
of data augmentation examples. Notice how each image has been randomly rotated, sheared,
zoomed, and horizontally flipped.
image_0_1933.jpg
image_0_9197.jpg
image_0_2914.jpg
image_0_953.jpg
image_0_4657.jpg
image_0_6934.jpg
I have constructed a montage of each of these images so you can visualize them in Figure 2.2
(right). Notice how each image has been randomly rotated, sheared, zoomed, and horizontally
flipped. In each case the image retains the original class label: dog; however, each image has been
modified slightly, thereby giving our neural network new patterns to learn from when training. Since
the input images will constantly be changing (while the class labels remain the same), it’s common
to see our training accuracy decrease when compared to training without data augmentation.
However, as we’ll find out later in this chapter, data augmentation can help dramatically reduce
overfitting, all the while ensuring that our model generalizes better to new input samples. Further-
more, when working with datasets where we have too few examples to apply deep learning, we
can utilize data augmentation to generate additional training data, thereby reducing the amount of
hand-labeled data required to train a deep learning network.
2.3
Comparing Training With and Without Data Augmentation
In the first part of this section, we’ll discuss the Flowers-17 dataset, a very small dataset (in terms
of deep learning for computer vision tasks), and how data augmentation can help us artificially
increase the size of this dataset by generating additional training samples. From there we’ll perform
two experiments:
1. Train MiniVGGNet on Flowers-17 without data augmentation.
2. Train MiniVGGNet on Flowers-17 with data augmentation.
As we’ll find out, applying data augmentation dramatically reduces overfitting and allows
MiniVGGNet to obtain substantially higher classification accuracy.
2.3.1
The Flowers-17 Dataset
The Flowers-17 dataset [10] is a fine-grained classification challenge where our task is to recognize
17 distinct species of flowers. The image dataset is quite small, having only 80 images per class for
a total of 1,360 images. A general rule of thumb when applying deep learning to computer vision
tasks is to have 1,000-5,000 examples per class, so we are certainly at a huge deficit here.
We call the Flowers-17 a fine-grained classification task because all categories are very similar
(i.e., species of flower). In fact, we can think of each of these categories as subcategories. The
categories are certainly different, but share a significant amount of common structure (e.x., petals,Chapter 2. Data Augmentation
18
Figure 2.3: A sample of five (out of the seventeen total) classes in the Flowers-17 dataset where
each class represents a specific flower species.
stamen, pistil, etc.). Fine-grained classification tasks tend to be the most challenging for deep
learning practitioners as it implies that our machine learning models need to learn extremely discrim-
inating features to distinguish between classes that are very similar. This fine-grained classification
task becomes even more problematic given our limited training data.
2.3.2
Aspect-aware Preprocessing
Up until this point, we have only preprocessed images by resizing them to a fixed size, ignoring the
aspect ratio. In some situations, especially for basic benchmark datasets, doing so is acceptable.
However, for more challenging datasets we should still seek to resize to a fixed size, but
maintain the aspect ratio. To visualize this action, consider Figure 2.4.
On the left, we have an input image that we need to resize to a fixed width and height. Ignoring
the aspect ratio, we resize the image to 256 × 256 pixels (middle), effectively squishing and
distorting the image such that it meets our desired dimensions. A better approach would be to
take into account the aspect ratio of the image (right) where we first resize along the shorter
dimension such that the width is 256 pixels and then crop the image along the height, such that the
height is 256 pixels.
While we have effectively discarded part of the image during the crop, we have also maintained
the original aspect ratio of the image. Maintaining a consistent aspect ratio allows our Convolutional
Neural Network to learn more discriminative, consistent features. This is a common technique that
we’ll be applying when working with more advanced datasets throughout the rest of the Practitioner
Bundle and ImageNet Bundle.
To see how aspect-aware preprocessing is implemented, let’s update our pyimagesearch
project structure to include a AspectAwarePreprocessor:
--- pyimagesearch
|
|--- __init__.py
|
|--- callbacks
|
|--- nn
|
|--- preprocessing
|
|
|--- __init__.py
|
|
|--- aspectawarepreprocessor.py2.3 Comparing Training With and Without Data Augmentation
19
Figure 2.4: Left: The original input image (410 × 310). Middle: Resizing the image to 256 × 256
pixels, ignoring the aspect ratio. Notice how the image now appears squished and distorted. Right:
Resizing the image to 256 × 256 while maintaining the aspect ratio.
|
|
|
|
|--- imagetoarraypreprocessor.py
|
|--- simplepreprocessor.py
|--- utils
Notice how we have added a new file named aspectawarepreprocessor.py inside the
preprocessing sub-module – this location is where our new preprocessor will be live. Open up
aspectawarepreprocessor.py and insert the following code:
1
2
3
# import the necessary packages
import imutils
import cv2
4
5
6
7
8
9
10
11
class AspectAwarePreprocessor:
def __init__(self, width, height, inter=cv2.INTER_AREA):
# store the target image width, height, and interpolation
# method used when resizing
self.width = width
self.height = height
self.inter = inter
Just as in our SimplePreprocessor, our constructor requires two parameters (the desired
width and height of the target output image) along with the interpolation method used when
resizing the image. We can then define the preprocess function below:
13
14
15
16
17
18
def preprocess(self, image):
# grab the dimensions of the image and then initialize
# the deltas to use when cropping
(h, w) = image.shape[:2]
dW = 0
dH = 020
Chapter 2. Data Augmentation
The preprocess function accepts a single argument, the image that we wish to preprocess.
Line 16 grabs the width and height of the input image, while Lines 17 and 18 determine the
delta offsets we’ll be using when cropping along the larger dimension. Again, our aspect-aware
preprocessor is a two step algorithm:
1. Step #1: Determine the shortest dimension and resize along it.
2. Step #2: Crop the image along the largest dimension to obtain the target width and height.
The following code block handles checking if the width is smaller than the height, and if so,
resizes along the width:
20
21
22
23
24
25
26
27
# if the width is smaller than the height, then resize
# along the width (i.e., the smaller dimension) and then
# update the deltas to crop the height to the desired
# dimension
if w < h:
image = imutils.resize(image, width=self.width,
inter=self.inter)
dH = int((image.shape[0] - self.height) / 2.0)
Otherwise, if the height is smaller than the width, then we resize along the height:
29
30
31
32
33
34
35
# otherwise, the height is smaller than the width so
# resize along the height and then update the deltas
# to crop along the width
else:
image = imutils.resize(image, height=self.height,
inter=self.inter)
dW = int((image.shape[1] - self.width) / 2.0)
Now that our image is resized, we need to re-grab the width and height and use the deltas to
crop the center of the image:
37
38
39
40
41
# now that our images have been resized, we need to
# re-grab the width and height, followed by performing
# the crop
(h, w) = image.shape[:2]
image = image[dH:h - dH, dW:w - dW]
42
43
44
45
46
47
# finally, resize the image to the provided spatial
# dimensions to ensure our output image is always a fixed
# size
return cv2.resize(image, (self.width, self.height),
interpolation=self.inter)
When cropping (due to rounding errors), our image target image dimensions may be off by ±
one pixel; therefore, we make a call to cv2.resize to ensure our output image has the desired
width and height. The preprocessed image is then returned to the calling function. Now that we’ve
implemented our AspectAwarePreprocessor, let’s put it to work when training the MiniVGGNet
architecture on the Flowers-17 dataset.2.3 Comparing Training With and Without Data Augmentation
2.3.3
21
Flowers-17: No Data Augmentation
To start, let’s establish a baseline using no data augmentation when training the MiniVGGNet
architecture (Chapter 15, Starter Bundle) on the Flowers-17 dataset. Open up a new file, name it
minivggnet_flowers17.py, and we’ll get to work:
1
2
3
4
5
6
7
8
9
10
11
12
13
14
# import the necessary packages
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from pyimagesearch.preprocessing import ImageToArrayPreprocessor
from pyimagesearch.preprocessing import AspectAwarePreprocessor
from pyimagesearch.datasets import SimpleDatasetLoader
from pyimagesearch.nn.conv import MiniVGGNet
from keras.optimizers import SGD
from imutils import paths
import matplotlib.pyplot as plt
import numpy as np
import argparse
import os
Lines 2-14 import our required Python packages. Most of these imports you’ve seen before,
but I want to draw your attention to:
1. Line 6: Here we import our newly defined AspectAwarePreprocessor.
2. Line 7: Despite using a separate image preprocessor, we’ll still be able to use SimpleDatasetLoader
to load our dataset from disk.
3. Line 8: We’ll be training the MiniVGGNet architecture on our dataset.
Next, we parse our command line arguments:
16
17
18
19
20
# construct the argument parse and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-d", "--dataset", required=True,
help="path to input dataset")
args = vars(ap.parse_args())
We only need a single switch here, --dataset, which is the path to our Flowers-17 dataset
directory on disk.
Let’s go ahead and extract the class labels from our input images:
22
23
24
25
26
27
# grab the list of images that we’ll be describing, then extract
# the class label names from the image paths
print("[INFO] loading images...")
imagePaths = list(paths.list_images(args["dataset"]))
classNames = [pt.split(os.path.sep)[-2] for pt in imagePaths]
classNames = [str(x) for x in np.unique(classNames)]
Our Flowers-17 dataset has the following directory structure:
flowers17/{species}/{image}
An example of an image in the dataset follows:Chapter 2. Data Augmentation
22
flowers17/bluebell/image_0241.jpg
Therefore, to extract the class labels, we can simply extract the second to last index after
splitting on the path separator (Line 26) yielding the text bluebell. If you struggle to see how
this path and label extraction works, I would suggest opening a Python shell and playing around
with file paths and path separators. In particular, notice how you can split a string based on the
operating system’s path separator and then use Python indexing to extract various parts of the array.
Line 27 then determines the unique set of class labels (in this case, 17 total classes) from the image
paths.
Given our imagePaths, we can load the Flowers-17 dataset from disk:
29
30
31
# initialize the image preprocessors
aap = AspectAwarePreprocessor(64, 64)
iap = ImageToArrayPreprocessor()
32
33
34
35
36
37
# load the dataset from disk then scale the raw pixel intensities
# to the range [0, 1]
sdl = SimpleDatasetLoader(preprocessors=[aap, iap])
(data, labels) = sdl.load(imagePaths, verbose=500)
data = data.astype("float") / 255.0
Line 30 initializes our AspectAwarePreprocessor such that every image it processes will be
64 × 64 pixels. The ImageToArrayPreprocessor is then initialized on Line 31, allowing us to
convert images to Keras-compatible arrays. We then instantiate the SimpleDatasetLoader using
these two preprocessors, respectively (Line 35).
The data and corresponding labels are loaded from disk on Line 36. All images in the data
array are then normalized to the range [0, 1] by dividing the raw pixel intensities by 255.
Now that our data is loaded we can perform a training and testing split (75 percent for training,
25 percent for testing) along with one-hot encoding our labels:
39
40
41
42
# partition the data into training and testing splits using 75% of
# the data for training and the remaining 25% for testing
(trainX, testX, trainY, testY) = train_test_split(data, labels,
test_size=0.25, random_state=42)
43
44
45
46
# convert the labels from integers to vectors
trainY = LabelBinarizer().fit_transform(trainY)
testY = LabelBinarizer().fit_transform(testY)
To train our flower classifier we’ll be using the MiniVGGNet architecture along with the SGD
optimizer:
48
49
50
51
52
53
54
# initialize the optimizer and model
print("[INFO] compiling model...")
opt = SGD(lr=0.05)
model = MiniVGGNet.build(width=64, height=64, depth=3,
classes=len(classNames))
model.compile(loss="categorical_crossentropy", optimizer=opt,
metrics=["accuracy"])2.3 Comparing Training With and Without Data Augmentation
23
55
56
57
58
59
# train the network
print("[INFO] training network...")
H = model.fit(trainX, trainY, validation_data=(testX, testY),
batch_size=32, epochs=100, verbose=1)
The MiniVGGNet architecture will accept images with spatial dimensions 64 × 64 × 3 (64
pixels wide, 64 pixels tall, and 3 channels). The total number of classes is len(classNames)
which, in this case, equals seventeen, one for each of the categories in the Flowers-17 dataset.
We’ll train MiniVGGNet using SGD with an initial learning rate of α = 0.05. We’ll purposely
leave out learning rate decay so we can demonstrate the affect data augmentation has in the next
section. Lines 58 and 59 train MiniVGGNet for a total of 100 epochs.
We then evaluate our network and plot our loss and accuracy over time:
61
62
63
64
65
# evaluate the network
print("[INFO] evaluating network...")
predictions = model.predict(testX, batch_size=32)
print(classification_report(testY.argmax(axis=1),
predictions.argmax(axis=1), target_names=classNames))
66
67
68
69
70
71
72
73
74
75
76
77
78
# plot the training loss and accuracy
plt.style.use("ggplot")
plt.figure()
plt.plot(np.arange(0, 100), H.history["loss"], label="train_loss")
plt.plot(np.arange(0, 100), H.history["val_loss"], label="val_loss")
plt.plot(np.arange(0, 100), H.history["acc"], label="train_acc")
plt.plot(np.arange(0, 100), H.history["val_acc"], label="val_acc")
plt.title("Training Loss and Accuracy")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend()
plt.show()
To obtain a baseline accuracy on Flowers-17 using MiniVGGNet, just execute the following
command:
$ python minivggnet_flowers17.py --dataset ../flowers17/images
[INFO] loading images...
[INFO] processed 500/1360
[INFO] processed 1000/1360
[INFO] compiling model...
[INFO] training network...
Train on 1020 samples, validate on 340 samples
Epoch 1/100
...
Epoch 100/100
3s - loss: 0.0030 - acc: 1.0000 - val_loss: 1.7683 - val_acc: 0.6206
[INFO] evaluating network...
precision
recall f1-score
support
bluebell
buttercup
coltsfoot
0.48
0.67
0.53
0.67
0.60
0.40
0.56
0.63
0.46
18
20
20Chapter 2. Data Augmentation
24
cowslip
crocus
daffodil
daisy
dandelion
fritillary
iris
lilyvalley
pansy
snowdrop
sunflower
tigerlily
tulip
windflower 0.35
0.62
0.43
0.74
0.61
0.72
0.80
0.59
0.82
0.64
0.95
0.88
0.18
0.67 0.44
0.50
0.33
0.85
0.83
0.82
0.76
0.67
0.74
0.39
0.91
0.74
0.25
0.62 0.39
0.56
0.38
0.79
0.70
0.77
0.78
0.62
0.78
0.49
0.93
0.80
0.21
0.65 18
20
27
20
23
22
21
15
19
23
23
19
16
16
avg / total 0.64 0.62 0.62 340
As we can see from the output, we are able to obtain 64 percent classification accuracy, which is
fairly reasonable given our limited amount of training data. However, what is concerning is our loss
and accuracy plot (Figure (2.5). As the plot demonstrates, our network quickly starts overfitting past
epoch 20. The reason for this behavior is because we only have 1,020 training examples with 60
images per class (the other images are used for testing). Keep in mind that we should ideally have
anywhere between 1,000-5,000 examples per class when training a Convolutional Neural Network.
Figure 2.5: Learning plot for MiniVGGNet applied to the Flowers-17 dataset without data augmen-
tation. Notice how overfitting starts to occur past epoch 25 as our validation loss increases.
Furthermore, training accuracy skyrockets past 95% in the first few epochs, eventually obtaining
100% accuracy in the later epochs – this output is a clear case of overfitting. Due to the lack of
substantial training data, MiniVGGNet is modeling the underlying patterns in the training data2.3 Comparing Training With and Without Data Augmentation
25
too closely and is unable to generalize to the test data. To combat the overfitting, we can apply
regularization techniques – in the context of this chapter, our regularization method will be data
augmentation. In practice, you would also include other forms of regularization (weight decay,
dropout, etc.) to further reduce the effects of overfitting.
2.3.4
Flowers-17: With Data Augmentation
In this example we are going to apply the example same training process as the previous section
only with one addition: we’ll be applying data augmentation. To see how data augmentation
can increase our classification accuracy while preventing overfitting, open up a new file, name it
minivggnet_flowers17_data_aug.py, and let’s get to work:
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
# import the necessary packages
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from pyimagesearch.preprocessing import ImageToArrayPreprocessor
from pyimagesearch.preprocessing import AspectAwarePreprocessor
from pyimagesearch.datasets import SimpleDatasetLoader
from pyimagesearch.nn.conv import MiniVGGNet
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import SGD
from imutils import paths
import matplotlib.pyplot as plt
import numpy as np
import argparse
import os
Our imports are the same as in minivggnet_flowers17.py, with the exception of Line
9 where we import the ImageDataGenerator class used for data augmentation.
Next, let’s parse our command line arguments and extract the class names from the image paths:
17
18
19
20
21
# construct the argument parse and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-d", "--dataset", required=True,
help="path to input dataset")
args = vars(ap.parse_args())
22
23
24
25
26
27
28
# grab the list of images that we’ll be describing, then extract
# the class label names from the image paths
print("[INFO] loading images...")
imagePaths = list(paths.list_images(args["dataset"]))
classNames = [pt.split(os.path.sep)[-2] for pt in imagePaths]
classNames = [str(x) for x in np.unique(classNames)]
As well as load our dataset from disk, construct our training/testing splits, and encode our
labels:
30
31
32
33
# initialize the image preprocessors
aap = AspectAwarePreprocessor(64, 64)
iap = ImageToArrayPreprocessor()Chapter 2. Data Augmentation
26
34
35
36
37
38
# load the dataset from disk then scale the raw pixel intensities
# to the range [0, 1]
sdl = SimpleDatasetLoader(preprocessors=[aap, iap])
(data, labels) = sdl.load(imagePaths, verbose=500)
data = data.astype("float") / 255.0
39
40
41
42
43
# partition the data into training and testing splits using 75% of
# the data for training and the remaining 25% for testing
(trainX, testX, trainY, testY) = train_test_split(data, labels,
test_size=0.25, random_state=42)
44
45
46
47
# convert the labels from integers to vectors
trainY = LabelBinarizer().fit_transform(trainY)
testY = LabelBinarizer().fit_transform(testY)
Our next code block is very important as it initializes our ImageDataGenerator:
49
50
51
52
# construct the image generator for data augmentation
aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,
height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,
horizontal_flip=True, fill_mode="nearest")
Here we’ll allow images to be:
1. Randomly rotated ±30 degrees
2. Horizontally and vertically shifted by a factor of 0.2
3. Sheared by 0.2
4. Zoomed by uniformly sampling in the range [0.8, 1.2]
5. Randomly horizontally flipped
Depending on your exact dataset you’ll want to tweak these data augmentation values. It’s
typical to see rotation ranges between [10, 30] depending on your application. Horizontal and
vertical shifts normally fall in the range [0.1, 0.2] (same goes for zoom values). Unless horizontally
flipping your image changes the class label, you should always include horizontal flipping as well.
Just as in the previous experiment we’ll train MiniVGGNet using the SGD optimizer:
54
55
56
57
58
59
60
# initialize the optimizer and model
print("[INFO] compiling model...")
opt = SGD(lr=0.05)
model = MiniVGGNet.build(width=64, height=64, depth=3,
classes=len(classNames))
model.compile(loss="categorical_crossentropy", optimizer=opt,
metrics=["accuracy"])
However, the code used to train our network has to change slightly as we are now using an
image generator:
62
63
64
65
66
# train the network
print("[INFO] training network...")
H = model.fit_generator(aug.flow(trainX, trainY, batch_size=32),
validation_data=(testX, testY), steps_per_epoch=len(trainX) // 32,
epochs=100, verbose=1)2.3 Comparing Training With and Without Data Augmentation
27
Instead of calling the .fit method of model, we now need to call .fit_generator. The first
parameter to .fit_generator is aug.flow, our data augmentation function used to generate new
training samples from the training data. The aug.flow requires us to pass in our training data
and corresponding labels. We also need to supply the batch size so the generator can construct
appropriate batches when training the network.
We then supply the validation_data as a 2-tuple of (testX, testY) – this data is used for
validation at the end of every epoch. The steps_per_epoch parameter controls the number of
batches per epoch – we can programmatically determine the proper steps_per_epoch value by
dividing the total number of training samples by our batch size and casting it to an integer. Finally,
epochs controls the total number of epochs our network should be trained for, in this case, 100
epochs.
After training our network we’ll evaluate it and plot the corresponding accuracy/loss plot:
68
69
70
71
72
# evaluate the network
print("[INFO] evaluating network...")
predictions = model.predict(testX, batch_size=32)
print(classification_report(testY.argmax(axis=1),
predictions.argmax(axis=1), target_names=classNames))
73
74
75
76
77
78
79
80
81
82
83
84
85
# plot the training loss and accuracy
plt.style.use("ggplot")
plt.figure()
plt.plot(np.arange(0, 100), H.history["loss"], label="train_loss")
plt.plot(np.arange(0, 100), H.history["val_loss"], label="val_loss")
plt.plot(np.arange(0, 100), H.history["acc"], label="train_acc")
plt.plot(np.arange(0, 100), H.history["val_acc"], label="val_acc")
plt.title("Training Loss and Accuracy")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend()
plt.show()
Notice how we do not apply data augmentation to the validation data. We only apply data
augmentation to the training set.
To train MiniVGGNet on Flowers-17 with data augmentation, just execute the following
command:
$ python minivggnet_flowers17_data_aug.py --dataset ../flowers17/images
[INFO] loading images...
[INFO] processed 500/1360
[INFO] processed 1000/1360
[INFO] compiling model...
[INFO] training network...
Epoch 1/100
3s - loss: 3.4073 - acc: 0.2108 - val_loss: 3.0306 - val_acc: 0.1882
...
Epoch 100/100
3s - loss: 0.2769 - acc: 0.9078 - val_loss: 1.3560 - val_acc: 0.6794
[INFO] evaluating network...
precision
recall f1-score
support
bluebell
0.67
0.44
0.53
18Chapter 2. Data Augmentation
28
buttercup
coltsfoot
cowslip
crocus
daffodil
daisy
dandelion
fritillary
iris
lilyvalley
pansy
snowdrop
sunflower
tigerlily
tulip
windflower 0.59
0.56
0.45
0.82
0.67
1.00
0.63
0.94
0.78
0.44
1.00
0.54
1.00
0.80
0.22
0.72 0.80
0.50
0.50
0.45
0.30
0.95
0.96
0.77
0.86
0.73
0.74
0.65
0.96
0.84
0.25
0.81 0.68
0.53
0.47
0.58
0.41
0.97
0.76
0.85
0.82
0.55
0.85
0.59
0.98
0.82
0.24
0.76 20
20
18
20
27
20
23
22
21
15
19
23
23
19
16
16
avg / total 0.71 0.68 0.68 340
Immediately after the network finishes training, you’ll notice an increase of accuracy from 64%
to 71%, a 10.9% improvement from our previous run. However, accuracy isn’t everything – the
real question is whether data augmentation has helped prevent overfitting. To answer that question,
we’ll need to examine the loss and accuracy plot in Figure 2.6.
Figure 2.6: Applying MiniVGGNet to Flowers-17 with data augmentation. Overfitting is still a
concern; however, we are able to obtain substantially higher classification accuracy and lower loss.
While there is still overfitting occurring, the effect is significantly dampened by using data
augmentation. Again, keep in mind that these two experiments are identical – the only changes
we made were whether or not data augmentation was applied. As a regularizer, you can also see2.4 Summary
29
data augmentation having an impact. We were able to increase our validation accuracy, thereby
improving the generalizability of our model, despite having lowering training accuracy.
Further accuracy can be obtained by decaying the learning rate over time. Learning rate was
specifically left out of this chapter so we could focus solely on the impact data augmentation as a
regularizer has when training Convolutional Neural Networks.
2.4
Summary
Data augmentation is a type of regularization technique that operates on the training data. As the
name suggests, data augmentation randomly jitters our training data by applying a series of random
translations, rotations, shears, and flips. Applying these simple transformations does not change the
class label of the input image; however, each augmented image can be considered a “new” image
that the training algorithm has not seen before. Therefore, our training algorithm is being constantly
presented with new training samples, allowing it to learn more robust and discriminative patterns.
As our results demonstrated, applying data augmentation increased our classification accuracy
while helping mitigate the effects of overfitting. Furthermore, data augmentation also allowed us
to train a Convolutional Neural Network on only 60 samples per class, far below the suggested
1,000-5,000 samples per class.
While it’s always better to gather “natural” training samples, in a pinch, data augmentation
can be used to overcome small dataset limitations. When it comes to your own experiments, you
should apply data augmentation to nearly every experiment you run. There is a slight performance
hit you must take due to the fact that the CPU is now responsible for randomly transforming your
inputs; however, this performance hit is mitigated by using threading and augmenting your data in
the background before it is passed to the thread responsible for training your network.
Again, in nearly all remaining chapters inside the Practitioner Bundle and ImageNet Bun-
dle, we’ll be using data augmentation. Take the time to familiarize yourself with this technique
now as it will help you obtain better performing deep learning models (using less data) quicker.3. Networks as Feature Extractors
Over the new few chapters, we’ll be discussing the concept of transfer learning, the ability to use a
pre-trained model as a “shortcut” to learn patterns from data it was not originally trained on.
Consider a traditional machine learning scenario where we are given two classification chal-
lenges. In the first challenge, our goal is to train a Convolutional Neural Network to recognize dogs
vs. cats in an image (as we’ll do in Chapter 10).
Then, in the second project, we are tasked with recognizing three separate species of bears:
grizzly bears, polar bears, and giant pandas. Using standard practices in machine learning, neural
networks, and deep learning, we would treat these these challenges as two separate problems. First,
we would gather a sufficient labeled dataset of dogs and cats, followed by training a model on the
dataset. We would then repeat the process a second time, only this time, gathering images of our
bear breeds, and then training a model on top of the labeled bear dataset.
Transfer learning proposes a different training paradigm – what if we could use an existing pre-
trained classifier and use it as a starting point for a new classification task? In context of the
proposed challenges above, we would first train a Convolutional Neural Network to recognize
dogs versus cats. Then, we would use the same CNN trained on dog and cat data to be used to
distinguish between bear classes, even though no bear data was mixed with the dog and cat data.
Does this sound too good to be true? It’s actually not. Deep neural networks trained on
large-scale datasets such as ImageNet have demonstrated to be excellent at the task of transfer
learning. These networks learn a set of rich, discriminating features to recognize 1,000 separate
object classes. It makes sense that these filters can be reused for classification tasks other than what
the CNN was originally trained on.
In general, there are two types of transfer learning when applied to deep learning for computer
vision:
1. Treating networks as arbitrary feature extractors.
2. Removing the fully-connected layers of an existing network, placing new FC layer set on
top of the CNN, and fine-tuning these weights (and optionally previous layers) to recognize
object classes.
In this chapter, we’ll be focusing primarily on the first method of transfer learning, treating32
Chapter 3. Networks as Feature Extractors
networks as feature extractors. We’ll then discuss how to fine-tune the weights of a network to a
specific classification task in Chapter 5.
3.1
Extracting Features with a Pre-trained CNN
Up until this point, we have treated Convolutional Neural Networks as end-to-end image classifiers:
1. We input an image to the network.
2. The image forward propagates through the network.
3. We obtain the final classification probabilities from the end of the network.
However, there is no “rule” that says we must allow the image to forward propagate through
the entire network. Instead, we can stop the propagation at an arbitrary layer, such as an activation
or pooling layer, extract the values from the network at this time, and then use them as feature
vectors. For example, let’s consider the VGG16 network architecture by Simonyan and Zisserman
[11] (Figure 3.1, left).
Figure 3.1: Left: The original VGG16 network architecture that outputs probabilities for each
of the 1,000 ImageNet class labels. Right: Removing the FC layers from VGG16 and instead
returning the output of the final POOL layer. This output will serve as our extracted features.
Along with the layers in the network, we have also included the input and output shapes of the3.1 Extracting Features with a Pre-trained CNN
33
volumes for each layer. When treating networks as a feature extractor, we essentially “chop off” the
network at an arbitrary point (normally prior to the fully-connected layers, but it really depends on
your particular dataset).
Now the last layer in our network is a max pooling layer (Figure 3.1, right) which will have the
output shape of 7 × 7 × 512 implying there are 512 filters each of size 7 × 7. If we were to forward
propagate an image through this network with its FC head removed, we would be left with 512,
7 × 7 activations that have either activated or not based on the image contents. Therefore, we can
actually take these 7 × 7 × 512 = 25, 088 values and treat them as a feature vector that quantifies
the contents of an image.
If we repeat this process for an entire dataset of images (including datasets that VGG16 was
not trained on), we’ll be left with a design matrix of N images, each with 25,088 columns used to
quantify their contents (i.e., feature vectors). Given our feature vectors, we can train an off-the-shelf
machine learning model such a Linear SVM, Logistic Regression classifier, or Random Forest on
top of these features to obtain a classifier that recognizes new classes of images.
Keep in mind that the CNN itself is not capable of recognizing these new classes – instead,
we are using the CNN as an intermediary feature extractor. The downstream machine learning
classifier will take care of learning the underlying patterns of the features extracted from the CNN.
Later in this chapter, I’ll be demonstrating how you can use pre-trained CNNs (specifically
VGG16) and the Keras library to obtain > 95% classification accuracy on image datasets such as
Animals, CALTECH-101, and Flowers-17. Neither of these datasets contain images that VGG16
was trained on, but by applying transfer learning, we are able to build super accurate image
classifiers with little effort. The trick is extracting these features and storing them in an efficient
manner. To accomplish this task, we’ll need HDF5.
3.1.1
What Is HDF5?
HDF5 is binary data format created by the HDF5 group [12] to store gigantic numerical datasets on
disk (far too large to store in memory) while facilitating easy access and computation on the rows of
the datasets. Data in HDF5 is stored hierarchically, similar to how a file system stores data. Data is
first defined in groups, where a group is a container-like structure which can hold datasets and other
groups. Once a group has been defined, a dataset can be created within the group. A dataset can be
thought of as a multi-dimensional array (i.e., a NumPy array) of a homogeneous data type (integer,
float, unicode, etc.). An example of an HDF5 file containing a group with multiple datasets is
displayed in Figure 3.2.
HDF5 is written in C; however, by using the h5py module (h5py.org), we can gain access to
the underlying C API using the Python programming language. What makes h5py so awesome
is the ease of interaction with data. We can store huge amounts of data in our HDF5 dataset and
manipulate the data in a NumPy-like fashion. For example, we can use standard Python syntax to
access and slice rows from multi-terabyte datasets stored on disk as if they were simple NumPy
arrays loaded into memory. Thanks to specialized data structures, these slices and row accesses are
lighting quick. When using HDF5 with h5py, you can think of your data as a gigantic NumPy array
that is too large to fit into main memory but can still be accessed and manipulated just the same.
Perhaps best of all, the HDF5 format is standardized, meaning that datasets stored in HDF5
format are inherently portable and can be accessed by other developers using different programming
languages such as C, MATLAB, and Java.
In the rest of this chapter, we’ll be writing a custom Python class that allows us to efficiently
accept input data and write it to an HDF5 dataset. This class will then serve two purposes:
1. Facilitate a method for us to apply transfer learning by taking our extracted features from
VGG16 and writing them to an HDF5 dataset in an efficient manner.
2. Allow us to generate HDF5 datasets from raw images to facilitate faster training (Chapter 9).Chapter 3. Networks as Feature Extractors
34
Figure 3.2: An example of a HDF5 file with three datasets. The first dataset contains the
label_names for CALTECH-101. We then have labels, which maps the each image to its
corresponding class label. Finally, the features dataset contains the image quantifications ex-
tracted by the CNN.
If you do not already have HDF5 and h5py installed on your system, please see the supplemen-
tary material for Chapter 6 of the Starter Bundle for instructions to configure your system.
3.1.2
Writing Features to an HDF5 Dataset
Before we can even think about treating VGG16 (or any other CNN) as a feature extractor, we
first need to develop a bit of infrastructure. In particular, we need to define a Python class
named HDF5DatasetWriter, which as the name suggests, is responsible for taking an input set of
NumPy arrays (whether features, raw images, etc.) and writing them to HDF5 format. To do so,
create a new sub-module in the pyimagesearch package named io and then place a file named
hdf5datasetwriter.py inside of io:
--- pyimagesearch
|
|--- __init__.py
|
|--- callbacks
|
|--- io
|
|
|--- __init__.py
|
|
|--- hdf5datasetwriter.py
|
|--- nn
|
|--- preprocessing
|
|--- utils3.1 Extracting Features with a Pre-trained CNN
35
From there, open up hdf5datasetwriter.py and we’ll get to work:
1
2
3
# import the necessary packages
import h5py
import os
We’ll start off easy with our imports. We only need two Python packages to build the func-
tionality inside this class – the built-in os module and h5py so we have access to the HDF5
bindings.
From there, let’s define the constructor:
5
6
7
8
9
10
11
12
13
class HDF5DatasetWriter:
def __init__(self, dims, outputPath, dataKey="images",
bufSize=1000):
# check to see if the output path exists, and if so, raise
# an exception
if os.path.exists(outputPath):
raise ValueError("The supplied ‘outputPath‘ already "
"exists and cannot be overwritten. Manually delete "
"the file before continuing.", outputPath)
14
15
16
17
18
19
20
21
22
# open the HDF5 database for writing and create two datasets:
# one to store the images/features and another to store the
# class labels
self.db = h5py.File(outputPath, "w")
self.data = self.db.create_dataset(dataKey, dims,
dtype="float")
self.labels = self.db.create_dataset("labels", (dims[0],),
dtype="int")
23
24
25
26
27
28
# store the buffer size, then initialize the buffer itself
# along with the index into the datasets
self.bufSize = bufSize
self.buffer = {"data": [], "labels": []}
self.idx = 0
The constructor to HDF5DatasetWriter accepts four parameters, two of which are optional.
The dims parameter controls the dimension or shape of the data we will be storing in the dataset.
Think of dims as the .shape of a NumPy array. If we were storing the (flattened) raw pixel
intensities of the 28 × 28 = 784 MNIST dataset, then dims=(70000, 784) as there are 70,000
examples in MNIST, each with a dimensionality of 784. If we wanted to store the raw CIFAR-10
images, then we would set dims=(60000, 32, 32, 3) as there are 60,000 total images in the
CIFAR-10 dataset, each represented by a 32 × 32 × 3 RGB image.
In the context of transfer learning and feature extraction, we’ll be using the VGG16 architecture
and taking the outputs after the final POOL layer. The output of the final POOL layer is 512 × 7 ×
7 which, when flattened, yields a feature vector of length 25,088. Therefore, when using VGG16
for feature extraction, we’ll set dims=(N, 25088) where N is the total number of images in our
dataset.
The next parameter to the HDF5DatasetWriter constructor is the outputPath – this is the
path to where our output HDF5 file will be stored on disk. The optional dataKey is the name of the
dataset that will store the data our algorithm will learn from. We default this value to "images",36
Chapter 3. Networks as Feature Extractors
since in most cases we’ll be storing raw images in HDF5 format. However, for this example, when
we instantiate the HDF5DatasetWriter we’ll set dataKey="features" to indicate that we are
storing features extracted from a CNN in the file.
Finally, bufSize controls the size of our in-memory buffer, which we default to 1,000 feature
vectors/images. Once we reach bufSize, we’ll flush the buffer to the HDF5 dataset.
Lines 10-13 then make a check to see if outputPath already exists. If it does, we raise an
error to the end user (as we don’t want to overwrite an existing database).
Line 18 opens the HDF5 file for writing using the supplied outputPath. Lines 19 and
20 create a dataset with the dataKey name and the supplied dims – this is where we will store
our raw images/extracted features. Lines 21 and 22 create a second dataset, this one to store the
(integer) class labels for each record in the dataset Lines 25-28 then initialize our buffers.
Next, let’s review the add method used to add data to our buffer:
30
31
32
33
def add(self, rows, labels):
# add the rows and labels to the buffer
self.buffer["data"].extend(rows)
self.buffer["labels"].extend(labels)
34
35
36
37
# check to see if the buffer needs to be flushed to disk
if len(self.buffer["data"]) >= self.bufSize:
self.flush()
The add method requires two parameters: the rows that we’ll be adding to the dataset, along
with their corresponding class labels. Both the rows and labels are added to their respective
buffers on Lines 32 and 33. If the buffer fills up, we call the flush method to write the buffers to
file and reset them.
Speaking of the flush method, let’s define the function now:
39
40
41
42
43
44
45
def flush(self):
# write the buffers to disk then reset the buffer
i = self.idx + len(self.buffer["data"])
self.data[self.idx:i] = self.buffer["data"]
self.labels[self.idx:i] = self.buffer["labels"]
self.idx = i
self.buffer = {"data": [], "labels": []}
If we think of our HDF5 dataset as a big NumPy array, then we need to keep track of the current
index into the next available row where we can store data (without overwriting existing data) –
Line 41 determines the next available row in the matrix. Lines 42 and 43 then apply NumPy array
slicing to store the data and labels in the buffers. Line 45 then resets the buffers.
We’ll also define a handy utility function named storeClassLabels which, if called, will
store the raw string names of the class labels in a separate dataset:
47
48
49
50
51
52
53
def storeClassLabels(self, classLabels):
# create a dataset to store the actual class label names,
# then store the class labels
dt = h5py.special_dtype(vlen=unicode)
labelSet = self.db.create_dataset("label_names",
(len(classLabels),), dtype=dt)
labelSet[:] = classLabels3.2 The Feature Extraction Process
37
Finally, our last function close will be used to write any data left in the buffers to HDF5 as
well as close the dataset:
55
56
57
58
59
def close(self):
# check to see if there are any other entries in the buffer
# that need to be flushed to disk
if len(self.buffer["data"]) > 0:
self.flush()
60
61
62
# close the dataset
self.db.close()
As you can see, the HDF5DatasetWriter doesn’t have much to do with machine learning or
deep learning at all – it’s simply a class used to help us store data in HDF5 format. As you continue
in your deep learning career, you’ll notice that much of the initial labor when setting up a new
problem is getting the data into a format you can work with. Once you have your data in a format
that’s straightforward to manipulate, it becomes substantially easier to apply machine learning and
deep learning techniques to your data.
All that said, since the HDF5DatasetWriter class is a utility class non-specific to deep learning
and computer vision, I’ve kept the explanation of the class shorter than the other code examples in
this book. If you find yourself struggling to understand this class I would suggest you:
1. Finish reading the rest of this chapter so you can understand how we use it in context of
feature extraction.
2. Take the time to educate yourself on some basic Python programming paradigms – I provide
a list of Python programming sources I recommend here: http://pyimg.co/ida57.
3. Take apart this class and implement by hand, piece-by-piece, until you understand what is
going on under the hood.
Now that our HDF5DatasetWriter is implemented, we can move on to actually extracting
features using pre-trained Convolutional Neural Networks.
3.2
The Feature Extraction Process
Let’s define a Python script that can be used to extract features from an arbitrary image dataset
(provided the input dataset follows a specific directory structure). Open up a new file, name it
extract_features.py, and we’ll get to work:
1
2
3
4
5
6
7
8
9
10
11
12
13
# import the necessary packages
from keras.applications import VGG16
from keras.applications import imagenet_utils
from keras.preprocessing.image import img_to_array
from keras.preprocessing.image import load_img
from sklearn.preprocessing import LabelEncoder
from pyimagesearch.io import HDF5DatasetWriter
from imutils import paths
import numpy as np
import progressbar
import argparse
import random
import osChapter 3. Networks as Feature Extractors
38
Lines 2-13 import our required Python packages. Notice how on Line 2 we import the Keras
implementation of the pre-trained VGG16 network – this is the architecture we’ll be using as
our feature extractor. The LabelEncoder class on Line 6 will be used to convert our class labels
from strings to integers. We also import our HDF5DatasetWriter on Line 7 so we can write the
features extracted from our CNN to a HDF5 dataset.
One import you haven’t seen yet is progressbar on Line 10. This package has nothing to
do with deep learning, but I like to use it for long running tasks as it displays a nicely formatted
progress bar to your terminal, as well as provides approximate timings as to when your script will
finish executing:
1
Extracting Features
30% |############
| ETA: 0:00:18
If you do not already have progressbar installed on your system, you can install it via:
$ pip install progressbar
Otherwise, you can simply comment out all lines that use progressbar (the package is only
used for fun, after all).
Let’s move on to our command line arguments:
15
16
17
18
19
20
21
22
23
24
25
# construct the argument parse and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-d", "--dataset", required=True,
help="path to input dataset")
ap.add_argument("-o", "--output", required=True,
help="path to output HDF5 file")
ap.add_argument("-b", "--batch-size", type=int, default=32,
help="batch size of images to be passed through network")
ap.add_argument("-s", "--buffer-size", type=int, default=1000,
help="size of feature extraction buffer")
args = vars(ap.parse_args())
Our extract_features.py script will require two command line arguments, followed by two
optional ones. The --dataset switch controls the path to our input directory of images that we
wish to extract features from. The --output switch determines the path to our output HDF5 data
file.
We can then supply a --batch-size – this is the number of images in a batch that will be
passed through VGG16 at a time. A value of 32 is reasonable here, but you can increase it if your
machine has sufficient memory. The --buffer-size switch controls the number of extracted
features we’ll store in memory before writing the buffer for our HDF5 dataset. Again, if your
machine has sufficient memory, you can increase the buffer size.
The next step is to grab our image paths from disk, shuffle them, and encode the labels:
27
28
# store the batch size in a convenience variable
bs = args["batch_size"]
29
30
31
32
# grab the list of images that we’ll be describing then randomly
# shuffle them to allow for easy training and testing splits via
# array slicing during training time3.2 The Feature Extraction Process
33
34
35
39
print("[INFO] loading images...")
imagePaths = list(paths.list_images(args["dataset"]))
random.shuffle(imagePaths)
36
37
38
39
40
41
# extract the class labels from the image paths then encode the
# labels
labels = [p.split(os.path.sep)[-2] for p in imagePaths]
le = LabelEncoder()
labels = le.fit_transform(labels)
On Line 34 we grab our imagePaths, the filenames for all images in our dataset. We then
purposely shuffle them on Line 35. Why do we bother with the shuffling? Well, keep in mind that
in previous examples in this book we computed a training and testing split prior to training our
classifier. However, since we’ll be working with datasets too large to fit into memory, we won’t be
able to perform this shuffle in memory – therefore, we shuffle the image paths before we extract the
features. Then, at training time, we can compute the 75 percent index into the HDF5 dataset and
use that index as the end of the training data and the start of the testing data (this point will become
more clear in Section 3.3 below).
Line 39 then extracts the class label names from our file paths, assuming our file paths have the
directory structure:
dataset_name/{class_label}/example.jpg
Provided that our dataset does follow this directory structure (as all examples in this book do),
Line 39 splits the path into an array based on the path separator (‘/’ on Unix machines and ‘\’
on Windows), and then grabs the second-to-last entry in the array – this operation yields the class
label of the particular image. Given the labels, we then encode them as integers on Lines 40 and
41 (we’ll perform one-hot encoding during the training process).
We can now load the VGG16 network weights and instantiate our HDF5DatasetWriter:
43
44
45
# load the VGG16 network
print("[INFO] loading network...")
model = VGG16(weights="imagenet", include_top=False)
46
47
48
49
50
51
# initialize the HDF5 dataset writer, then store the class label
# names in the dataset
dataset = HDF5DatasetWriter((len(imagePaths), 512 * 7 * 7),
args["output"], dataKey="features", bufSize=args["buffer_size"])
dataset.storeClassLabels(le.classes_)
Line 45 we load the pre-trained VGG16 network from disk; however, notice how we have
included the parameter include_top=False – supplying this value indicates that the final fully-
connected layers should not be included in the architecture. Therefore, when forward propagating
an image through the network, we’ll obtain the feature values after the final POOL layer rather than
the probabilities produced by the softmax classifier in the FC layers.
Lines 49 and 50 instantiates the HDF5DatasetWriter. The first parameter is our dimensions
of the dataset, where there will be len(imagePaths) total images, each with a feature vector of
size 512 × 7 × 7 = 25, 088. Line 51 then stores the string names of the class labels according to the
label encoder.
Now it’s time to perform the actual feature extraction:40
53
54
55
56
57
Chapter 3. Networks as Feature Extractors
# initialize the progress bar
widgets = ["Extracting Features: ", progressbar.Percentage(), " ",
progressbar.Bar(), " ", progressbar.ETA()]
pbar = progressbar.ProgressBar(maxval=len(imagePaths),
widgets=widgets).start()
58
59
60
61
62
63
64
65
66
# loop over the images in patches
for i in np.arange(0, len(imagePaths), bs):
# extract the batch of images and labels, then initialize the
# list of actual images that will be passed through the network
# for feature extraction
batchPaths = imagePaths[i:i + bs]
batchLabels = labels[i:i + bs]
batchImages = []
Lines 54-57 initialize our progress bar so we can visualize and estimate how long the feature
extraction process is going to take. Again, using progressbar is optional, so feel free to comment
these lines out.
On Line 60 we start looping over our imagePaths in batches of --batch-size. Lines 64
and 65 extract the image paths and labels for the corresponding batch, while Line 66 initializes a
list to store the images about to be loaded and fed into VGG16.
Preparing an image for feature extraction is exactly the same as preparing an image for
classification via a CNN:
68
69
70
71
72
73
# loop over the images and labels in the current batch
for (j, imagePath) in enumerate(batchPaths):
# load the input image using the Keras helper utility
# while ensuring the image is resized to 224x224 pixels
image = load_img(imagePath, target_size=(224, 224))
image = img_to_array(image)
74
75
76
77
78
79
# preprocess the image by (1) expanding the dimensions and
# (2) subtracting the mean RGB pixel intensity from the
# ImageNet dataset
image = np.expand_dims(image, axis=0)
image = imagenet_utils.preprocess_input(image)
80
81
82
# add the image to the batch
batchImages.append(image)
On Line 69 we loop over each image path in the batch. Each image is loaded from disk and
converted to a Keras-compatible array (Lines 72 and 73). We then preprocess the image on Lines
78 and 79, followed by adding it to batchImages (Line 82).
To obtain our feature vectors for the images in batchImages, all we need to do is call the
.predict method of model:
84
85
86
87
88
# pass the images through the network and use the outputs as
# our actual features
batchImages = np.vstack(batchImages)
features = model.predict(batchImages, batch_size=bs)3.2 The Feature Extraction Process
89
90
91
41
# reshape the features so that each image is represented by
# a flattened feature vector of the ‘MaxPooling2D‘ outputs
features = features.reshape((features.shape[0], 512 * 7 * 7))
92
93
94
95
# add the features and labels to our HDF5 dataset
dataset.add(features, batchLabels)
pbar.update(i)
We use the .vstack method of NumPy on Lines 86 to “vertically stack” our images such that
they have the shape (N, 224, 224, 3) where N is the size of the batch.
Passing batchImages through our network yields our actual feature vectors – remember, we
chopped off the fully-connected layers at the head of VGG16, so now we are left with the values
after the final max pooling operation (Line 87). However, the output of the POOL has the shape (N,
512, 7, 7), implying there are 512 filters, each of size 7 × 7. To treat these values as a feature
vector, we need to flatten them into an array with shape (N, 25088), which is exactly what Line
91 accomplishes. Line 94 adds our features and batchLabels to our HDF5 dataset.
Our final code block handles closing our HDF5 dataset:
97
98
99
# close the dataset
dataset.close()
pbar.finish()
In the remainder of this section, we are going to practice extracting features using a pre-trained
CNN from various datasets.
3.2.1
Extracting Features From Animals
The first dataset we are going to extract features from using VGG16 is our “Animals” dataset. This
dataset consists of 3,000 images, of three classes: dogs, cats, and pandas. To utilize VGG16 to
extract features from these images, simply execute the following command:
$ python extract_features.py --dataset ../datasets/animals/images \
--output ../datasets/animals/hdf5/features.hdf5
[INFO] loading images...
[INFO] loading network...
Extracting Features: 100% |####################################| Time: 0:00:35
Using my Titan X GPU, I was able to extract features from all 3,000 images in approximately
35 seconds. After the script executes, take a look inside your animals/hdf5 directory and you’ll
find a file named features.hdf5:
$ ls ../datasets/animals/hdf5/
features.hdf5
To investigate the .hdf5 file for the Animals dataset, fire up a Python shell:
$ python
>>> import h5py
>>> p = "../datasets/animals/hdf5/features.hdf5"
>>> db = h5py.File(p)
>>> list(db.keys())
[u’features’, u’label_names’, u’labels’]Chapter 3. Networks as Feature Extractors
42
Notice how our HDF5 file has three datasets: features, label_names, and labels. The
features dataset is where our actual extracted features are stored. You can examine the shape of
this dataset using the following commands:
>>> db["features"].shape
(3000, 25088)
>>> db["labels"].shape
(3000,)
>>> db["label_names"].shape
(3,)
Notice how the .shape is (3000, 25088) – this result implies that each of the 3,000 images
in our Animals dataset is quantified via feature vector with length 25,088 (i.e., the values inside
VGG16 after the final POOL operation). Later in this chapter, we’ll learn how we can train a classifier
on these features.
3.2.2
Extracting Features From CALTECH-101
Just as we extracted features from the Animals dataset, we can do the same with CALTECH-101:
$ python extract_features.py --dataset ../datasets/caltech-101/images \
--output ../datasets/caltech-101/hdf5/features.hdf5
[INFO] loading images...
[INFO] loading network...
Extracting Features: 100% |####################################| Time: 0:01:27
We now have a file named features.hdf5 in the caltech-101/hdf5 directory:
$ ls ../datasets/caltech-101/hdf5/
features.hdf5
Examining this file you’ll see that each of the 8,677 images is represented by a 25, 088-dim
feature vector.
3.2.3
Extracting Features From Flowers-17
Finally, let’s apply CNN feature extraction to the Flowers-17 dataset:
$ python extract_features.py --dataset ../datasets/flowers17/images \
--output ../datasets/flowers17/hdf5/features.hdf5
[INFO] loading images...
[INFO] loading network...
Extracting Features: 100% |####################################| Time: 0:00:19
Examining the features.hdf5 file for Flowers-17 you’ll see that each of the 1,360 images in
the dataset is quantified via a 25, 088-dim feature vector.3.3 Training a Classifier on Extracted Features
3.3
43
Training a Classifier on Extracted Features
Now that we’ve used a pre-trained CNN to extract features from a handful of datasets, let’s see
how discriminative these features really are, especially given that the VGG16 was trained on
ImageNet and not Animals, CALTECH-101, or Flowers-17.
Take second now and venture a guess on how good of a job a simple linear model might do at
using these features to classify an image – would you guess better than 60% classification accuracy?
Does 70% seem unreasonable? Surely 80% is unlikely? And 90% classification accuracy would be
unfathomable, right?
Let’s find out for ourselves. Open up a new file, name it train_model.py, and insert the
following code:
1
2
3
4
5
6
7
# import the necessary packages
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
import argparse
import pickle
import h5py
Lines 2-7 import our required Python packages. The GridSearchCV class will be used to help
us turn the parameters to our LogisticRegression classifier. We’ll be using pickle to serialize
our LogisticRegression model to disk after training. Finally, h5py will be used so we can
interface with our HDF5 dataset of features.
We can now parse our command line arguments:
9
10
11
12
13
14
15
16
17
# construct the argument parse and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-d", "--db", required=True,
help="path HDF5 database")
ap.add_argument("-m", "--model", required=True,
help="path to output model")
ap.add_argument("-j", "--jobs", type=int, default=-1,
help="# of jobs to run when tuning hyperparameters")
args = vars(ap.parse_args())
Our script requires two command line arguments, followed by a third optional one:
1. --db: The path to our HDF5 dataset containing our extracted features and class labels.
2. --model: Here we supply the path to our output Logistic Regression classifier.
3. --jobs: An optional integer used to specify the number of concurrent jobs when running a
grid search to tune our hyperparameters to the Logistic Regression model.
Let’s open our HDF5 dataset and determine where our training/testing split will be:
19
20
21
22
23
# open the HDF5 database for reading then determine the index of
# the training and testing split, provided that this data was
# already shuffled *prior* to writing it to disk
db = h5py.File(args["db"], "r")
i = int(db["labels"].shape[0] * 0.75)Chapter 3. Networks as Feature Extractors
44
As I mentioned earlier in this chapter, we purposely shuffled our image paths prior to writing
the associated images/feature vectors to the HDF5 dataset – the reason why becomes clear on Lines
22 and 23.
Given that our dataset is too large to fit into memory, we need an efficient method to determine
our training and testing split. Since we know how many entries there are in the HDF5 dataset (and
we know we want to use 75% of the data for training and 25% for evaluation), we can simply
compute the 75% index i into the database. Any data before the index i is considered training data
– anything after i is testing data.
Given our training and testing splits, let’s train our Logistic Regression classifier:
25
26
27
28
29
30
31
32
# define the set of parameters that we want to tune then start a
# grid search where we evaluate our model for each value of C
print("[INFO] tuning hyperparameters...")
params = {"C": [0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0]}
model = GridSearchCV(LogisticRegression(), params, cv=3,
n_jobs=args["jobs"])
model.fit(db["features"][:i], db["labels"][:i])
print("[INFO] best hyperparameters: {}".format(model.best_params_))
33
34
35
36
37
38
# evaluate the model
print("[INFO] evaluating...")
preds = model.predict(db["features"][i:])
print(classification_report(db["labels"][i:], preds,
target_names=db["label_names"]))
Lines 28-31 run a grid search over the parameter C, the strictness of the Logistic Regression
classifier to determine what the optimal value is. A full detailed review of Logistic Regression
is outside the scope of this book, so please see Andrew Ng’s notes for a thorough review of the
Logistic Regression classifier [13].
Take note of how we indicate the training data and training labels via array slices:
db["features"][:i]
db["labels"][:i]
Again, any data before index i is part of our training set. Once the best hyperparameters are
found, we then evaluate the classifier on the testing data (Lines 36-38).
Notice here that our testing data and testing labels are accessed via the array slices:
db["features"][i:]
db["labels"][i:]
Anything after the index i is part of our testing set. Even though our HDF5 dataset resides on
disk (and is too large to fit into memory), we can still treat it as if it was a NumPy array, which
is one of the huge advantages of using HDF5 and h5py together for deep learning and machine
learning tasks.
Finally, we save our LogisticRegression model to disk and close the database:
40
41
# serialize the model to disk
print("[INFO] saving model...")3.3 Training a Classifier on Extracted Features
42
43
44
45
f = open(args["model"], "wb")
f.write(pickle.dumps(model.best_estimator_))
f.close()
45
46
47
# close the database
db.close()
Also notice how there is no specific code related to either of the Animals, CALTECH-101, or
Flowers-17 datasets – as long as our input dataset of images conforms to the directory structure
detailed in Section 3.2 above, we can use both extract_features.py and train_model.py to
rapidly build robust image classifiers based on features extracted from CNNs. How robust, you
ask? Let’s let the results do the talking.
3.3.1
Results on Animals
To train a Logistic Regression classifier on the features extracted via the VGG16 network on the
Animals dataset, simply execute the following command:
$ python train_model.py --db ../datasets/animals/hdf5/features.hdf5 \
--model animals.cpickle
[INFO] tuning hyperparameters...
[INFO] best hyperparameters: {’C’: 0.1}
[INFO] evaluating...
precision
recall f1-score
support
cats
dogs
panda 0.96
0.98
0.98 0.98
0.95
1.00 0.97
0.97
0.99 252
253
245
avg / total 0.98 0.98 0.98 750
Notice here that we are able to reach 98% classification accuracy! This number is a massive im-
provement from our previous best of 71% in Chapter 12 of the Starter Bundle.
3.3.2
Results on CALTECH-101
These incredible results continue to the CALTECH-101 dataset as well. Execute this command to
evaluate the performance of VGG16 features on CALTECH-101:
$ python train_model.py \
--db ../datasets/caltech-101/hdf5/features.hdf5 \
--model caltech101.cpickle
[INFO] tuning hyperparameters...
[INFO] best hyperparameters: {’C’: 1000.0}
[INFO] evaluating...
precision
recall f1-score
support
Faces
Faces_easy
Leopards
Motorbikes 1.00
0.98
1.00
1.00 0.98
1.00
1.00
1.00 0.99
0.99
1.00
1.00 114
104
44
197
...
windsor_chair 0.92 0.92 0.92 13Chapter 3. Networks as Feature Extractors
46
wrench
yin_yang 0.88
1.00 0.78
1.00 0.82
1.00 9
11
avg / total 0.96 0.96 0.96 2170
This time we are able to obtain 96% classification accuracy on 101 separate object categories
with minimal effort!
3.3.3
Results on Flowers-17
Finally, let’s apply the VGG16 features to the Flowers-17 dataset, where previously we struggled to
break 71 percent accuracy, even when using data augmentation:
$ python train_model.py \
--db ../datasets/flowers17/hdf5/features.hdf5 \
--model flowers17.cpickle
[INFO] tuning hyperparameters...
[INFO] best hyperparameters: {’C’: 0.1}
[INFO] evaluating...
precision
recall f1-score
support
bluebell
buttercup
coltsfoot
cowslip
crocus
daffodil
daisy
dandelion
fritillary
iris
lilyvalley
pansy
snowdrop
sunflower
tigerlily
tulip
windflower 1.00
0.90
1.00
0.67
0.94
0.94
1.00
1.00
1.00
1.00
0.73
0.95
0.95
0.96
1.00
0.76
1.00 1.00
0.78
1.00
0.95
1.00
0.77
0.95
1.00
0.96
0.94
0.94
1.00
0.72
1.00
1.00
0.84
0.94 1.00
0.84
1.00
0.78
0.97
0.85
0.97
1.00
0.98
0.97
0.82
0.98
0.82
0.98
1.00
0.80
0.97 25
23
20
19
16
22
20
18
23
16
17
20
29
24
12
19
17
avg / total 0.93 0.92 0.92 340
This time we reach 93% classification accuracy, much higher than the 71% before. Clearly, the
networks such as VGG are capable of performing transfer learning, encoding their discriminative
features into output activations that we can use to train our own custom image classifiers.
3.4
Summary
In this chapter, we started to explore transfer learning, the concept of using a pre-trained Convo-
lutional Neural Network to classify class labels outside of what it was originally trained on. In
general, there are two methods to perform transfer learning when applied to deep learning and
computer vision:
1. Treat networks as feature extractors, forward propagating the image until a given layer, and
then taking these activations and treating them as feature vectors.3.4 Summary
47
2. Fine-tuning networks by adding a brand-new set of fully-connected layers to the head of
the network and tuning these FC layers to recognize new classes (while still using the same
underlying CONV filters).
We focused strictly on the feature extraction component of transfer learning in this chapter,
demonstrating that deep CNNs such as VGG, Inception, and ResNet are capable of acting as
powerful feature extraction machines, even more powerful than hand-designed algorithms such as
HOG [14], SIFT [15], and Local Binary Patterns [16], just to name a few. Whenever approaching a
new problem with deep learning and Convolutional Neural Networks, always consider if applying
feature extraction will obtain reasonable accuracy – if so, you can skip the network training process
entirely, saving you a ton of time, effort, and headache.
We’ll go through my optimal pathway to apply deep learning techniques such as feature
extraction, fine-tuning, and training from scratch in Chapter 8. Until then, let’s continue studying
transfer learning.4. Understanding rank-1 & rank-5 Accuracies
Before we get too far in our discussion of advanced deep learning topics (such as transfer learning),
let’s first take a step back and discuss the concept of rank-1, rank-5, and rank-N accuracy. When
reading deep learning literature, especially in the computer vision and image classification space,
you’ll likely encounter the concept of ranked accuracy. For example, nearly all papers that present
machine learning methods evaluated on the ImageNet dataset present their results in terms of both
rank-1 and rank-5 accuracy (we’ll find out why both rank-1 and rank-5 accuracy are reported later
in this chapter).
What exactly is rank-1 and rank-5 accuracy? And how do they differ from the traditional
accuracy (i.e., precision)? In this chapter, we’ll discuss ranked accuracy, learn how to implement
it, and then apply it to machine learning models trained on the Flowers-17 and CALTECH-101
datasets.
4.1
Ranked Accuracy
Figure 4.1: Left: An input image of a frog that our neural network will try to classify. Right: An
input image of a car.
Ranked accuracy is best explained in terms of an example. Let’s suppose we are are evaluating
a neural network trained on the CIFAR-10 dataset which includes ten classes: airplane, automobile,Chapter 4. Understanding rank-1 & rank-5 Accuracies
50
Class Label
Airplane
Automobile
Bird
Cat
Deer
Dog
Frog
Horse
Ship
Truck
Probability
0.0%
0.0%
2.1%
0.03%
0.01%
0.56%
97.3%
0.0%
0.0%
0.0%
Class Label
Airplane
Automobile
Bird
Cat
Deer
Dog
Frog
Horse
Ship
Truck
Probability
1.1%
38.7%
0.0%
0.5%
0.0%
0.4%
0.11%
1.4%
2.39%
55.4%
Table 4.1: Left: Class label probabilities returned by our neural network for Figure 4.1 (left).
Right: Class label probabilities returned by our network for Figure 4.1 (right).
bird, cat, deer, dog, frog, horse, ship, and truck. Given the following input image (Figure 4.1, left)
we ask our neural network to compute the probabilities for each class label – the neural network
then returns the class label probabilities listed in Table 4.1 (left).
The class label with the largest probability is frog (97.3%) which is indeed the correct prediction.
If we were to repeat this process of:
1. Step #1: Computing the class label probabilities for each input image in the dataset.
2. Step #2: Determining if the ground-truth label is equal to the predicted class label with the
largest probability.
3. Step #3: Tallying the number of times where Step #2 is true.
We would arrive at our rank-1 accuracy. Rank-1 accuracy is, therefore, the percentage of
predictions where the top prediction matches the ground-truth label – this is the “standard” type of
accuracy we are used to computing: take the total number of correct predictions and divide it by
the number of data points in the dataset.
We can then extend this concept to rank-5 accuracy. Instead of caring only about the number
one prediction, we care about the top-5 predictions. Our evaluation process now becomes:
1. Step #1: Compute the class label probabilities for each input image in the dataset.
2. Step #2: Sort the predicted class label probabilities in descending order, such that labels
with higher probability are placed at the front of the list.
3. Step #3: Determine if the ground-truth label exists in the top-5 predicted labels from Step
#2.
4. Step #4: Tally the number of times where Step #3 is true.
Rank-5 is simply an extension to rank-1 accuracy: instead of caring about only the #1 prediction
from the classifier, we’ll take into account the top-5 predictions from the network. For example,
let’s again consider an input image that is to be categorized into a CIFAR-10 category based on an
arbitrary neural network (Figure 4.1, right). After being passed through our network, we obtain the
class label probabilities detailed in Table 4.1 (right).
Our image is clearly of a car; however, our network has reported truck as the top prediction
– this would be considered an incorrect prediction for rank-1 accuracy. But if we examine the
top-5 predictions by the network, we see that automobile is actually the number two prediction,
which would be accurate when computing rank-5 accuracy. This approach can easily be extended
to arbitrary rank-N accuracy as well; however, we normally only compute rank-1 and rank-5
accuracies – which raises the question, why bother computing rank-5 accuracy at all?
For the CIFAR-10 dataset, computing the rank-5 accuracy is a bit silly, but for large, challenging4.1 Ranked Accuracy
51
datasets, especially for fine-grained classification, it’s often helpful to look at the top-5 predictions
from a given CNN. Perhaps the best example of why we compute rank-1 and rank-5 accuracy can
be found in Szegedy et al. [17] where we can see a Siberian husky on the left and an Eskimo dog
on the right (Figure 4.2). Most humans would fail to recognize the difference between the two
animals; however, both of these classes are valid labels in the ImageNet dataset.
Figure 4.2: Left: Siberian husky. Right: Eskimo dog.
When working with large datasets that cover many class labels with similar characteristics, we
often examine the rank-5 accuracy as an extension to the rank-1 accuracy to see how our network is
performing. In an ideal world our rank-1 accuracy would increase at the same rate as our rank-5
accuracy, but on challenging datasets, this is not always the case.
Therefore, we examine the rank-5 accuracy as well to ensure that our network is still “learning”
in later epochs. It may be the case where rank-1 accuracy stagnates towards the end of training,
but rank-5 accuracy continues to improve as our network learns more discriminating features (but
not discriminative enough to overtake the top #1 predictions). Finally, depending on the image
classification challenge (ImageNet being the canonical example), you are required to report both
your rank-1 and rank-5 accuracies together.
4.1.1
Measuring rank-1 and rank-5 Accuracies
Computing rank-1 and rank-5 accuracy can be accomplished by building a simple utility function.
Inside our pyimagesearch module we’ll add this functionality to the utils sub-module by adding
a file named ranked.py:
--- pyimagesearch
|
|--- __init__.py
|
|--- callbacks
|
|--- io
|
|--- nn
|
|--- preprocessing
|
|--- utils
|
|
|--- __init__.py
|
|
|--- captchahelper.py
|
|
|--- ranked.py
Open up ranked.py and we’ll define the rank5_accuracy function:
1
2
# import the necessary packages
import numpy as np
3
4
def rank5_accuracy(preds, labels):52
5
6
7
Chapter 4. Understanding rank-1 & rank-5 Accuracies
# initialize the rank-1 and rank-5 accuracies
rank1 = 0
rank5 = 0
Line 4 defines our rank5_accuracy function. This method accepts two parameters:
• preds: An N × T matrix where N, the number of rows, contains the probabilities associated
with each class label T .
• labels: The ground-truth labels for the images in the dataset.
We then initialize the rank-1 and rank-5 accuracies on Lines 6 and 7, respectively.
Let’s go ahead and compute the rank-1 and rank-5 accuracies:
9
10
11
12
13
14
# loop over the predictions and ground-truth labels
for (p, gt) in zip(preds, labels):
# sort the probabilities by their index in descending
# order so that the more confident guesses are at the
# front of the list
p = np.argsort(p)[::-1]
15
16
17
18
19
# check if the ground-truth label is in the top-5
# predictions
if gt in p[:5]:
rank5 += 1
20
21
22
23
# check to see if the ground-truth is the #1 prediction
if gt == p[0]:
rank1 += 1
On Line 10 we start looping over the predictions and ground-truth class labels for each example
in the dataset. Line 14 sorts the probabilities of the predictions p in descending order, such that the
indices of the largest probabilities are placed at the front of the list. If the ground-truth label exists
in the top-5 predictions, we increment our rank5 variable (Lines 18 and 19). If the ground-truth
label is equal to the number one position, we increment our rank1 variable (Lines 22 and 23).
Our final code block handles converting rank1 and rank5 to percentages by dividing by the
total number of labels:
25
26
27
# compute the final rank-1 and rank-5 accuracies
rank1 /= float(len(labels))
rank5 /= float(len(labels))
28
29
30
# return a tuple of the rank-1 and rank-5 accuracies
return (rank1, rank5)
Line 30 returns a 2-tuple of the rank-1 and rank-5 accuracies to the calling function.
4.1.2
Implementing Ranked Accuracy
To demonstrate how to compute rank-1 and rank-5 accuracy for a dataset, let’s go back to Chapter
3 where we used a pre-trained Convolutional Neural Network on the ImageNet dataset as a feature
extractor. Based on these extracted features we trained a Logistic Regression classifier on the data
and evaluated the model. We’ll now extend our accuracy reports to include rank-5 accuracy as well.
While we are computing rank-1 and rank-5 accuracy for our Logistic Regression model, keep
in mind that both rank-1 and rank-5 accuracy can be computed for any machine learning, neural4.1 Ranked Accuracy
53
network, or deep learning model – it is common to run into both of these metrics outside of the
deep learning community. With all that said, open up a new file, name it rank_accuracy.py, and
insert the following code:
1
2
3
4
5
# import the necessary packages
from pyimagesearch.utils.ranked import rank5_accuracy
import argparse
import pickle
import h5py
Lines 2-5 import our required Python packages. We’ll be using our newly defined rank5_accuracy
function to compute the rank-1 and rank-5 accuracies of our predictions, respectively. The pickle
package is used to load our pre-trained scikit-learn classifier from disk. Finally, h5py will be used
to interface with our HDF5 database of features extracted from our CNN in Chapter 3.
The next step is to parse our command line arguments:
7
8
9
10
11
12
13
# construct the argument parse and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-d", "--db", required=True,
help="path HDF5 database")
ap.add_argument("-m", "--model", required=True,
help="path to pre-trained model")
args = vars(ap.parse_args())
Our script will require two arguments: --db, which is the path to our HDF5 database of
extracted features sand --model, the path to our pre-trained Logistic Regression classifier.
The next code block handles loading the pre-trained model from disk as well as determining
the index of the training and testing split into the HDF5 dataset, assuming that 75% of the data was
used for training and 25% for testing:
15
16
17
# load the pre-trained model
print("[INFO] loading pre-trained model...")
model = pickle.loads(open(args["model"], "rb").read())
18
19
20
21
22
23
# open the HDF5 database for reading then determine the index of
# the training and testing split, provided that this data was
# already shuffled *prior* to writing it to disk
db = h5py.File(args["db"], "r")
i = int(db["labels"].shape[0] * 0.75)
Finally, let’s compute our rank-1 and rank-5 accuracies:
25
26
27
28
29
# make predictions on the testing set then compute the rank-1
# and rank-5 accuracies
print("[INFO] predicting...")
preds = model.predict_proba(db["features"][i:])
(rank1, rank5) = rank5_accuracy(preds, db["labels"][i:])
30
31
32
# display the rank-1 and rank-5 accuracies
print("[INFO] rank-1: {:.2f}%".format(rank1 * 100))Chapter 4. Understanding rank-1 & rank-5 Accuracies
54
33
print("[INFO] rank-5: {:.2f}%".format(rank5 * 100))
34
35
36
# close the database
db.close()
Line 28 computes the probabilities for each class label for every data point in the testing
set. Based on the predicted probabilities and the ground-truth labels of the testing data, we can
compute the ranked accuracies on Line 29. Lines 32 and 33 then display the rank-1 and rank-5
to our terminal, respectively.
Please take note that we have coded this example such that it will work with any example from
Chapter 3 where we extracted features from a CNN and then trained a scikit-learn model on top of
the features. Later in the Practitioner Bundle and ImageNet Bundle, we’ll compute the rank-1 and
rank-5 accuracies for Convolutional Neural Networks trained from scratch as well.
4.1.3
Ranked Accuracy on Flowers-17
To start, let’s compute the rank-1 and rank-5 accuracy for the Flowers-17 dataset:
$ python rank_accuracy.py --db ../datasets/flowers17/hdf5/features.hdf5 \
--model ../chapter03-feature_extraction/flowers17.cpickle
[INFO] loading pre-trained model...
[INFO] predicting...
[INFO] rank-1: 92.06%
[INFO] rank-5: 99.41%
On the Flowers-17 dataset, we obtain 92.06% rank-1 accuracy using a Logistic Regression
classifier trained on features extracted from the VGG16 architecture. Examining the rank-5 accuracy
we see that our classifier is nearly perfect, obtaining 99.41% rank-5 accuracy.
4.1.4
Ranked Accuracy on CALTECH-101
Let’s try another example, this one on the larger CALTECH-101 dataset:
$ python rank_accuracy.py --db ../datasets/caltech101/hdf5/features.hdf5 \
--model ../chapter03-feature_extraction/caltech101.cpickle
[INFO] loading pre-trained model...
[INFO] predicting...
[INFO] rank-1: 95.58%
[INFO] rank-5: 99.45%
Here we obtain 95.58% rank-1 accuracy and 99.45% rank-5 accuracy, a substantial improve-
ment from previous computer vision and machine learning techniques that struggled to break 60%
classification accuracy.
4.2
Summary
In this chapter, we reviewed the concept of rank-1 and rank-5 accuracy. Rank-1 accuracy is the
number of times our ground-truth label equals our class label with the largest probability. Rank-5
accuracy extends on rank-1 accuracy, allowing it to be a bit more “lenient” – here we compute
rank-5 accuracy as the number of times our ground-truth label appears in the top-5 predicted class
labels with the largest probability.4.2 Summary
55
We typically report rank-5 accuracy on large, challenging datasets such as ImageNet where it is
often hard for even humans to correctly label the image. In this case, we’ll consider a prediction for
our model to be “correct” if the ground-truth label simply exists in its top-5 predictions. As we
discussed in Chapter 9 of the Starter Bundle, a network that is truly generalizing well will produce
contextually similar predictions in its top-5 probabilities.
Finally, keep in mind that rank-1 and rank-5 accuracy are not specific to deep learning and
image classification – you will often see these metrics in other classification tasks as well.5. Fine-tuning Networks
In Chapter 3 we learned how to treat a pre-trained Convolutional Neural Network as feature
extractor. Using this feature extractor, we forward propagated our dataset of images through the
network, extracted the activations at a given layer, and saved the values to disk. A standard machine
learning classifier (in this case, Logistic Regression) was then trained on top of the CNN features,
exactly as we would do if we were using hand-engineered features such as SIFT [15], HOG [14],
LBPs [16], etc. This CNN feature extractor approach, called transfer learning, obtained remarkable
accuracy, far higher than any of our previous experiments on the Animals, CALTECH-101, or
Flowers-17 dataset.
But there is another type of transfer learning, one that can actually outperform the feature
extraction method if you have sufficient data. This method is called fine-tuning and requires us
to perform “network surgery”. First, we take a scalpel and cut off the final set of fully-connected
layers (i.e., the “head” of the network) from a pre-trained Convolutional Neural Network, such as
VGG, ResNet, or Inception. We then replace the head with a new set of fully-connected layers with
random initializations. From there all layers below the head are frozen so their weights cannot be
updated (i.e., the backward pass in backpropagation does not reach them).
We then train the network using a very small learning rate so the new set of FC layers can start
to learn patterns from the previously learned CONV layers earlier in the network. Optionally, we
may unfreeze the rest of the network and continue training. Applying fine-tuning allows us to apply
pre-trained networks to recognize classes that they were not originally trained on; furthermore, this
method can lead to higher accuracy than feature extraction.
In the remainder of this chapter we’ll discuss the fine-tuning in more detail, including network
surgery. We’ll end by providing an example of applying fine-tuning to the Flowers-17 dataset and
outperforming all other approaches we’ve tried in this book thus far.
5.1
Transfer Learning and Fine-tuning
Fine-tuning is a type of transfer learning. We apply fine-tuning to deep learning models that have
already been trained on a given dataset. Typically, these networks are state-of-the-art architectures
such as VGG, ResNet, and Inception that have been trained on the ImageNet dataset.58
Chapter 5. Fine-tuning Networks
As we found out in Chapter 3 on feature extraction, these networks contain rich, discriminative
filters that can be used on datasets and class labels outside the ones they have already been trained
on. However, instead of simply applying feature extraction, we are going to perform network
surgery and modify the actual architecture so we can re-train parts of the network.
If this sounds like something out of a bad horror movie; don’t worry, there won’t be any blood
and gore – but we will have some fun and learn a lot with our experiments. To understand how fine-
tuning works, consider Figure 5.1 (left) where we have the layers of the VGG16 network. As we
know, the final set of layers (i.e., the “head”) are our fully-connected layers along with our softmax
classifier. When performing fine-tuning, we actually remove the head from the network, just as
in feature extraction (middle). However, unlike feature extraction, when we perform fine-tuning
we actually build a new fully-connected head and place it on top of the original architecture
(right).
Figure 5.1: Left: The original VGG16 network architecture. Middle: Removing the FC layers
from VGG16 and treating the final POOL layer as a feature extractor. Right: Removing the original
FC layers and replacing them with a brand new FC head. These new FC layers can then be fine-tuned
to the specific dataset (the old FC layers are no longer used).
In most cases your new FC head will have fewer parameters than the original one; however, that
really depends on your particular dataset. The new FC head is randomly initialized (just like any
other layer in a new network) and connected to the body of the original network, and we are ready
to train.
However, there is a problem – our CONV layers have already learned rich, discriminating filters
while our FC layers are brand new and totally random. If we allow the gradient to backpropagate
from these random values all the way through the body of our network, we risk destroying these5.1 Transfer Learning and Fine-tuning
59
powerful features. To circumvent this, we instead let our FC head “warm up” by (ironically)
“freezing” all layers in the body of the network (I told you the cadaver analogy works well here) as
in Figure 5.2 (left).
Figure 5.2: Left: When we start the fine-tuning process we freeze all CONV layers in the network
and only allow the gradient to backpropagate through the FC layers. Doing this allows our network
to “warm up”. Right: After the FC layers have had a chance to warm up we may choose to unfreeze
all layers in the network and allow each of them to be fine-tuned as well.
Training data is forward propagated through the network as we normally would; however, the
backpropagation is stopped after the FC layers, which allows these layers to start to learn patterns
from the highly discriminative CONV layers. In some cases, we may never unfreeze the body of the
network as our new FC head may obtain sufficient accuracy. However, for some datasets it is often
advantageous to allow the original CONV layers to be modified during the fine-tuning process as
well (Figure 5.2, right).
After the FC head has started to learn patterns in our dataset, pause training, unfreeze the body,
and then continue the training, but with a very small learning rate – we do not want to deviate our
CONV filters dramatically. Training is then allowed to continue until sufficient accuracy is obtained.
Fine-tuning is a super powerful method to obtain image classifiers from pre-trained CNNs on
custom datasets, even more powerful than feature extraction in most cases. The downside is that
fine-tuning can require a bit more work and your choice in FC head parameters does play a big part
in network accuracy – you can’t rely strictly on regularization techniques here as your network has
already been pre-trained and you can’t deviate from the regularization already being performed by
the network.
Secondly, for small datasets, it can be challenging to get your network to start “learning” fromChapter 5. Fine-tuning Networks
60
a “cold” FC start, which is why we freeze the body of the network first. Even still, getting past
the warm-up stage can be a bit of a challenge and might require you to use optimizers other than
SGD (covered in Chapter 7). While fine-tuning does require a bit more effort, if it is done correctly,
you’ll nearly always enjoy higher accuracy.
5.1.1
Indexes and Layers
Prior to performing network surgery, we need to know the layer name and index of every layer
in a given deep learning model. We need this information as we’ll be required to “freeze” and
“unfreeze” certain layers in a pre-trained CNN. Without knowing the layer names and indexes ahead
of time, we would be “cutting blindly”, an out-of-control surgeon with no game plan. If we instead
take a few minutes to examine the network architecture and implementation, we can better prepare
for our surgery.
Let’s go ahead and take a look at the layer names and indexes in VGG16. Open up a new file,
name it inspect_model.py, and insert the following code:
1
2
3
# import the necessary packages
from keras.applications import VGG16
import argparse
4
5
6
7
8
9
# construct the argument parse and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-i", "--include-top", type=int, default=1,
help="whether or not to include top of CNN")
args = vars(ap.parse_args())
10
11
12
13
14
15
# load the VGG16 network
print("[INFO] loading network...")
model = VGG16(weights="imagenet",
include_top=args["include_top"] > 0)
print("[INFO] showing layers...")
16
17
18
19
20
# loop over the layers in the network and display them to the
# console
for (i, layer) in enumerate(model.layers):
print("[INFO] {}\t{}".format(i, layer.__class__.__name__))
Lines 2 imports our VGG16 Keras implementation, the network we’ll be examining and prepping
for surgery. Lines 6-9 parse our command line arguments. A single switch is needed here
--include-top, which is used to indicate if the head of the network should be included in the
model summary.
Lines 23 and 24 load VGG16 with pre-trained ImageNet weights from disk – the head of the
network is optionally included. Finally, Lines 19 and 20 allow us to investigate our model.
For each layer in the network, we print its corresponding index, i. Given this information,
we’ll know the index of where the FC head starts (and where to replace it with our new FC head).
To investigate the VGG16 architecture, just execute the following command:
$ python inspect_model.py
[INFO] showing layers...
[INFO] 0
InputLayer
[INFO] 1
Conv2D
[INFO] 2
Conv2D5.1 Transfer Learning and Fine-tuning
[INFO]
[INFO]
[INFO]
[INFO]
[INFO]
[INFO]
[INFO]
[INFO]
[INFO]
[INFO]
[INFO]
[INFO]
[INFO]
[INFO]
[INFO]
[INFO]
[INFO]
[INFO]
[INFO]
[INFO]
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
61
MaxPooling2D
Conv2D
Conv2D
MaxPooling2D
Conv2D
Conv2D
Conv2D
MaxPooling2D
Conv2D
Conv2D
Conv2D
MaxPooling2D
Conv2D
Conv2D
Conv2D
MaxPooling2D
Flatten
Dense
Dense
Dense
Here we can see that Layers 20-22 are our fully-connected layers. To verify, let’s re-execute the
inspect_model.py script, this time supplying the switch --include-top -1 which will leave
off the FC head:
$ python inspect_model.py --include-top -1
[INFO] showing layers...
[INFO] 0
InputLayer
[INFO] 1
Conv2D
[INFO] 2
Conv2D
[INFO] 3
MaxPooling2D
[INFO] 4
Conv2D
[INFO] 5
Conv2D
[INFO] 6
MaxPooling2D
[INFO] 7
Conv2D
[INFO] 8
Conv2D
[INFO] 9
Conv2D
[INFO] 10
MaxPooling2D
[INFO] 11
Conv2D
[INFO] 12
Conv2D
[INFO] 13
Conv2D
[INFO] 14
MaxPooling2D
[INFO] 15
Conv2D
[INFO] 16
Conv2D
[INFO] 17
Conv2D
[INFO] 18
MaxPooling2D
Notice how the final layer in the network is now the POOL layer (just like in Chapter 3 on feature
extraction). This body of the network will serve as a starting point for fine-tuning.
5.1.2
Network Surgery
Before we can replace the head of a pre-trained CNN, we need something to replace it with –
therefore, we need to define our own fully-connected head of the network. To start, create a new
file named fcheadnet.py in the nn.conv sub-module of pyimagesearch:Chapter 5. Fine-tuning Networks
62
--- pyimagesearch
|
|--- __init__.py
|
|--- callbacks
|
|--- io
|
|--- nn
|
|
|--- __init__.py
|
|
|--- conv
|
|
|
|--- __init__.py
|
|
|
|--- lenet.py
|
|
|
|--- minivggnet.py
|
|
|
|--- fcheadnet.py
|
|
|
|--- shallownet.py
|
|--- preprocessing
|
|--- utils
Then, open up fcheadnet.py and insert the following code:
1
2
3
4
# import the necessary
from keras.layers.core
from keras.layers.core
from keras.layers.core
packages
import Dropout
import Flatten
import Dense
Lines 2-4 import our required Python packages. As you can see, these three packages are
typically only used for fully-connected networks (the exception being the dropout layer).
Next, let’s define the FCHeadNet class:
6
7
8
9
10
11
12
13
14
class FCHeadNet:
@staticmethod
def build(baseModel, classes, D):
# initialize the head model that will be placed on top of
# the base, then add a FC layer
headModel = baseModel.output
headModel = Flatten(name="flatten")(headModel)
headModel = Dense(D, activation="relu")(headModel)
headModel = Dropout(0.5)(headModel)
15
16
17
# add a softmax layer
headModel = Dense(classes, activation="softmax")(headModel)
18
19
20
# return the model
return headModel
Just as in our previous network implementations, we define the build method responsible
for constructing the actual network architecture. This method requires three parameters: the
baseModel (the body of the network), the total number of classes in our dataset, and finally D,
the number of nodes in the fully-connected layer.
Line 11 initializes the headModel which is responsible for connecting our network with the
rest of the body, baseModel.output. From there Lines 12-17 build a very simple fully-connected
architecture of:
INPUT => FC => RELU => DO => FC => SOFTMAX5.1 Transfer Learning and Fine-tuning
63
Again, this fully-connected head is very simplistic compared to the original head from VGG16
which consists of two sets of 4,096 FC layers. However, for most fine-tuning problems you are not
seeking to replicate the original head of the network, but rather simplify it so it is easier to fine-tune
– the fewer parameters in the head, the more likely we’ll be to correctly tune the network to a new
classification task. Finally, Line 20 returns the newly constructed FC head to the calling function.
As we’ll see in the next section, we’ll be replacing the head of VGG16 with our newly defined
FCHeadNet via network surgery.
5.1.3
Fine-tuning, from Start to Finish
It is now time to apply fine-tuning from start to finish. Open up a new file, name it finetune_flowers17.py,
and insert the following code:
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
# import the necessary packages
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from pyimagesearch.preprocessing import ImageToArrayPreprocessor
from pyimagesearch.preprocessing import AspectAwarePreprocessor
from pyimagesearch.datasets import SimpleDatasetLoader
from pyimagesearch.nn.conv import FCHeadNet
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import RMSprop
from keras.optimizers import SGD
from keras.applications import VGG16
from keras.layers import Input
from keras.models import Model
from imutils import paths
import numpy as np
import argparse
import os
Lines 2-18 require importing our Python packages, more packages that we have seen before in
our previous examples (although many of them we are already familiar with). Lines 5-7 import our
image preprocessors along with a our dataset load. Line 8 imports our newly defined FCHeadNet to
replace the head of VGG16 (Line 12). Importing the ImageDataGenerator class on Line 9 implies
that we’ll be applying data augmentation to our dataset.
Lines 10 and 11 import our optimizers required for our network to actually learn patterns from
the input data. We’re already quite familiar with SGD, but we haven’t yet covered RMSprop –
we’ll save a discussion on advanced optimization techniques until Chapter 7, but for the time being
simply understand that RMSprop is frequently used in situations where we need to quickly obtain
reasonable performance (as is the case when we are trying to “warm up” a set of FC layers).
Lines 13 and 14 import two classes required when applying fine-tuning with Keras – Input
and Model. We’ll need both of these when performing network surgery.
Let’s go ahead and parse our command line arguments:
20
21
22
23
24
# construct the argument parse and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-d", "--dataset", required=True,
help="path to input dataset")
ap.add_argument("-m", "--model", required=True,Chapter 5. Fine-tuning Networks
64
25
26
help="path to output model")
args = vars(ap.parse_args())
We’ll require two command line arguments for our script, --dataset, the path to the input
directory containing the Flowers-17 dataset, and --model, the path to our output serialized weights
after training.
We can also initialize ImageDataGenerator, responsible for performing data augmentation
when training our network:
28
29
30
31
# construct the image generator for data augmentation
aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,
height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,
horizontal_flip=True, fill_mode="nearest")
As I mentioned in Chapter 2, in nearly all cases you should be applying data augmentation as
it rarely hurts accuracy and often helps increase it and avoid overfitting. The same is especially
true for fine-tuning when we might not have enough data to train a deep CNN from scratch.
The next code block handles grabbing the imagePaths from disk along with parsing the
classNames from the file paths:
33
34
35
36
37
38
# grab the list of images that we’ll be describing, then extract
# the class label names from the image paths
print("[INFO] loading images...")
imagePaths = list(paths.list_images(args["dataset"]))
classNames = [pt.split(os.path.sep)[-2] for pt in imagePaths]
classNames = [str(x) for x in np.unique(classNames)]
Again, we make the assumption that our input dataset has the following directory structure:
dataset_name/{class_name}/example.jpg
Therefore, we can use the path separator to easily (and conveniently) extract the class label
from the file path.
We are now ready to load our image dataset from disk:
40
41
42
# initialize the image preprocessors
aap = AspectAwarePreprocessor(224, 224)
iap = ImageToArrayPreprocessor()
43
44
45
46
47
48
# load the dataset from disk then scale the raw pixel intensities to
# the range [0, 1]
sdl = SimpleDatasetLoader(preprocessors=[aap, iap])
(data, labels) = sdl.load(imagePaths, verbose=500)
data = data.astype("float") / 255.0
Lines 41 and 42 initialize our image preprocessors. We’ll be resizing all input images to
224 × 224 pixels (maintaining the original aspect ratio of the image), the required input size for
the VGG16 network. Lines 46 and 47 then apply our image preprocessors to load the data and
labels from disk.
Next, let’s create our training and testing splits (75% of the data for training, 25% for testing)
and one-hot encode the labels:5.1 Transfer Learning and Fine-tuning
50
51
52
53
65
# partition the data into training and testing splits using 75% of
# the data for training and the remaining 25% for testing
(trainX, testX, trainY, testY) = train_test_split(data, labels,
test_size=0.25, random_state=42)
54
55
56
57
# convert the labels from integers to vectors
trainY = LabelBinarizer().fit_transform(trainY)
testY = LabelBinarizer().fit_transform(testY)
Here comes the fun part – performing network surgery:
59
60
61
62
# load the VGG16 network, ensuring the head FC layer sets are left
# off
baseModel = VGG16(weights="imagenet", include_top=False,
input_tensor=Input(shape=(224, 224, 3)))
63
64
65
66
# initialize the new head of the network, a set of FC layers
# followed by a softmax classifier
headModel = FCHeadNet.build(baseModel, len(classNames), 256)
67
68
69
70
# place the head FC model on top of the base model -- this will
# become the actual model we will train
model = Model(inputs=baseModel.input, outputs=headModel)
Lines 61 and 62 load the VGG16 architecture from disk using the supplied, pre-trained
ImageNet weights. We purposely leave off the head of VGG16 as we’ll be replacing it with our
own FCHeadNet. We also want to explicitly define the input_tensor to be 224 × 224 × 3 pixels
(again, assuming channeling ordering) otherwise we’ll run into errors when trying to train our
network as the shapes of the volumes will not match up.
Line 66 instantiates the FCHeadNet using the baseModel body as input, len(classNames)
as the total number of class labels (17 in the case of Flowers-17), along with 256 nodes in the FC
layer.
The actual “surgery” is performed on Line 70 where we construct a new model using the body
of VGG16 (baseModel.input) as the input and the headModel as the output. However, we’re
not ready to train our network yet – keep in mind that earlier in this chapter, I mentioned we need
to freeze the weights in the body so they are not updated during the backpropagation phase.
We can accomplish this freezing by setting the .trainable parameter to False for every layer
in baseModel:
72
73
74
75
# loop over all layers in the base model and freeze them so they
# will *not* be updated during the training process
for layer in baseModel.layers:
layer.trainable = False
Now that we’ve connected the head to the body and frozen the layers in the body, we can warm
up the new head of the network:
77
78
79
# compile our model (this needs to be done after our setting our
# layers to being non-trainable
print("[INFO] compiling model...")66
80
81
82
Chapter 5. Fine-tuning Networks
opt = RMSprop(lr=0.001)
model.compile(loss="categorical_crossentropy", optimizer=opt,
metrics=["accuracy"])
83
84
85
86
87
88
89
90
91
# train the head of the network for a few epochs (all other
# layers are frozen) -- this will allow the new FC layers to
# start to become initialized with actual "learned" values
# versus pure random
print("[INFO] training head...")
model.fit_generator(aug.flow(trainX, trainY, batch_size=32),
validation_data=(testX, testY), epochs=25,
steps_per_epoch=len(trainX) // 32, verbose=1)
Line 80 initializes the RMSprop optimizer, an algorithm we’ll discuss more in Chapter 7.
Notice how we are using a small learning rate of 1e − 3 to warm up the FC head. When applying
fine-tuning you’ll nearly always use a learning rate that is one, if not multiple, orders of magnitude
smaller than the original learning rate used to train the network.
Lines 88-91 then train our new FC head using our data augmentation method. Again, keep
in mind that while each image is being fully forward propagated, the gradients are only being
partially backpropagated – the backpropagation ends after the FC layers, as our goal here is to only
“warm-up” the head and not change the weights in the body of the network. Here we allow the
warm-up phase to train for 25 epochs. Typically you’ll allow your own FC head to warmup for
10-30 epochs, depending on your dataset.
After the warm-up phase, we’ll pause to evaluate network performance on the testing set:
93
94
95
96
97
# evaluate the network after initialization
print("[INFO] evaluating after initialization...")
predictions = model.predict(testX, batch_size=32)
print(classification_report(testY.argmax(axis=1),
predictions.argmax(axis=1), target_names=classNames))
The above code will allow us to compare the effects of fine-tuning before and after allowing
the head to warm up.
Now that our FC layers have been partly trained and initialized, let’s unfreeze some of the CONV
layers in the body and make them trainable:
99
100
101
102
# now that the head FC layers have been trained/initialized, lets
# unfreeze the final set of CONV layers and make them trainable
for layer in baseModel.layers[15:]:
layer.trainable = True
Making a given layer in the body trainable again is an example of setting the parameter
.trainable to True for the given layer. In some cases you’ll want to allow the entire body to be
trainable; however, for deeper architectures with many parameters such as VGG, I suggest only
unfreezing the top CONV layers and then continuing training. If classification accuracy continues to
improve (without overfitting), you may want to consider unfreezing more layers in the body.
At this point we should have a warm start to training, so we’ll switch over to SGD (again with
a small learning rate) and continue training:5.1 Transfer Learning and Fine-tuning
104
105
106
107
108
109
67
# for the changes to the model to take affect we need to recompile
# the model, this time using SGD with a *very* small learning rate
print("[INFO] re-compiling model...")
opt = SGD(lr=0.001)
model.compile(loss="categorical_crossentropy", optimizer=opt,
metrics=["accuracy"])
110
111
112
113
114
115
116
# train the model again, this time fine-tuning *both* the final set
# of CONV layers along with our set of FC layers
print("[INFO] fine-tuning model...")
model.fit_generator(aug.flow(trainX, trainY, batch_size=32),
validation_data=(testX, testY), epochs=100,
steps_per_epoch=len(trainX) // 32, verbose=1)
This time we permit our network to train over 100 epochs, allowing the CONV filters to adapt to
the underlying patterns in the training data.
Finally, we can evaluate our fine-tuned network as well as serialize the weights to disk:
118
119
120
121
122
# evaluate the network on the fine-tuned model
print("[INFO] evaluating after fine-tuning...")
predictions = model.predict(testX, batch_size=32)
print(classification_report(testY.argmax(axis=1),
predictions.argmax(axis=1), target_names=classNames))
123
124
125
126
# save the model to disk
print("[INFO] serializing model...")
model.save(args["model"])
To perform network surgery and fine-tune VGG16 on the Flowers-17 dataset, just execute the
following command:
$ python finetune_flowers17.py --dataset ../datasets/flowers17/images \
--model flowers17.model
[INFO] loading images...
[INFO] processed 500/1360
[INFO] processed 1000/1360
[INFO] compiling model...
[INFO] training head...
Epoch 1/25
10s - loss: 4.8957 - acc: 0.1510 - val_loss: 2.1650 - val_acc: 0.3618
...
Epoch 10/25
10s - loss: 1.1318 - acc: 0.6245 - val_loss: 0.5132 - val_acc: 0.8441
...
Epoch 23/25
10s - loss: 0.7203 - acc: 0.7598 - val_loss: 0.4679 - val_acc: 0.8529
Epoch 24/25
10s - loss: 0.7355 - acc: 0.7520 - val_loss: 0.4268 - val_acc: 0.8853
Epoch 25/25
10s - loss: 0.7504 - acc: 0.7598 - val_loss: 0.3981 - val_acc: 0.8971
[INFO] evaluating after initialization...
precision
recall f1-score
supportChapter 5. Fine-tuning Networks
68
bluebell
buttercup
coltsfoot
cowslip
crocus
daffodil
daisy
dandelion
fritillary
iris
lilyvalley
pansy
snowdrop
sunflower
tigerlily
tulip
windflower 0.75
0.94
0.94
0.70
1.00
0.87
0.90
0.96
1.00
1.00
0.93
0.83
0.88
1.00
0.90
0.86
0.83 1.00
0.85
0.85
0.78
0.80
0.96
0.95
0.96
0.86
0.95
0.93
1.00
0.96
0.96
1.00
0.38
0.94 0.86
0.89
0.89
0.74
0.89
0.91
0.93
0.96
0.93
0.98
0.93
0.90
0.92
0.98
0.95
0.52
0.88 18
20
20
18
20
27
20
23
22
21
15
19
23
23
19
16
16
avg / total 0.90 0.90 0.89 340
Notice how our initial accuracy is extremely low for the first epoch (≈ 36%) during the warm
up phase. This result is due to the fact that the FC layers in our new head are randomly initialized
and still trying to learn the patterns from the previously trained CONV filters. However, accuracy
quickly rises – by epoch 10 we are above 80% classification accuracy, and by the end of epoch 25
we have reached almost 90% accuracy.
Now that our FCHeadNet has obtained a warm start, we switch over to SGD and unfreeze the
first set of CONV layers in the body, allowing the network to train for another 100 epochs. Accuracy
continues to improve, all the way to 95% classification accuracy, higher than the 93% we obtained
using feature extraction:
1
2
3
4
5
6
7
8
9
10
11
12
...
[INFO] re-compiling model...
[INFO] fine-tuning model...
Epoch 1/100
12s - loss: 0.5127 - acc: 0.8147 - val_loss:
...
Epoch 99/100
12s - loss: 0.1746 - acc: 0.9373 - val_loss:
Epoch 100/100
12s - loss: 0.1845 - acc: 0.9402 - val_loss:
[INFO] evaluating after fine-tuning...
precision
recall f1-score
0.3640 - val_acc: 0.8912
0.2286 - val_acc: 0.9265
0.2019 - val_acc: 0.9412
support
13
14
15
16
17
18
19
20
21
22
23
24
bluebell
buttercup
coltsfoot
cowslip
crocus
daffodil
daisy
dandelion
fritillary
iris
lilyvalley
0.94
0.95
1.00
0.85
0.90
1.00
1.00
0.96
1.00
1.00
1.00
0.94
1.00
0.90
0.94
0.90
0.78
0.95
1.00
0.95
0.95
0.93
0.94
0.98
0.95
0.89
0.90
0.88
0.97
0.98
0.98
0.98
0.97
18
20
20
18
20
27
20
23
22
21
155.2 Summary
25
26
27
28
29
30
69
pansy
snowdrop
sunflower
tigerlily
tulip
windflower 1.00
0.92
0.96
0.90
0.70
0.94 1.00
0.96
1.00
1.00
0.88
0.94 1.00
0.94
0.98
0.95
0.78
0.94 19
23
23
19
16
16
avg / total 0.95 0.94 0.94 340
31
32
Additional accuracy can be obtained by performing more aggressive data augmentation and
continually unfreezing more and more CONV blocks in VGG16. While fine-tuning is certainly more
work than feature extraction, it also enables us to tune and modify the weights in our CNN to a
particular dataset – something that feature extraction does not allow. Thus, when given enough
training data, consider applying fine-tuning as you’ll likely obtain higher classification accuracy
than simple feature extraction alone.
5.2
Summary
In this chapter, we discussed the second type of transfer learning, fine-tuning. Fine-tuning works by
replacing the fully-connected head of a network with a new, randomly initialized head. The layers
in the body of the original network are frozen while we train the new FC layers.
Once our network starts to obtain reasonable accuracy, implying that the FC layers have started
to learn patterns from both (1) the underlying training data and (2) the previously trained CONV
filters earlier in the network, we unfreeze part (if not all) of the body – training is then allowed to
continue.
Applying fine-tuning is an extremely powerful technique as we do not have to train an en-
tire network from scratch. Instead, we can leverage pre-existing network architectures, such as
state-of-the-art models trained on the ImageNet dataset which consist of a rich, discriminative set
of filters. Using these filters, we can “jump start” our learning, allowing us to perform network
surgery, which ultimately leads to a higher accuracy transfer learning model with less effort (and
headache) than training from scratch.
For more practical examples of transfer learning and fine-tuning, be sure to refer to the ImageNet
Bundle where I demonstrate how to:
1. Recognize the make and model of a vehicle.
2. Automatically identify and correct image orientation.6. Improving Accuracy with Network Ensembles
In this chapter, we’ll explore the concept of ensemble methods, the process of taking multiple clas-
sifiers and aggregating them into one big meta-classifier. By averaging multiple machine learning
models together, we can outperform (i.e., achieve higher accuracy) by using just a single model
chosen at random. In fact, nearly all state-of-the-art publications you read that compete in the
ImageNet challenge report their best findings over ensembles of Convolutional Neural Networks.
We’ll start with this chapter with a discussion on Jensen’s Inequality – the theory ensemble
methods hinge on. From there I’ll demonstrate how to train multiple CNNs from a single script and
evaluate their performance. We’ll then combine these CNNs into a single meta-classifier and notice
an increase in accuracy.
6.1
Ensemble Methods
The term “ensemble methods” generally refers to training a “large” number of models (where
the exact value of “large” depends on the classification task) and then combining their output
predictions via voting or averaging to yield an increase in classification accuracy. In fact, ensemble
methods are hardly specific to deep learning and Convolutional Neural Networks. We’ve been
using ensemble methods for years. Techniques such as AdaBoost [18] and Random Forests [19]
are the quintessential examples of ensemble methods.
In Random Forests, we train multiple Decision Trees [20, 21] and use our forest to make
predictions. As you can see from Figure 6.2 (left), our Random Forest consists of multiple decision
trees aggregated together. Each decision tree “votes” on what it thinks the final classification should
be. These votes are tabulated by the meta-classifier, and the category with the most votes is chosen
as the final classification.
The same concept can be applied to deep learning and Convolutional Neural Networks. Here
we train multiple networks and then ask each network to return the probabilities for each class label
given an input data point (Figure 6.2, left). These probabilities are averaged together, and the final
classification is obtained. To understand why averaging predictions over multiple models works, we
first need to discuss Jensen’s Inequality. We’ll then provide Python and Keras code to implement
an ensemble of CNNs and see for ourselves that classification accuracy does indeed increase.72
Chapter 6. Improving Accuracy with Network Ensembles
Figure 6.1: A Random Forest consists of multiple decision trees. The outputs of each decision tree
are averaged together to obtain the final classification. Image reproduced from Nguyen et al. [22]
6.1.1
Jensen’s Inequality
In the most general terms, an ensemble is a finite collection of models that can be used to obtain
better average predictive accuracy than using a single model in the ensemble collection. The
seminal work of Dietterich [23] details the theory of why ensemble methods can typically obtain
higher accuracy than a single model alone.
Dietterich’s work hinges on Jensen’s Inequality, which is known as the “diversity” or the
“ambiguity decomposition” in machine learning literature. The formal definition of Jensen’s
Inequality states that the convex combined (average) ensemble will have error less than or equal to
the average error of the individual models. It may be that one individual model has a lower error
than the average of all models, but since there is no criterion that we can use to “select” this model,
we can be assured that the average of all models will perform no worse than selecting any single
model at random. In short, we can only get better by averaging our predictions together; we don’t
have to fear making our classifier worse.
For those of us who enjoy visual examples, perhaps Jensen’s Inequality and the concept of
model averaging is best explained by asking you to look at this jar of candies andguess how many
candies are inside (Figure 6.2, right).
How many candies would you guess? 100? 200? 500? Your guess might be extremely above or
below the actual number of candies in the jar. It could be very close. Or if you’re very lucky, you
might guess the exact number of candies.
However, there is a little trick to this game – and it’s based on Jensen’s Inequality. If you were to
ask me how many candies are in the jar, I would go around to you and everyone else who purchased
a copy of Deep Learning for Computer Vision with Python and ask each of them what they thought
the candy count is. I would then take all of these guesses and average them together – and I
would use this average as my final prediction.
Now, it may be that a handful of you are really good guessers and can beat the average; however,
I don’t have any criterion to determine which of you are really good guessers. Since I cannot
tell who are the best guessers, I’ll instead take the average of everyone I ask – and thereby I’m
guaranteed to do no worse (on average) than selecting any one of your guesses at random. I may
not win the candy guessing game each time we play, but I’ll always be in the running; and that, in
essence, is Jensen’s Inequality.
The difference between randomly guessing candy counts and deep learning models is that6.1 Ensemble Methods
73
Figure 6.2: Left: An ensemble of neural networks consists of multiple networks. When classifying
an input image the data point is passed to each network where it classifies the image independently
of all other networks. The classifications across networks are then averaged to obtain the final
prediction. Right: Ensemble methods are possible due to Jensen’s Inequality. By averaging guesses
as to the number of candies in the jar, we can better approximate the true number of candies.
we assume our CNNs are performing well and are good guessers (i.e., not randomly guessing).
Therefore, if we average the results of these predictors together, we’ll often see a rise in our
classification accuracy. This improvement is exactly why you see state-of-the-art publications on
deep learning train multiple models and then report their best accuracies over these ensembles.
6.1.2
Constructing an Ensemble of CNNs
The first step in building an ensemble of CNNs is to train each individual CNN. At this point in Deep
Learning for Computer Vision with Python we’ve seen many examples of training a single CNN –
but how do we train multiple networks? In general, we have two options:
1. Run the script we use to train a single network multiple times, changing the path to the output
serialized model weights to be unique for each run.
2. Create a separate Python script that uses for loop to train N networks and outputs the
serialized model at the end of each iteration.
Both methods are perfectly acceptable to train a simple ensemble of CNNs. Since we’re quite
comfortable running a single command to generate a single output CNN, let’s try the second option
where a single script is responsible for training multiple networks. Open up a new file, name it
train_models.py, and insert the following code:
1
2
3
# set the matplotlib backend so figures can be saved in the background
import matplotlib
matplotlib.use("Agg")
4
5
6
7
8
9
10
# import the necessary packages
from sklearn.preprocessing import LabelBinarizer
from sklearn.metrics import classification_report
from pyimagesearch.nn.conv import MiniVGGNet
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import SGD74
11
12
13
14
15
Chapter 6. Improving Accuracy with Network Ensembles
from keras.datasets import cifar10
import matplotlib.pyplot as plt
import numpy as np
import argparse
import os
Lines 2 and 3 import the matplotlib package and then set the backend such that we can save
plots to disk. Lines 6-15 then import our remaining Python packages. All of these packages we
have used before, but I’ll call out the important ones below:
• Line 8: We’ll be training multiple MiniVGGNet models to form our ensemble.
• Line 9: We’ll be using the ImageDataGenerator class to apply data augmentation when
training our network.
• Lines 10 and 11: Our MiniVGGNet models will be trained on the CIFAR-10 dataset using
the SGD optimizer.
The train_models.py script will require two command line arguments followed by an addi-
tional optional one:
17
18
19
20
21
22
23
24
25
# construct the argument parse and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-o", "--output", required=True,
help="path to output directory")
ap.add_argument("-m", "--models", required=True,
help="path to output models directory")
ap.add_argument("-n", "--num-models", type=int, default=5,
help="# of models to train")
args = vars(ap.parse_args())
The --output argument will serve as the base output directory where we’ll save classification
reports along with loss/accuracy plots for each of the networks we will train. We then have the
--models switch which controls the path to the output directory where we will be storing our
serialized network weights.
Finally, the --num-models argument indicates the number of networks in our ensemble. We
default this value to 5 networks. While traditional ensemble methods such as Random Forests
typically consist of > 30 Decision Trees (and in many cases > 100), we normally only see 5-10
Convolutional Neural Networks in an ensemble – the reason is due to the fact that CNNs are much
more time-consuming and computationally expensive to train.
Our next code block handles loading the CIFAR-10 dataset from disk, scaling the pixel in-
tensities to the range [0, 1], and one-hot encoding our class labels so we can apply categorical
cross-entropy as our loss function:
27
28
29
30
31
# load the training and testing data, then scale it into the
# range [0, 1]
((trainX, trainY), (testX, testY)) = cifar10.load_data()
trainX = trainX.astype("float") / 255.0
testX = testX.astype("float") / 255.0
32
33
34
35
36
37
# convert the labels from integers to vectors
lb = LabelBinarizer()
trainY = lb.fit_transform(trainY)
testY = lb.transform(testY)6.1 Ensemble Methods
38
39
40
75
# initialize the label names for the CIFAR-10 dataset
labelNames = ["airplane", "automobile", "bird", "cat", "deer",
"dog", "frog", "horse", "ship", "truck"]
We also need to initialize our ImageDataGenerator so we can apply data augmentation to the
CIFAR-10 training data:
42
43
44
45
# construct the image generator for data augmentation
aug = ImageDataGenerator(rotation_range=10, width_shift_range=0.1,
height_shift_range=0.1, horizontal_flip=True,
fill_mode="nearest")
Here we’ll allow images to be randomly rotated 10 degrees, shifted by a factor of 0.1, and
randomly horizontally flipped.
We are now ready to train each individual MiniVGGNet model in the ensemble:
47
48
49
50
51
52
53
54
55
56
57
# loop over the number of models to train
for i in np.arange(0, args["num_models"]):
# initialize the optimizer and model
print("[INFO] training model {}/{}".format(i + 1,
args["num_models"]))
opt = SGD(lr=0.01, decay=0.01 / 40, momentum=0.9,
nesterov=True)
model = MiniVGGNet.build(width=32, height=32, depth=3,
classes=10)
model.compile(loss="categorical_crossentropy", optimizer=opt,
metrics=["accuracy"])
On Line 48 we start looping over the number of --num-models to train. Line 52 initializes
the SGD optimizer using a learning rate of α = 0.01, a momentum of γ = 0.9, and a standard
Keras learning rate decay of the learning rate divided by total number of epochs (Chapter 16,
Starter Bundle). We’ll also indicate that Nesterov accelerations should be used. Lines 54-57 then
instantiate the individual MiniVGGNet model and compile it.
Next, let’s train the network and serialize it to disk:
59
60
61
62
# train the network
H = model.fit_generator(aug.flow(trainX, trainY, batch_size=64),
validation_data=(testX, testY), epochs=40,
steps_per_epoch=len(trainX) // 64, verbose=1)
63
64
65
66
# save the model to disk
p = [args["models"], "model_{}.model".format(i)]
model.save(os.path.sep.join(p))
Lines 60-62 train our MiniVGGNet model using the fit_generator method. We use fit_generator
because we need the .flow method of the ImageDataGenerator to apply data augmentation. The
network will be trained for a total of 64 epochs using batch sizes of 64. The steps_per_epoch
parameter controls the number of batches per epoch, which is simply the number of training samples
divided by our batch size.76
Chapter 6. Improving Accuracy with Network Ensembles
After the network finishes training, we construct a unique output path for it and save the weights
to disk (Lines 65 and 66). Let’s also save a classification_report to disk for each network
as well so we can review performance once the script finishes executing:
68
69
70
71
# evaluate the network
predictions = model.predict(testX, batch_size=64)
report = classification_report(testY.argmax(axis=1),
predictions.argmax(axis=1), target_names=labelNames)
72
73
74
75
76
77
# save the classification report to file
p = [args["output"], "model_{}.txt".format(i)]
f = open(os.path.sep.join(p), "w")
f.write(report)
f.close()
The same goes for plotting our loss and accuracy over time:
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
# plot the training loss and accuracy
p = [args["output"], "model_{}.png".format(i)]
plt.style.use("ggplot")
plt.figure()
plt.plot(np.arange(0, 40), H.history["loss"],
label="train_loss")
plt.plot(np.arange(0, 40), H.history["val_loss"],
label="val_loss")
plt.plot(np.arange(0, 40), H.history["acc"],
label="train_acc")
plt.plot(np.arange(0, 40), H.history["val_acc"],
label="val_acc")
plt.title("Training Loss and Accuracy for model {}".format(i))
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend()
plt.savefig(os.path.sep.join(p))
plt.close()
It’s important to note that we would never jump straight to training an ensemble – we would
first run a series of experiments to determine which combination of architecture, optimizer, and
hyperparameters yields the highest accuracy on a given dataset.
Once you’ve reached this optimal set of combination, we would then switch over to training
multiple models to form an ensemble. Training an ensemble as your very first experiment is consid-
ered premature optimization as you don’t know what combination of architecture, optimizer, and
hyperparameters will work best for your given dataset.
With that said, we know from Chapter 15 of the Starter Bundle that MiniVGGNet trained with
SGD gives a reasonable classification accuracy of 83% – by applying ensemble methods we hope
to increase this accuracy.
To train our set of MiniVGGNet models, just execute the following command:
$ python train_models.py --output output --models models
[INFO] training model 1/5
[INFO] training model 2/56.1 Ensemble Methods
77
[INFO] training model 3/5
[INFO] training model 4/5
[INFO] training model 5/5
Since we are now training five networks rather than one, it will take 5x as long for this script to
run. Once it executes, take a look at your output directory:
$ ls output/
model_0.png model_1.png
model_0.txt model_1.txt
model_2.png
model_2.txt
model_3.png
model_3.txt
model_4.png
model_4.txt
Here you will see the output classification reports and training curves for each of the networks.
Using grep we can easily extract the classification accuracy of each network:
$ grep ’avg / total’ output/*.txt
output/model_0.txt:avg / total
output/model_1.txt:avg / total
output/model_2.txt:avg / total
output/model_3.txt:avg / total
output/model_4.txt:avg / total
0.83
0.83
0.83
0.82
0.83
0.83
0.83
0.83
0.82
0.83
0.83
0.83
0.83
0.82
0.83
10000
10000
10000
10000
10000
Four of the five networks obtain 83% classification accuracy while the remaining network
reaches only 82% accuracy. Furthermore, looking at all five training plots (Figure 6.3) we can
see that each set of learning curves looks somewhat similar, although each also looks unique,
demonstrating that each MiniVGGNet model “learned” in a different manner.
Now that we’ve trained our five individual ensembles, it’s time to combine their predictions and
see if our classification accuracy increases.
6.1.3
Evaluating an Ensemble
To construct and evaluate our ensemble of CNNs, create a separate file named test_ensemble.py
and insert the following code:
1
2
3
4
5
6
7
8
9
# import the necessary packages
from sklearn.preprocessing import LabelBinarizer
from sklearn.metrics import classification_report
from keras.models import load_model
from keras.datasets import cifar10
import numpy as np
import argparse
import glob
import os
10
11
12
13
14
15
# construct the argument parse and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-m", "--models", required=True,
help="path to models directory")
args = vars(ap.parse_args())
Lines 2-9 import our required Python packages while Lines 12-15 parse our command line
arguments. We only need a single switch with here, --models, the path to where our serialized
network weights are stored on disk.78
Chapter 6. Improving Accuracy with Network Ensembles
Figure 6.3: Training and validation plots for each of the five networks in our ensemble.
From there we can load the CIFAR-10 dataset, keeping only the testing set since we are only
evaluating (and not training) our networks:
17
18
19
# load the testing data, then scale it into the range [0, 1]
(testX, testY) = cifar10.load_data()[1]
testX = testX.astype("float") / 255.0
20
21
22
23
# initialize the label names for the CIFAR-10 dataset
labelNames = ["airplane", "automobile", "bird", "cat", "deer",
"dog", "frog", "horse", "ship", "truck"]
24
25
26
27
# convert the labels from integers to vectors
lb = LabelBinarizer()
testY = lb.fit_transform(testY)6.1 Ensemble Methods
79
We now need to gather the paths to our pre-trained MiniVGGNet networks, which is easy
enough using the glob module built into Python:
29
30
31
32
33
# construct the path used to collect the models then initialize the
# models list
modelPaths = os.path.sep.join([args["models"], "*.model"])
modelPaths = list(glob.glob(modelPaths))
models = []
Line 31 constructs a wildcard path (notice the asterisk “*” in the file path) to all .model files
in the --models directory. Using glob.glob on Line 32 we can automatically find all file paths
inside --models that end with the .model file extension. After executing Line 32 our modelPaths
list now contains the following entries:
[’models/model_0.model’, ’models/model_1.model’, ’models/model_2.model’,
’models/model_3.model’, ’models/model_4.model’]
Line 33 then initializes a list of models which will store the deserialized MiniVGGNet networks
loaded from disk.
Let’s go ahead and load each model from disk now:
35
36
37
38
39
40
# loop over the model paths, loading the model, and adding it to
# the list of models
for (i, modelPath) in enumerate(modelPaths):
print("[INFO] loading model {}/{}".format(i + 1,
len(modelPaths)))
models.append(load_model(modelPath))
On Line 37 we loop over each of the individual modelPath file paths. We then load the
serialized network via load_model and append it to the models list.
Finally, we are ready to evaluate our ensemble:
42
43
44
# initialize the list of predictions
print("[INFO] evaluating ensemble...")
predictions = []
45
46
47
48
49
50
# loop over the models
for model in models:
# use the current model to make predictions on the testing data,
# then store these predictions in the aggregate predictions list
predictions.append(model.predict(testX, batch_size=64))
51
52
53
54
55
56
# average the probabilities across all model predictions, then show
# a classification report
predictions = np.average(predictions, axis=0)
print(classification_report(testY.argmax(axis=1),
predictions.argmax(axis=1), target_names=labelNames))
On Line 44 we initialize our list of predictions. Each model in the models list will produce
ten probabilities (one for each class label in the CIFAR-10 dataset) for every data point in the testingChapter 6. Improving Accuracy with Network Ensembles
80
set. Given that there are 10,000 data points in the CIFAR-10 dataset, each model will produce
an array of size 10, 000 × 10 – each row corresponds to a given data point and each column the
corresponding probability.
To accumulate these predictions, we loop over each individual model on Line 47. We then call
.predict on the testing data and update the predictions list with the probabilities produced by
the respective model. After we have looped over the five models in our ensemble and updated the
predictions list, our predictions array now has the shape (5, 10000, 10), implying that
there are five models, each of which produced 10 class label probabilities for each of the 10,000
testing data points. Line 54 then averages the probabilities for each testing data point across all
five models.
To see this for ourselves, we can investigate the shape of our predictions array which is now
(10000, 10) implying that probabilities for each of the five models have been averaged together.
The averaging is why we call this method an ensemble – we are taking the output of multiple,
independent models and averaging them together to obtain our final output. According to Jensen’s
Inequality, applying ensemble methods should perform no worse (on average) than selecting one of
the individual models at random.
Finally, Lines 55 and 56 display a classification report for our ensemble predictions. To
determine if our ensemble of MiniVGGNet models increased classification accuracy, execute the
following command:
$ python test_ensemble.py --models models
[INFO] loading model 1/5
[INFO] loading model 2/5
[INFO] loading model 3/5
[INFO] loading model 4/5
[INFO] loading model 5/5
[INFO] evaluating ensemble...
precision
recall f1-score
support
airplane
automobile
bird
cat
deer
dog
frog
horse
ship
truck 0.89
0.93
0.80
0.72
0.80
0.77
0.86
0.91
0.89
0.87 0.82
0.93
0.74
0.67
0.86
0.77
0.91
0.87
0.94
0.93 0.85
0.93
0.77
0.69
0.83
0.77
0.88
0.89
0.92
0.90 1000
1000
1000
1000
1000
1000
1000
1000
1000
1000
avg / total 0.84 0.84 0.84 10000
Looking at the output classification report we can see that we increased our accuracy from 83%
to 84%, simply by combining the output of multiple networks, even though these networks were
trained on the same dataset using the exact same hyperparameters. In general, you can expect an
increase of 1-5% accuracy when applying ensembles of Convolutional Neural Networks depending
on your dataset.
6.2
Summary
In this chapter, we reviewed the ensembles machine learning technique and how training multiple,
independent models followed by averaging the results together can increase classification accuracy.6.2 Summary
81
The theoretical justification for ensemble methods can be found by reviewing Jensen’s Inequality
which states that on average, we are better off averaging the results of multiple models together
rather than picking one at random.
In fact, the top results you see reported by state-of-the-art papers (including Inception [17],
ResNet [24], etc.) are the average over multiple models (typically 3-5, depending on how long the
authors had to train their networks before their publication was due). Depending on your dataset,
you can normally expect a 1-5% increase in accuracy.
While ensembles may be a simple method to improve classification accuracy they are also a
computationally expensive one – rather than training a single network, we are now responsible for
training N of them. Training a CNN is already a time-consuming operation, so a method that scales
linearly may not be practical in some situations.
To alleviate the computational burden of training multiple models, Huang et al. [25] propose
the idea of using cyclic learning rate schedules to train multiple models during a single training
process in their 2017 paper, Snapshot Ensembles: Train 1, get M for free.
This method works by starting training with a high learning rate, quickly lowering it, saving
the model weights, and then resetting the learning rate to its original value without re-initializing
network weights. This action enables the network to theoretically extend coverage to areas of
local minima (or at least areas of low loss) multiple times during the training process. Snapshot
Ensembles are outside the scope of this book but are worth investigating if you need to boost your
classification accuracy but cannot afford to train multiple models.7. Advanced Optimization Methods
So far in this book, we have only studied and used Stochastic Gradient Descent (SGD) to optimize
our networks – but there are other optimization methods that are used in deep learning. Specifically,
these more advanced optimization techniques seek to either:
1. Reduce the amount of time (i.e., number of epochs) to obtain reasonable classification
accuracy.
2. Make the network more “well-behaved” for a larger range of hyperparameters other than the
learning rate.
3. Ideally, obtain higher classification accuracy than what is possible with SGD.
With the latest incarnation of deep learning, there has been an explosion of new optimization
techniques, each seeking to improve on SGD and provide the concept of adaptive learning rates. As
we know, SGD modifies all parameters in a network equally in proportion to a given learning rate.
However, given that the learning rate of a network is (1) the most important hyperparameter to tune
and (2) a hard, tedious hyperparameter to set correctly, deep learning researchers have postulated
that it’s possible to adaptively tune the learning rate (and in some cases, per parameter) as the
network trains.
In this chapter, we’ll review adaptive learning rate methods. I’ll also provide suggestions on
which optimization algorithms you should be using in your own projects.
7.1
Adaptive Learning Rate Methods
In order to understand each of the optimization algorithms in this section, we are going to examine
them in terms of pseudocode – specifically the update step. Much of this chapter has been inspired
by the excellent overview of optimization methods by Karpathy [26] and Ruder [27]. We’ll
extend (and in some cases, simplify) their explanations of these methods to make the content more
digestible.
To get started, let’s take a look at an algorithm we are already familiar with – the update phase
of vanilla SGD:
W += -lr * dW84
Chapter 7. Advanced Optimization Methods
Here we have three values:
1. W: Our weight matrix.
2. lr: The learning rate.
3. dW: The gradient of W.
Our learning rate here is fixed and, provided it is small enough, we know our loss will decrease
during training. We’ve also seen extensions to SGD which incorporate momentum and Nesterov
acceleration in Chapter 7. Given this notation, let’s explore common adaptive learning rate
optimizers you will encounter in your deep learning career.
7.1.1
Adagrad
The first adaptive learning rate method we are going to explore is Adagrad, first introduced by
Duchi et al [28]. Adagrad adapts the learning rate to the network parameters. Larger updates are
performed on parameters that change infrequently while smaller updates are done on parameters
that change frequently.
Below we can see a pseudocode representation of the Adagrad update:
cache += (dW ** 2)
W += -lr * dW / (np.sqrt(cache) + eps)
The first parameter you’ll notice here is the cache – this variable maintains the per-parameter
sum of squared gradients and is updated at every mini-batch in the training process. By examining
the cache, we can see which parameters are updated frequently and which ones are updated
infrequently.
We can then divide the lr * dx by the square-root of the cache (adding in an epsilon value
for smoothing and preventing division by zero errors). Scaling the update by all previous sum of
square gradients allows us to adaptively update the parameters in our network.
Weights that have frequently updated/large gradients in the cache will scale the size of the
update down, effectively lowering the learning rate for the parameter. On the other hand, weights
that have infrequent updates/smaller gradients in the cache will scale up the size of the update,
effectively raising the learning rate for the specific parameter.
The primary benefit of Adagrad is that we no longer have to manually tune the learning rate –
most implementations of the Adagrad algorithm leave the initial learning rate at 0.01 and allow the
adaptive nature of the algorithm to tune the learning rate on a per-parameter basis.
However, the weakness of Adagrad can be seen by examining the cache. At each mini-batch,
the squared gradients are accumulated in the denominator. Since the gradients are squared (and
are therefore always positive), this accumulation keeps growing and growing during the training
process. As we know, dividing a small number (the gradient) by a very large number (the cache)
will result in an update that is infinitesimally small, too small for the network to actually learn
anything in later epochs
This phenomena occurs for even small, infrequently updated parameters as positive values in
the cache grows monotonically, which is why we rarely see Adagrad used to train (modern) deep
learning neural networks. However, it is important to review so we can understand the extensions
to the Adagrad algorithm.
7.1.2
Adadelta
The Adadelta algorithm was proposed by Zeiler in their 2012 paper, ADADELTA: An Adaptive
Learning Rate Method [29]. Adadelta can be seen as an extension to Adagrad that seeks to reduce
the monotonically decreasing learning rate caused by the cache.7.1 Adaptive Learning Rate Methods
85
In the Adagrad algorithm, we update our cache with all of the previously squared gradients.
However, Adadelta restricts this cache update by only accumulating a small number of past gradients
– when actually implemented, this operation amounts to computing a decaying average of all past
squared gradients.
Adadelta can thus be seen as an improvement to Adagrad; however, the very closely related
RMSprop algorithm (which also performs cache decay) is often preferred to Adadelta.
7.1.3
RMSprop
Developed independently of Adadelta, the RMSprop algorithm is an (unpublished) optimization
algorithm shown in the slides of Geoffrey Hinton’s Coursera class [2]. Similar to Adadelta,
RMSprop attempts to rectify the negative effects of a globally accumulated cache by converting the
cache into an exponentially weighted moving average.
Let’s take a look at the RMSprop pseudocode update:
cache = decay_rate * cache + (1 - decay_rate) * (dW ** 2)
W += -lr * dW / (np.sqrt(cache) + eps)
The first aspect of RMSprop you’ll notice is that the actual update to the weight matrix W is
identical to that of Adagrad – what matters here is how the cache is updated. The decay_rate,
often defined as ρ, is a hyperparameter typically set to 0.9. Here we can see that previous entries in
the cache will be weighted substantially smaller than new updates. This “moving average” aspect
of RMSprop allows the cache to “leak out" old squared gradients and replace them with newer,
“fresher” ones.
Again, the actual update to W is identical to that of Adagrad – the crux of the algorithm hinges
on exponentially decaying the cache, enabling us to avoid monotonically decreasing learning rates
during the training process. In practice, RMSprop tends to be more effective than both Adagrad and
Adadelta when applied to training a variety of deep learning networks [5]. Furthermore, RMSprop
tends to converge significantly faster than SGD.
Outside of SGD, RMSprop has arguably been the second most used optimization algorithm
in recent deep learning literature; however, the next optimization method we are about to discuss,
Adam, is now being used more than RMSprop.
7.1.4
Adam
The Adam (Adaptive Moment Estimation) optimization algorithm, proposed by Kingma and Ba in
their 2014 paper, Adam: A Method for Stochastic Optimization [1] is essentially RMSprop only
with momentum added to it:
m = beta1 * m + (1 - beta1) * dW
v = beta2 * v + (1 - beta2) * (dW ** 2)
x += -lr * m / (np.sqrt(v) + eps)
R
Again, I want to draw special attention to these pseudocode updates as they were derived and
popularized by Karpathy’s excellent optimization method notes [26].
The values of both m and v are similar to SGD momentum, relying on their respective previous
values from time t − 1. The value m represents the first moment (mean) of the gradients while v is
the second moment (variance).Chapter 7. Advanced Optimization Methods
86
The actual update to W is near identical to RMSprop, only now we are using the “smoothed”
version (due to computing the mean) of m rather than the raw gradient dW – using the mean tends to
lead to more desirable updates as we can smooth over noisy updates in the raw dW values. Typically
beta1 is set to 0.9 while beta2 is set to 0.999 – these values are rarely (if ever) changed when
using the Adam optimizer.
In practice, Adam tends to work better than RMSprop in many situations. For more details on
the Adam optimization algorithm, please see Kingma and Ba [1].
7.1.5
Nadam
Just like Adam is RMSprop with momentum, Nadam is RMSprop with Nesterov acceleration.
Nadam was proposed by Timothy Dozat, a Ph.D. student at Stanford University [30]. We typically
don’t see Nadam used “in the wild”, but is important to understand that this variation of Adam does
exist.
7.2
Choosing an Optimization Method
Given the choices between all of these optimization algorithms, which one should you choose?
Unfortunately, the answer is highly inconclusive – the work of Schaul et al. in 2014, Unit tests
for Stochastic Optimization [31], attempted to benchmark many of these optimization methods and
found that while adaptive learning rate algorithms performed favorably, there was no clear winner.
Deep learning optimization algorithms (and how to choose them) is still an open area of
research, and will likely continue to be for many years. Therefore, instead of exhaustively trying
every optimization algorithm you can find, throwing each at your dataset and noting what sticks,
it’s better to master two or three optimization algorithms. Often, the success of a deep learning
project is a combination of the optimization algorithm (and associated parameters) along with how
adept the researcher is at “driving” the algorithm.
7.2.1
Three Methods You Should Learn how to Drive: SGD, Adam, and RMSprop
“The choice of which algorithm to use, at this point, seems to depend largely
on the user’s familiarity with the algorithm (for ease of hyperparameter tuning).” –
Goodfellow et al. [5]
Given the success of adaptive learning rate algorithms such as RMSprop and Adam, you might
be tempted to simply ignore SGD and treat it like an archaic tool. After all, “better" methods exist,
right?
However, implying ignoring SGD would be a big mistake. Take a look at any recent state-of-
the-art deep learning publication on challenging image classification datasets such as ImageNet:
AlexNet [6], VGGNet [11], SqueezeNet [32], Inception [17], ResNet [33] – all of these state-of-
the-art architectures were trained using SGD.
But why is this? We can clearly see the benefits in algorithms that apply adaptive learning rates
such as RMSprop and Adam – networks can converge faster. However, the speed of convergence,
while important, is not the most important factor – hyperparameters still win out. If you cannot
tune the hyperparameters to a given optimizer (and associated model), your network will never
obtain reasonable accuracy.
While SGD certainly converges slower than adaptive learning rate algorithms, it’s also a more
studied algorithm. Researchers are more familiar with SGD and have spent years using it to train
networks.
For example, consider a professional race car driver who has been driving the same make and
model of a race car for five years. Then, one day, the driver’s sponsor changes and they are forced7.3 Summary
87
to drive a new vehicle. The driver has no time to try out the new race car, and they are forced to start
racing with no experience in the car. Will the driver perform as well in their first few races? Most
likely not – the driver is not familiar with the vehicle and its intricacies (but still might perform
reasonably as the driver is a professional after all).
The same goes for deep learning architectures and optimization algorithms. The more experi-
ments we perform with a given architecture and optimization algorithm, the more we learn about
the intricacies of the training process. Given that SGD has been the cornerstone of training neural
networks for nearly 60 years, it’s no wonder that this algorithm is still consistently used today – the
rate of convergence simply doesn’t matter (as much) when compared to the performance (accuracy)
of the model.
Simply put: If we can obtain higher accuracy on a given dataset using SGD, we’ll likely use
SGD even if it takes 1.5x longer to train than when using Adam or RMSprop simply because
we understand the hyperparameters better. Currently, the most used deep learning optimization
algorithms are:
1. SGD
2. RMSprop
3. Adam
I would recommend that you master SGD first and apply it to every architecture and dataset
you encounter. In some cases. it will perform great, and, in others, it will perform poorly. The
goal here is for you to expose yourself to as many deep learning problems as possible using a
specific optimization algorithm and learn how to tune the associated hyperparameters. Remember,
deep learning is part science and part art – mastering an optimization algorithm is absolutely an art
that requires much practice. From there, move on to either RMSprop or Adam.
I personally recommend studying Adam prior to RMSprop as, in my experience, Adam tends
to outperform RMSprop in most situations.
7.3
Summary
In this chapter, we discussed adaptive learning rate optimization algorithms that can be used in place
of SGD. Choosing an optimization algorithm to train a deep neural network is highly dependent on
your familiarity with:
1. The dataset
2. The model architecture
3. The optimization algorithm (and associated hyperparameters)
Instead of exhaustively running experiments to try every optimization algorithm you can
find, it’s instead better to master two or three techniques and how to tune their hyperparameters.
Becoming an expert at these techniques will enable you to apply new model architectures to datasets
you haven’t worked with before with much more ease.
My personal recommendation is to spend a lot of time early in your deep learning career
mastering how to use SGD; specifically, SGD with momentum. Once you feel comfortable
applying SGD to a variety of architectures and datasets, move on to Adam and RMSprop.
Finally, keep in mind that the speed of model rate convergence is secondary to loss and accuracy
– choose an optimization algorithm that you can (confidently) tune the hyperparameters to, resulting
in a reasonably performing network.8. Optimal Pathway to Apply Deep Learning
In Chapter 10 of the Starter Bundle, we examined a recipe to train a neural network. The four
ingredients to the recipe included:
1. Your dataset
2. A loss function
3. A neural network architecture
4. An optimization method
Using this recipe, we can train any type of deep learning model. However, what this recipe
does not cover is the optimal way to combine these ingredients together, as well as which parts of
the recipe you need to fiddle with if you aren’t obtaining your desired results.
As you’ll find out in your deep learning career, arguably the hardest aspect of deep learning is
examining your accuracy/loss curve and making the decision on what to do next. If your training
error is too high, what do you do? What happens if your validation error is also high? How do you
adjust your recipe when your validation error matches your training error. . . but then your testing
set error is high?
Inside this chapter, I’ll discuss the optimal way to apply deep learning techniques, starting with
rules of thumb you can use to adjust your recipe for training. I’ll then provide a decision process
that you can use when deciding if you should train your deep learning model from scratch or apply
transfer learning. By the end of this chapter, you’ll have a strong understanding of rules of thumb
that expert deep learning practitioners use when training their own networks.
8.1
A Recipe for Training
The following section is heavily inspired by Andrew Ng’s excellent tutorial at NIPS 2016 titled, Nuts
and Bolts of Building Deep Learning Applications [34]. In this talk, Ng discussed how we can get
deep learning methods to work in our own products, businesses, and academic research. Arguably
the most important takeaway from Ng’s talk follows (summarized by Malisiewicz [35]):
R
“Most issues in applied deep learning come from training data/testing data mismatch. In
some scenarios this issue just doesn’t come up, but you’d be surprised how often applied ma-90
Chapter 8. Optimal Pathway to Apply Deep Learning
chine learning projects use training data (which is easy to collect and annotate) that is
different from the target application.” – Andrew Ng (summarized by Malisiewicz)
What both Ng and Malisiewicz are saying here is that you should take excruciating care
to make sure your training data is representative of your validation and testing sets. Yes,
obtaining, annotating, and labeling a dataset is extremely time consuming and even in some cases,
very expensive. And yes, deep learning methods do tend to generalize well in certain situations.
However, you cannot expect any machine learning model trained on data that is not representative
to succeed.
For example, suppose we are tasked with the responsibility of building a deep learning system
responsible for recognizing the make and model of a vehicle from a camera mounted to our car as
we drive down the road (Figure 8.1, left).
Figure 8.1: Left: Cars on a highway that we wish to identify using deep learning. Right: Example
“product shot” images of what our network was actually trained on.
The first step is to gather our training data. To speed up the data gathering process, we decide
to scrape websites that have both photos of cars and their make and model listed on the webpage –
great examples of such websites in include Autotrader.com, eBay, CarMax, etc. For each of these
websites we can build a simple spider that crawls the website, finds individual product listings, (i.e.,
the “car pages” that list the specifications of the vehicle), and then download the images and make
+ model information.
This method is quite simplistic, and outside the time it takes us to develop the spider, it won’t
take us long to accumulate a reasonably large labeled dataset. We then split this dataset into two:
training and validation, and proceed to train a given deep learning architecture to a high accuracy
(> 90%).
However, when we apply our newly trained model to example images, such as in Figure 8.1
(left), we find that results are terrible – we are lucky to obtain 5 percent accuracy when deployed in
the real-world. Why is this?
The reason is that we took the easy way out. We didn’t stop to consider that the product shots
of cars listed on Autotrader, CarMax, and eBay (Figure 8.1, right) are not representative of the
vehicles our deep learning vision system will be seeing mounted to the dash of our car. While our
deep learning system may be great at identifying the make and model of a vehicle in a product shot,
it will fail to recognize the make an model of a car from either a frontal or rear view, as is common
when driving.
There is no shortcut to building your own image dataset. If you expect a deep learning system
to obtain high accuracy in a given real-world situation, then make sure this deep learning system
was trained on images representative of where it will be deployed – otherwise you will be very
disappointed in its performance.8.1 A Recipe for Training
91
Assuming we have gathered sufficient training data that is representative of the classification
task we are trying to solve, Andrew Ng has provided with a four step process to aid us in our
training [34].
Figure 8.2: Slide 13 of Andrew Ng’s talk [34]. Here Ng proposes four separate data splits when
training a deep learning model.
Based on Figure 8.2 we can see that Ng is proposing four sets of data splits when training a
deep learning model:
1. Training
2. Training-validation (which Ng refers to as “development”)
3. Validation
4. Testing
We’ve already seen training, validation, and testing splits before – but what is this new “training-
validation” set? Ng recommends that we take all of our data and split it into 60% for training and
the remaining 40% for testing. We then split the testing data into two parts: one for validation
and the other for true testing (i.e., the data we never touch until we are ready to evaluate the
performance of our network). From our training set, we then take a small chunk of it and add it to
our “training-validation set”. The training set will help us determine the bias of our model while
the training-validation set will help determine variance.
If our training error is too high, as in Figure 8.3 (top-left) below, then we should consider
deepening our current architecture by adding in more layers and neurons. We should also consider
training for longer (i.e., more epochs) while simultaneously tweaking our learning rate – using a
smaller learning rate may enable you to train for longer while helping prevent overfitting. Finally,
if after many experiments using our current architecture and varying learning rates does not prove
useful, then we likely need to try an entirely different model architecture.
Moving on to the second item in the flow chart, if our training-validation error is high (Figure
8.3, top-right), then we should examine the regularization parameters in our network. Are we
applying dropout layers inside the network architecture? Is data augmentation being used to
help generate new training samples? What about the actual loss/update function itself – is a
regularization penalty being included? Examine these questions in the context of your own deep
learning experiments and start adding in regularization.
You should also consider gathering more training data (again, taking care that this training data
is representative of where the model will be deployed) at this point – in nearly all cases having more
training data is never a bad thing. It is likely that your model does not have enough training data
to learn the underlying patterns in your example images. Finally, after exhausting these options,92
Chapter 8. Optimal Pathway to Apply Deep Learning
Figure 8.3: The four stages of Andrew Ng’s machine learning recipe. Top-right: Our training error
is high, implying that we need a more powerful model to represent the underlying patterns in the
data. Top-left: Our training error has decreased, but our training-validation error is high. This
implies we should obtain more data or apply strong regularization. Bottom-left: If both training
and training-validation error are low, but validation error is high we should examine our training
data and ensure it mimics our validation and testing sets properly. Bottom-right: If training,
training-validation, and validation error are all low but testing error is high then we need to gather
more training + validation data.
you’ll once again want to consider using a different network architecture.
Continuing through the flowchart in Figure 8.3 (bottom-left), if our training-validation error
is low, but our validation set error is high, we need to examine our training data with a closer eye.
Are we absolutely, positively sure that our training images are similar to our validation images?
Be honest with yourself – you cannot expect a deep learning model trained on images not
representative of the images they’ll see in a validation or testing setting to perform well. If you
make the hard realization that this is indeed the case, go back to the dataset collection phase and
spend the time gathering more data. Without data representative of where your deep learning model
will be deployed, you will not obtain high accuracy results. You should also again inspect your
regularization parameters – are you regularizing strong enough? Finally, you may once again need
to consider a new model architecture.
Finally, we move on to the last step in the flow chart – is our testing error high? At this point,
we’ve overfit our model to the training and validation data (Figure 8.3 bottom-right). We need
to go back and gather more data for the validation set to help us identify when this overfitting8.2 Transfer Learning or Train from Scratch
93
is starting to occur. Using this methodology proposed by Andrew Ng, we can more easily make
(correct) decisions regarding updating our model/dataset when our experiments don’t turn out as
we expected.
8.2
Transfer Learning or Train from Scratch
The following section is inspired by the excellent “Transfer Learning” lesson of Stanford’s cs231n
class [36]. I’ve also included my own anecdotal experiences to aid in your own experiments. Given
the success of transfer learning in Chapter 3 on feature extraction and Chapter 5 on fine-tuning,
you may wonder when you should be applying transfer learning and when you should be training a
model from scratch.
To make this decision, you need to consider two important factors:
1. The size of your dataset.
2. The similarity of your dataset to the dataset the pre-trained CNN was trained on (which is
typically ImageNet).
Based on these factors we can construct a chart to help us make a decision on whether or not
we need to apply transfer learning or train from scratch (Figure 8.4). Let’s review each of the four
possibilities below.
Figure 8.4: A table you can use to determine if you should train our network from scratch or transfer
learning. Figure inspired by Greg Chu from Deep Learning Sandbox [37].
Your Dataset is Small and Similar to the Original Dataset
Since your dataset is small, you likely don’t have enough training examples to train a CNN from
scratch (again, keep in mind that you should ideally have 1,000-5,000 examples per class you want
to recognize). Furthermore, given the lack of training data, it’s likely not a good idea to attempt
fine-tuning as we’ll likely end up overfitting.
Instead, since your image dataset is similar to what the pre-trained network was trained on, you
should treat the network as a feature extractor and train a simple machine learning classifier on
top of these features. You should extract features from layers deeper in the architecture as these
features are more rich and representative of the patterns learned from the original dataset.
Your Dataset is Large and Similar to the Original Dataset
With a large dataset, we should have enough examples to apply fine-tuning without overfitting.
You may be tempted to train your own model from scratch here as well – this is an experiment
worth running. However, since your dataset is similar to the original dataset the network was
already trained on, the filters inside the network are likely already discriminative enough to obtain
a reasonable classifier. Therefore, apply fine-tuning in this case.
Your Dataset is Small and Different than the Original Dataset
Again, given a small dataset, we likely won’t obtain a high accuracy deep learning model by
training from scratch. Instead, we should again apply feature extraction and train a standard94
Chapter 8. Optimal Pathway to Apply Deep Learning
machine learning model on top of them – but since our data is different from the original dataset,
we should use lower level layers in the network as our feature extractors.
Keep in mind that the deeper we go into the network architecture, the more rich and discrimi-
native the features are specific to the dataset it was trained on. By extracting features from lower
layers in the network, we can still leverage these filters, but without the abstraction caused by the
deeper layers.
Your New Dataset is Large and Different than Original Dataset
In this case, we have two options. Given that we have sufficient training data, we can likely train
our own custom network from scratch. However, the pre-trained weights from models trained on
dataset such as ImageNet make for excellent initializations, even if the datasets are unrelated. We
should therefore perform two sets of experiments:
1. In the first set of experiments, attempt to fine-tune a pre-trained network to your dataset and
evaluate the performance.
2. Then in the second set of experiments, train a brand new model from scratch and evaluate.
Exactly which method performs best is entirely dependent on your dataset and classification
problem. However, I would recommend trying to fine-tune first as this method will allow you to
establish a baseline to beat when you move on to your second set of experiments and train your
network from scratch.
8.3
Summary
In this chapter, we explored the optimal pathway to apply deep learning techniques when training
your own custom networks. When gathering your training data, keep in mind there are no shortcuts
– take the time to ensure that data you use to train your model is representative of the images your
network will see when deployed in a real-world application.
There is an old computer science anecdote that states “Garbage in, garbage out”. If your input
data does not represent examples of data points your model will see after being trained, you’re
essentially falling into this garbage in, garbage out trap. That isn’t to say your data is “garbage”.
Instead, remind yourself of this anecdote when performing your own experiments and realize that
it’s not possible for your deep learning model to perform well on data points it was never trained to
recognize in the first place.
We also reviewed when you should consider transfer learning versus training your own network
from scratch. With small datasets, you should consider feature extraction. For larger datasets,
consider fine-tuning first (to establish a baseline) and then move on to training a model from scratch.9. Working with HDF5 and Large Datasets
So far in this book, we’ve only worked with datasets that can fit into the main memory of our
machines. For small datasets this is a reasonable assumption – we simply load each individual
image, preprocess it, and allow it to be fed through our network. However, for large scale deep
learning datasets (e.x., ImageNet), we need to create data generators that access only a portion of
the dataset at a time (i.e., a mini-batch), then allow the batch to be passed through the network.
Luckily, Keras ships with methods that allow you to use the raw file paths on disk as inputs to a
training process. You do not have to store the entire dataset in memory – simply supply the image
paths to the Keras data generator and your images will be loaded in batches and fed through the
network.
However, this method is terribly inefficient. Each and every image residing on your disk
requires an I/O operation which introduces latency into your training pipeline. Training deep
learning networks is already slow enough – we would do well to avoid the I/O bottleneck as much
as possible.
A more elegant solution would be to generate an HDF5 dataset for your raw images just as
we did in Chapter 3 on transfer learning and feature extraction, only this time we are storing the
images themselves rather than extracted features. Not only is HDF5 capable of storing massive
datasets, but it’s optimized for I/O operations, especially for extracting batches (called “slices”)
from the file. As we’ll see throughout the remainder of this book, taking the extra step to pack the
raw images residing on disk into an HDF5 file allows us to construct a deep learning framework
that can be used to rapidly build datasets and train deep learning networks on top of them.
In the remainder of this chapter, I’ll demonstrate how to construct an HDF5 dataset for the
Kaggle Dogs vs. Cats competition [3]. Then, in the next chapter, we’ll use this HDF5 dataset
to train the seminal AlexNet architecture [6], eventually resulting in a top-25 position on the
leaderboard in the subsequent chapter.
9.1
Downloading Kaggle: Dogs vs. Cats
To download the Kaggle: Dogs vs. Cats dataset you’ll first need to create an account on kaggle.com.
From there, head to the Dogs vs. Cats homepage (http://pyimg.co/xb5lb).Chapter 9. Working with HDF5 and Large Datasets
96
You’ll need need to download train.zip. Do not download test1.zip. The images inside
test1.zip are only used for computing predictions and submitting to the Kaggle evaluation
server. Since we need the class labels to construct our own training and testing splits we only need
train.zip. Submitting your own predicted results is outside the scope of this book but can easily
be accomplished by writing your predictions on test1.zip following the file format outlined in
sampleSubmission.csv.
After train.zip has been downloaded, unarchive it and you’ll find a directory named train –
this directory contains our actual images. The labels themselves can be derived from examining the
file names. I have included a sample of the file names below:
kaggle_dogs_vs_cats/train/cat.11866.jpg
...
kaggle_dogs_vs_cats/train/dog.11046.jpg
As I recommended in the Starter Bundle, I’ll be using the following data structure for this
project:
|---
|
|
|
|---
|
|
|
datasets
|--- kaggle_dogs_vs_cats
|
|--- hdf5
|
|--- train
dogs_vs_cats
|--- config
|--- build_dogs_vs_cats.py
|--- ...
Notice how I’ll be storing the train directory containing our example images in a folder dedi-
cated exclusively to the Kaggle: Dogs vs. Cats competition. From there, I have the dogs_vs_cats
directory which is where we’ll be storing the code for this project.
Now that we have downloaded the Dogs vs. Cats dataset and examined our directory structure,
let’s create our configuration file.
9.2
Creating a Configuration File
Now that we are starting to build more advanced projects and deep learning methods, I like to
create a special config Python module for each of my projects. For example, here is the directory
structure for the Kaggle Dogs vs. Cats project:
--- dogs_vs_cats
|
|--- config
|
|
|--- __init__.py
|
|
|--- dogs_vs_cats_config.py
|
|--- build_dogs_vs_cats.py
|
|--- crop_accuracy.py
|
|--- extract_features.py
|
|--- train_alexnet.py
|
|--- train_model.py
|
|--- output/
|
|
|--- __init__.py
|
|
|--- alexnet_dogs_vs_cats.model9.2 Creating a Configuration File
|
|
|
|
|
|
97
|--- dogs_vs_cats_features.hdf5
|--- dogs_vs_cats_mean.json
|--- dogs_vs_cats.pickle
You can ignore the actual Python scripts for now as we’ll be reviewing them in the next chapter,
but take a look at the directory named config. Inside of config you’ll find a single Python
file named dogs_vs_cats_config.py – I use this file to store all relevant configurations for the
project, including:
1. The paths to the input images.
2. The total number of class labels.
3. Information on the training, validation, and testing splits.
4. The paths to the HDF5 datasets.
5. Paths to output models, plots, logs, etc.
Using a Python file rather than a JSON file allows me to include snippets of Python code and
makes the configuration file more efficient to work with (a great example being manipulating file
paths using the os.path module). I would suggest you get into the habit of using Python-based
configuration files for your own deep learning projects as it will greatly improve your productivity
and allow you to control most of the parameters in your project through a single file.
9.2.1
Your First Configuration File
Let’s go ahead and take a look at my configuration file (dogs_vs_cats_config.py) for the Kaggle
Dogs vs. Cats dataset:
1
2
# define the paths to the images directory
IMAGES_PATH = "../datasets/kaggle_dogs_vs_cats/train"
3
4
5
6
7
8
9
# since we do not have validation data or access to the testing
# labels we need to take a number of images from the training
# data and use them instead
NUM_CLASSES = 2
NUM_VAL_IMAGES = 1250 * NUM_CLASSES
NUM_TEST_IMAGES = 1250 * NUM_CLASSES
10
11
12
13
14
15
# define the path to the output training, validation, and testing
# HDF5 files
TRAIN_HDF5 = "../datasets/kaggle_dogs_vs_cats/hdf5/train.hdf5"
VAL_HDF5 = "../datasets/kaggle_dogs_vs_cats/hdf5/val.hdf5"
TEST_HDF5 = "../datasets/kaggle_dogs_vs_cats/hdf5/test.hdf5"
On Line 2 I define the path to the directory containing the dog and cat images – these are the
images that we’ll be packing into a HDF5 dataset later in this chapter. Lines 7-9 define the total
number of class labels (two: one for dog, another for cat) along with the number of validation and
testing images (2,500 for each). We can then specify the path to our output HDF5 files for the
training, validation, and testing splits, respectively on Lines 13-15.
The second half of the configuration file defines the path to the output serialized weights, the
dataset mean, and a general “output” path to store plots, classification reports, logs, etc.:
17
18
19
# path to the output model file
MODEL_PATH = "output/alexnet_dogs_vs_cats.model"Chapter 9. Working with HDF5 and Large Datasets
98
20
21
# define the path to the dataset mean
DATASET_MEAN = "output/dogs_vs_cats_mean.json"
22
23
24
25
# define the path to the output directory used for storing plots,
# classification reports, etc.
OUTPUT_PATH = "output"
The DATASET_MEAN file will be used to store the average red, green, and blue pixel intensity
values across the entire (training) dataset. When we train our network, we’ll subtract the mean
RGB values from every pixel in the image (the same goes for testing and evaluation as well). This
method, called mean subtraction, is a type of data normalization technique and is more often used
than scaling pixel intensities to the range [0, 1] as it’s shown to be more effective on large datasets
and deeper neural networks.
9.3
Building the Dataset
Now that our configuration file has been defined, let’s move on to actually building our HDF5
datasets. Open up a new file, name it build_dogs_vs_cats.py, and insert the following code:
1
2
3
4
5
6
7
8
9
10
11
12
# import the necessary packages
from config import dogs_vs_cats_config as config
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from pyimagesearch.preprocessing import AspectAwarePreprocessor
from pyimagesearch.io import HDF5DatasetWriter
from imutils import paths
import numpy as np
import progressbar
import json
import cv2
import os
Lines 2-12 import our required Python packages. I like to import our project configuration file
as the first import in the project (Line 2). This method is a matter of taste, so feel free to place the
import wherever you like in the file. I also rename dogs_vs_cats_config as simply config to
make it less verbose when writing code.
From there, the rest of the imports you have encountered before in previous chapters; how-
ever, I would like to draw your attention to the HDF5DatasetWriter on Line 6, the very same
HDF5DatasetWriter we defined in Chapter 3 – this class will be used to pack our raw images on
disk into a single, serialized file.
We’ll also again be using the progressbar module, a simple utility library I like to use when
measuring the approximate time a given task is taking. This module is totally irrelevant to deep
learning, but again, I find it convenient to use as for large datasets it may take several hours to pack
a dataset of images into HDF5 format.
Next, let’s grab the paths to the images in the Kaggle Dogs vs. Cats dataset:
14
15
16
17
# grab the paths to the images
trainPaths = list(paths.list_images(config.IMAGES_PATH))
trainLabels = [p.split(os.path.sep)[2].split(".")[0]
for p in trainPaths]9.3 Building the Dataset
18
19
99
le = LabelEncoder()
trainLabels = le.fit_transform(trainLabels)
The Dogs vs. Cats dataset has the following example directory structure:
kaggle_dogs_vs_cats/train/cat.11866.jpg
...
kaggle_dogs_vs_cats/train/dog.11046.jpg
Notice how the name of the class is built-into the actual filename. Therefore, we need to extract
the file component of the file path, split on the . separator, and extract the class name – in fact, that
is exactly what Lines 16 and 17 do.
Given the paths to all images in the dataset, they loop over them individually and extract the
labels from the file paths. If you find these lines of code confusing, I would suggest taking a
second now to manually play with the code, specifically the os.path.sep variable and the .split
function used on the file path string to further see how these utilities are used to manipulate file
paths.
Lines 18 and 19 then encode the class labels. For the Kaggle Dogs vs. Cats project we’ll need
three splits: a training split, a validation split, and a testing split.
Our next code block handles generating each of these splits:
21
22
23
24
25
26
# perform stratified sampling from the training set to build the
# testing split from the training data
split = train_test_split(trainPaths, trainLabels,
test_size=config.NUM_TEST_IMAGES, stratify=trainLabels,
random_state=42)
(trainPaths, testPaths, trainLabels, testLabels) = split
27
28
29
30
31
32
33
# perform another stratified sampling, this time to build the
# validation data
split = train_test_split(trainPaths, trainLabels,
test_size=config.NUM_VAL_IMAGES, stratify=trainLabels,
random_state=42)
(trainPaths, valPaths, trainLabels, valLabels) = split
On Lines 23-26 we take our input images and labels and use them to construct the training and
testing split. However, we need to perform another split on Lines 30-33 to create the validation
set. The validation set is (almost always) taken from the training data. The size of the testing and
validation splits are controlled via the NUM_TEST_IMAGES and NUM_VAL_IMAGES, each of which
are defined in our config file.
Now that we have our training, testing, and validation splits, let’s create a simple list that will
allow us to loop over them and efficiently write the images in each dataset to our HDF5 file:
35
36
37
38
39
40
41
# construct a list pairing the training, validation, and testing
# image paths along with their corresponding labels and output HDF5
# files
datasets = [
("train", trainPaths, trainLabels, config.TRAIN_HDF5),
("val", valPaths, valLabels, config.VAL_HDF5),
("test", testPaths, testLabels, config.TEST_HDF5)]100
Chapter 9. Working with HDF5 and Large Datasets
42
43
44
45
46
# initialize the image preprocessor and the lists of RGB channel
# averages
aap = AspectAwarePreprocessor(256, 256)
(R, G, B) = ([], [], [])
On Line 38 we define a datasets list that includes our training, validation, and testing
variables. Each entry in the list is a 4-tuple consisting of:
1. The name of the split (i.e., training, testing, or validation).
2. The respective image paths for the split.
3. The labels for the split.
4. The path to the output HDF5 file for the split.
We then initialize our AspectAwarePreprocessor on Line 45 used to resize images to 256 ×
256 pixels (keeping the aspect ratio of the image in mind) prior to being written to HDF5. We’ll
also initialize three lists on Line 46 – R, G, and B, used to store the average pixel intensities for each
channel.
Finally, we are ready to build our HDF5 datasets:
48
49
50
51
52
# loop over the dataset tuples
for (dType, paths, labels, outputPath) in datasets:
# create HDF5 writer
print("[INFO] building {}...".format(outputPath))
writer = HDF5DatasetWriter((len(paths), 256, 256, 3), outputPath)
53
54
55
56
57
58
# initialize the progress bar
widgets = ["Building Dataset: ", progressbar.Percentage(), " ",
progressbar.Bar(), " ", progressbar.ETA()]
pbar = progressbar.ProgressBar(maxval=len(paths),
widgets=widgets).start()
On Line 49 we start looping over each of the 4-tuple values in the datasets list. For each
data split, we instantiate the HDF5DatasetWriter on Line 52. Here the dimensions of the output
dataset will be the (len(paths), 256, 256, 3), implying there are len(paths) total images,
each of them with a width of 256 pixels, a height of 256 pixels, and 3 channels.
Lines 54-58 then initialize our progress bar so we can easily monitor the process of the
dataset generation. Again, this code block (along with the rest of the progressbar function calls)
is entirely optional, so feel free to comment them out if you so wish.
Next, let’s write each image in a given data split to the writer:
60
61
62
63
64
# loop over the image paths
for (i, (path, label)) in enumerate(zip(paths, labels)):
# load the image and process it
image = cv2.imread(path)
image = aap.preprocess(image)
65
66
67
68
69
70
71
# if we are building the training dataset, then compute the
# mean of each channel in the image, then update the
# respective lists
if dType == "train":
(b, g, r) = cv2.mean(image)[:3]
R.append(r)9.3 Building the Dataset
72
73
101
G.append(g)
B.append(b)
74
75
76
77
# add the image and label # to the HDF5 dataset
writer.add([image], [label])
pbar.update(i)
78
79
80
81
# close the HDF5 writer
pbar.finish()
writer.close()
On Line 61 we start looping over each individual image and corresponding class label in the
data split. Lines 63 and 64 load the image from disk and then apply our aspect-aware preprocessor
to resize the image to 256 × 256 pixels.
We make a check on Line 69 to see if we are examining the train data split and, if so, we
compute the average of the Red, Green, and Blue channels (Line 70) and update their respective
lists on Lines 71-73. Computing the average of the RGB channels is only done for the training set
and is a requirement if we wish to apply mean subtraction normalization.
Line 76 adds the corresponding image and label to our HDF5DatasetWriter. Once all
images in the data split have been serialized to the HDF5 dataset, we close the writer on Line 81.
The final step is to serialize our RGB averages to disk:
83
84
85
86
87
88
89
# construct a dictionary of averages, then serialize the means to a
# JSON file
print("[INFO] serializing means...")
D = {"R": np.mean(R), "G": np.mean(G), "B": np.mean(B)}
f = open(config.DATASET_MEAN, "w")
f.write(json.dumps(D))
f.close()
Line 86 constructs a Python dictionary of the average RGB values over all images in the
training set. Keep in mind that each individual R, G, and B contains the average of channel for each
image in the dataset. Computing the mean of this list gives us the average pixel intensity value for
all images in the list. This dictionary is then serialized to disk in JSON format on Lines 87-88.
Let’s go ahead and serialize the Kaggle Dogs vs. Cats dataset to HDF5 format. Open up a
terminal and then issue the following command:
$ python build_dogs_vs_cats.py
[INFO] building kaggle_dogs_vs_cats/hdf5/train.hdf5...
Building Dataset: 100% |####################################| Time: 0:02:39
[INFO] building kaggle_dogs_vs_cats/hdf5/val.hdf5...
Building Dataset: 100% |####################################| Time: 0:00:20
[INFO] building kaggle_dogs_vs_cats/hdf5/test.hdf5...
Building Dataset: 100% |####################################| Time: 0:00:19
As you can see from my output, an HDF5 file was created for each of the training, testing, and
validation splits. The training split generation took the longest to generate as this split contained
the most data (2m39s). The testing and validation splits took substantially less time (≈ 20s) due
the fact that there is less data in these splits.
We can see each of these output files on our disk by listing the contents of the hdf5 directory:102
Chapter 9. Working with HDF5 and Large Datasets
$ ls -l ../datasets/kaggle_dogs_vs_cats/hdf5/
total 38400220
-rw-rw-r-- 1 adrian adrian 3932182144 Apr 7 18:00 test.hdf5
-rw-rw-r-- 1 adrian adrian 31457442144 Apr 7 17:59 train.hdf5
-rw-rw-r-- 1 adrian adrian 3932182144 Apr 7 18:00 val.hdf5
Looking at these file sizes you might be a bit surprised. The raw Kaggle Dogs vs. Cats images
residing on disk are only 595MB – why are the .hdf5 files so large? The train.hdf5 file alone is
31.45GB while the test.hdf5 and val.hdf5 files are almost 4GB. Why?
Well, keep in mind that raw image file formats such as JPEG and PNG apply data compression
algorithms to keep image file sizes small. However, we have effectively removed any type of com-
pression and are storing the images as raw NumPy arrays (i.e., bitmaps). This lack of compression
dramatically inflates our storage costs, but will also help speed up our training time as we won’t
have to waste processor time decoding the image – we can instead access the image directly from
the HDF5 dataset, preprocess it, and pass it through our network.
Let’s also take a look at our RGB mean file:
$ cat output/dogs_vs_cats_mean.json
{"B": 106.13178224639893, "R": 124.96761639328003, "G": 115.97504255599975}
Here we can see that the red channel has an average pixel intensity of 124.96 across all images
in the dataset. The blue channel has an average of 106.13 and the green channel an average of
115.97. We’ll be constructing a new image preprocessor to normalize our images by subtracting
these RGB averages from the input images prior to passing them through our network. This
mean normalization helps “center” the data around the zero mean. Typically, this normalization
enables our network to learn faster and is also why we use this type of normalization (rather than
[0, 1] scaling) on larger, more challenging datasets.
9.4
Summary
In this chapter, we learned how to serialize raw images into an HDF5 dataset suitable for training a
deep neural network. The reason we serialized the raw images into an HDF5 file rather than simply
accessing mini-batches of image paths on disk when training is due to I/O latency – for each image
on disk we would have to perform an I/O operation to read the image. This subtle optimization
doesn’t seem like a big deal, but I/O latency is a huge problem in a deep learning pipeline – the
training process is already slow enough, and if we make it hard for our networks to access our data,
we are only further shooting ourselves in the foot.
Conversely, if we serialize all images into an efficiently packed HDF5 file, we can leverage very
fast array slices to extract our mini-batches, thereby dramatically reducing I/O latency and helping
speed up the training process. Whenever you are using the Keras library and working with a dataset
too large to fit into memory, be sure you consider serializing your dataset into HDF5 format first –
as we’ll find out in the next chapter, it makes training your network an easier (and more efficient)
task.10. Competing in Kaggle: Dogs vs. Cats
In our previous chapter, we learned how to work with HDF5 and datasets too large to fit into
memory. To do so, we defined a Python utility script that can be used to take an input dataset of
images and serialize them into a highly efficient HDF5 dataset. Representing a dataset of images in
an HDF5 dataset allows us to avoid issues of I/O latency, thereby speeding up the training process.
For example, if we defined a dataset generator that loaded images sequentially from disk, we
would need N read operations, one for each image. However, by placing our dataset of images
into an HDF5 dataset, we can instead load batches of images using a single read. This action
dramatically reduces the number of I/O calls and allows us to work with very large image datasets.
In this chapter, we are going to extend our work and learn how to define an image generator for
HDF5 datasets suitable for training Convolutional Neural Networks with Keras. This generator will
open the HDF5 dataset, yield batches of images and associated training labels for the network to be
trained on, and proceed to do so until our model reaches sufficiently low loss/high accuracy.
To accomplish this process, we’ll first explore three new image pre-processors designed to
increase classification accuracy – mean subtraction, patch extraction, and cropping (also called
10-cropping or over-sampling). Once we’ve defined our new set of pre-processors, we’ll move on
defining the actual HDF5 dataset generator.
From there, we’ll implement the seminal AlexNet architecture from Krizhevsky et al.’s 2012
paper, ImageNet Classification with Deep Convolutional Neural Networks [6]. This implementation
of AlexNet will then be trained on the Kaggle Dogs vs. Cats challenge. Given the trained model,
we’ll evaluate its performance on the testing set, followed by using over-sampling methods to boost
classification accuracy further. As our results will demonstrate, our network architecture + cropping
methods will enable us to obtain a position in the top-25 leaderboard of the Kaggle Dogs vs. Cats
challenge.
10.1
Additional Image Preprocessors
In this section we’ll implement two new image pre-preprocessors:
1. A mean subtraction pre-processor designed to subtract the mean Red, Green, and Blue pixel
intensities across a dataset from an input image (which is a form of data normalization).104
Chapter 10. Competing in Kaggle: Dogs vs. Cats
2. A patch preprocessor used to randomly extract M × N pixel regions from an image during
training.
3. An over-sampling pre-processor used at testing time to sample five regions of an input image
(the four corners + center area) along with their corresponding horizontal flips (for a total of
10 crops).
Using over-sampling, we can boost our classification accuracy by passing the 10 crops through
our CNN and then averaging across the 10 predictions.
10.1.1
Mean Preprocessing
Let’s get started with the mean pre-processor. In Chapter 9 we learned how to convert an image
dataset to HDF5 format – part of this conversion involved computing the average Red, Green, and
Blue pixel intensities across all images in the entire dataset. Now that we have these averages,
we are going to perform a pixel-wise subtraction of these values from our input images as a form
of data normalization. Given an input image I and its R, G, B channels, we can perform mean
subtraction via:
• R = R − μ R
• G = G − μ G
• B = B − μ B
Where μ R , μ G , and μ B are computed when the image dataset is converted to HDF5 format.
Figure 10.1 includes a visualization of subtracting the mean RGB values from an input image –
notice how the subtraction is done pixel-wise.
Figure 10.1: An example of applying mean subtraction to an input image (left) by subtracting
R = 124.96, G = 115.97, B = 106.13 pixel-wise, resulting in the output image (right). Mean
subtraction is used to reduce the affects of lighting variations during classification.
To make this concept more concrete, let’s go ahead and implement our MeanPreprocessor
class:
--- pyimagesearch
|
|--- __init__.py
|
|--- callbacks
|
|--- nn
|
|--- preprocessing
|
|
|--- __init__.py
|
|
|--- aspectawarepreprocessor.py
|
|
|--- imagetoarraypreprocessor.py
|
|
|--- meanpreprocessor.py
|
|
|--- simplepreprocessor.py
|
|--- utils10.1 Additional Image Preprocessors
105
Notice how I have placed a new file named meanpreprocessor.py in the preprocessing
sub-module of pyimagesearch – this location is where our MeanPreprocessor class will live.
Let’s go ahead and implement this class now:
1
2
# import the necessary packages
import cv2
3
4
5
6
7
8
9
10
class MeanPreprocessor:
def __init__(self, rMean, gMean, bMean):
# store the Red, Green, and Blue channel averages across a
# training set
self.rMean = rMean
self.gMean = gMean
self.bMean = bMean
Line 5 defines the constructor to the MeanPreprocessor, which requires three arguments –
the respective Red, Green, and Blue averages computed across the entire dataset. These values are
then stored on Lines 8-10.
Next, let’s define the preprocess method, a required function for every pre-processor we
intend to apply to our image processing pipeline:
12
13
14
15
def preprocess(self, image):
# split the image into its respective Red, Green, and Blue
# channels
(B, G, R) = cv2.split(image.astype("float32"))
16
17
18
19
20
#
R
G
B
subtract the means for each channel
-= self.rMean
-= self.gMean
-= self.bMean
21
22
23
# merge the channels back together and return the image
return cv2.merge([B, G, R])
Line 15 uses the cv2.split function to split our input image into its respective RGB com-
ponents. Keep in mind that OpenCV represents images in BGR order rather than RGB ([38],
http://pyimg.co/ppao), hence why our return tuple has the signature (B, G, R) rather than (R, G,
B). We’ll also ensure that these channels are of a floating point data type as OpenCV images are
typically represented as unsigned 8-bit integers (in which case we can’t have negative values, and
modulo arithmetic would be performed instead).
Lines 17-20 perform the mean subtraction itself, subtracting the respective mean RGB values
from the RGB channels of the input image. Line 23 then merges the normalized channels back
together and returns the resulting image to the calling function.
10.1.2
Patch Preprocessing
The PatchPreprocessor is responsible for randomly sampling M × N regions of an image during
the training process. We apply patch preprocessing when the spatial dimensions of our input images
are larger than what the CNN expects – this is a common technique to help reduce overfitting,
and is, therefore, a form of regularization. Instead of using the entire image during training, we
instead crop a random portion of it and pass it to the network (see Figure 10.2 for an example of
crop preprocessing).106
Chapter 10. Competing in Kaggle: Dogs vs. Cats
Figure 10.2: Left: Our original 256 × 256 input image. Right: Randomly cropping a 227 × 227
region from the image.
Applying this cropping implies that a network never sees the exact same image (unless by
random happenstance), similar to data augmentation. As you know from our previous chapter, we
constructed an HDF5 dataset of Kaggle Dogs vs. Cats images where each image is 256 × 256 pixels.
However, the AlexNet architecture that we’ll be implementing later in this chapter can only accept
images of size 227 × 227 pixels.
So, what are we to do? Apply a SimplePreprocessor to resize our each of the 256 × 256
pixels down to 227 × 227? No, that would be wasteful, especially since this is an excellent
opportunity to perform data augmentation by randomly cropping a 227 × 227 region from the
256 × 256 image during training – in fact, this process is exactly how Krizhevsky et al. trains
AlexNet on the ImageNet dataset.
The PatchPreprocessor, just like all other image pre-processors, will be sorted in the
preprocessing sub-module of pyimagesearch:
--- pyimagesearch
|
|--- __init__.py
|
|--- callbacks
|
|--- nn
|
|--- preprocessing
|
|
|--- __init__.py
|
|
|--- aspectawarepreprocessor.py
|
|
|--- imagetoarraypreprocessor.py
|
|
|--- meanpreprocessor.py
|
|
|--- patchpreprocessor.py
|
|
|--- simplepreprocessor.py
|
|--- utils
Open up the patchpreprocessor.py file and let’s define the PatchPreprocessor class:
1
2
# import the necessary packages
from sklearn.feature_extraction.image import extract_patches_2d
3
4
5
6
7
8
class PatchPreprocessor:
def __init__(self, width, height):
# store the target width and height of the image
self.width = width
self.height = height10.1 Additional Image Preprocessors
107
Line 5 defines the construct to PatchPreprocessor – we simply need to supply the target
width and height of the cropped image.
We can then define the preprocess function:
10
11
12
13
14
def preprocess(self, image):
# extract a random crop from the image with the target width
# and height
return extract_patches_2d(image, (self.height, self.width),
max_patches=1)[0]
Extracting a random patches of size self.width x self.height is easy using the extract_patches_2d
function from the scikit-learn library. Given an input image, this function randomly extracts a
patch from image. Here we supply max_patches=1, indicating that we only need a single random
patch from the input image.
The PatchPreprocessor class doesn’t seem like much, but it’s actually a very effective
method to avoid overfitting by applying yet another layer of data augmentation. We’ll be using the
PatchPreprocessor when training AlexNet. The next pre-processor, CropPreprocessor, will
be used when evaluating our trained network.
10.1.3
Crop Preprocessing
Next, we need to define a CropPreprocessor responsible for computing the 10-crops for over-
sampling. During the evaluating phase of our CNN, we’ll crop the four corners of the input image +
the center region and then take their corresponding horizontal flips, for a total of ten samples per
input image (Figure 10.3).
Figure 10.3: Left: The original 256 × 256 input image. Right: Applying the 10-crop prepro-
cessor to extract ten 227 × 227 crops of the image including the center, four corners, and their
corresponding horizontal mirrors.
These ten samples will be passed through the CNN, and then the probabilities averaged.
Applying this over-sampling method tends to include 1-2 percent increases in classification accuracy
(and in some cases, even higher).
The CropPreprocessor class will also live in the preprocessing sub-module of pyimagesearch:
--- pyimagesearch
|
|--- __init__.py
|
|--- callbacks
|
|--- nnChapter 10. Competing in Kaggle: Dogs vs. Cats
108
|
|
|
|
|
|
|
|
|
|---
|
|
|
|
|
|
|
|---
preprocessing
|--- __init__.py
|--- aspectawarepreprocessor.py
|--- croppreprocessor.py
|--- imagetoarraypreprocessor.py
|--- meanpreprocessor.py
|--- patchpreprocessor.py
|--- simplepreprocessor.py
utils
Open up the croppreprocessor.py file and let’s define it:
1
2
3
# import the necessary packages
import numpy as np
import cv2
4
5
6
7
8
9
10
11
12
13
class CropPreprocessor:
def __init__(self, width, height, horiz=True, inter=cv2.INTER_AREA):
# store the target image width, height, whether or not
# horizontal flips should be included, along with the
# interpolation method used when resizing
self.width = width
self.height = height
self.horiz = horiz
self.inter = inter
Line 6 defines the constructor to to CropPreprocessor. The only required arguments are
the target width and height of each cropped region. We can also optionally specify whether
horizontal flipping should be applied (defaults to True) along with the interpolation algorithm
OpenCV will use for resizing. These arguments are all stored inside the class for use within the
preprocess method.
Speaking of which, let’s define the preprocess method now:
15
16
17
def preprocess(self, image):
# initialize the list of crops
crops = []
18
19
20
21
22
23
24
25
26
# grab the width and height of the image then use these
# dimensions to define the corners of the image based
(h, w) = image.shape[:2]
coords = [
[0, 0, self.width, self.height],
[w - self.width, 0, w, self.height],
[w - self.width, h - self.height, w, h],
[0, h - self.height, self.width, h]]
27
28
29
30
31
# compute the center crop of the image as well
dW = int(0.5 * (w - self.width))
dH = int(0.5 * (h - self.height))
coords.append([dW, dH, w - dW, h - dH])
The preprocess method requires only a single argument – the image which we are going to
apply over-sampling. We grab the width and height of the input image on Line 21, which then10.2 HDF5 Dataset Generators
109
allows us to compute the (x, y)-coordinates of the four corners (top-left, top-right, bottom-right,
bottom-left, respectively) on Lines 22-26. The center crop of the image is then computed on Lines
29 and 30, then added to the list of coords on Line 31.
We are now ready to extract each of the crops:
33
34
35
36
37
38
39
# loop over the coordinates, extract each of the crops,
# and resize each of them to a fixed size
for (startX, startY, endX, endY) in coords:
crop = image[startY:endY, startX:endX]
crop = cv2.resize(crop, (self.width, self.height),
interpolation=self.inter)
crops.append(crop)
On Line 35 we loop over each of the starting and ending (x, y)-coordinates of the rectangular
crops. Line 36 extracts the crop via NumPy array slicing which we then resize on Line 37 to
ensure the target width and height dimensions are met. The crop is the added to the crops list.
In the case that horizontal mirrors are to be computed, we can flip each of the five original
crops, leaving us with ten crops overall:
41
42
43
44
45
# check to see if the horizontal flips should be taken
if self.horiz:
# compute the horizontal mirror flips for each crop
mirrors = [cv2.flip(c, 1) for c in crops]
crops.extend(mirrors)
46
47
48
# return the set of crops
return np.array(crops)
The array of crops is then returned to the calling function on Line 48. Using both the
MeanPreprocessor for normalization and the CropPreprocessor for oversampling, we’ll be
able to obtain higher classification accuracy than is otherwise possible.
10.2
HDF5 Dataset Generators
Before we can implement the AlexNet architecture and train it on the Kaggle Dogs vs. Cats dataset,
we first need to define a class responsible for yielding batches of images and labels from our HDF5
dataset. Chapter 9 discussed how to convert a set of images residing on disk into an HDF5 dataset –
but how do we get them back out again?
The answer is to define an HDF5DatasetGenerator class in the io sub-module of pyimagesearch:
--- pyimagesearch
|
|--- __init__.py
|
|--- callbacks
|
|--- io
|
|
|--- __init__.py
|
|
|--- hdf5datasetgenerator.py
|
|
|--- hdf5datasetwriter.py
|
|--- nn
|
|--- preprocessing
|
|--- utils110
Chapter 10. Competing in Kaggle: Dogs vs. Cats
Previously, all of our image datasets could be loaded into memory so we could rely on Keras
generator utilities to yield our batches of images and corresponding labels. However, now that our
datasets are too large to fit into memory, we need to handle implementing this generator ourselves.
Go ahead and open the hdf5datasetgenerator.py file and we’ll get to work:
1
2
3
4
# import the necessary packages
from keras.utils import np_utils
import numpy as np
import h5py
5
6
7
8
9
10
11
12
13
14
15
16
class HDF5DatasetGenerator:
def __init__(self, dbPath, batchSize, preprocessors=None,
aug=None, binarize=True, classes=2):
# store the batch size, preprocessors, and data augmentor,
# whether or not the labels should be binarized, along with
# the total number of classes
self.batchSize = batchSize
self.preprocessors = preprocessors
self.aug = aug
self.binarize = binarize
self.classes = classes
17
18
19
20
21
# open the HDF5 database for reading and determine the total
# number of entries in the database
self.db = h5py.File(dbPath)
self.numImages = self.db["labels"].shape[0]
On Line 7 we define the constructor to our HDF5DatasetGenerator. This class accepts a
number of arguments, two of which are required and the rest optional. I have detailed each of the
arguments below:
• dbPath: The path to our HDF5 dataset that stores our images and corresponding class labels.
• batchSize: The size of mini-batches to yield when training our network.
• preprocessors: The list of image preprocessors we are going to apply (i.e., MeanPreprocessor,
ImageToArrayPreprocessor, etc.).
• aug: Defaulting to None, we could also supply a Keras ImageDataGenerator to apply data
augmentation directly inside our HDF5DatasetGenerator.
• binarize: Typically we will store class labels as single integers inside our HDF5 dataset;
however, as we know, if we are applying categorical cross-entropy or binary cross-entropy as
our loss function, we first need to binarize the labels as one-hot encoded vectors – this switch
indicates whether or not this binarization needs to take place (which defaults to True).
• classes: The number of unique class labels in our dataset. This value is required to
accurately construct our one-hot encoded vectors during the binarization phase.
These variables are stored on Lines 12-16 so we can access them from the rest of the class.
Line 20 opens a file pointer to our HDF5 dataset file Line 21 creates a convenience variable used
to access the total number of data points in the dataset.
Next, we need to define a generator function, which as the name suggests, is responsible for
yielding batches of images and class labels to the Keras .fit_generator function when training
a network:
23
24
def generator(self, passes=np.inf):
# initialize the epoch count10.2 HDF5 Dataset Generators
25
111
epochs = 0
26
27
28
29
30
31
32
33
34
# keep looping infinitely -- the model will stop once we have
# reach the desired number of epochs
while epochs < passes:
# loop over the HDF5 dataset
for i in np.arange(0, self.numImages, self.batchSize):
# extract the images and labels from the HDF dataset
images = self.db["images"][i: i + self.batchSize]
labels = self.db["labels"][i: i + self.batchSize]
Line 23 defines the generator function which can accept an optional argument, passes.
Think of the passes value as the total number of epochs – in most cases, we don’t want our
generator to be concerned with the total number of epochs; our training methodology (fixed number
of epochs, early stopping, etc.) should be responsible for that. However, in certain situations, it’s
often helpful to provide this information to the generator.
On Line 29 we start looping over the number of desired epochs – by default, this loop will run
indefinitely until either:
1. Keras reaches training termination criteria.
2. We explicitly stop the training process (i.e., ctrl + c).
Line 31 starts looping over each batch of data points in the dataset. We extract the images and
labels of size batchSize from our HDF5 dataset on Lines 33 and 34.
Next, let’s check to see if the labels should be one-hot encoded:
36
37
38
39
# check to see if the labels should be binarized
if self.binarize:
labels = np_utils.to_categorical(labels,
self.classes)
We can then also see if any image preprocessors should be applied:
41
42
43
44
# check to see if our preprocessors are not None
if self.preprocessors is not None:
# initialize the list of processed images
procImages = []
45
46
47
48
49
50
51
# loop over the images
for image in images:
# loop over the preprocessors and apply each
# to the image
for p in self.preprocessors:
image = p.preprocess(image)
52
53
54
# update the list of processed images
procImages.append(image)
55
56
57
58
# update the images array to be the processed
# images
images = np.array(procImages)
Provided the preprocessors is not None (Line 42), we loop over each of the images in the
batch and apply each of the preprocessors by calling the preprocess method on the individual
image. Doing this enables us to chain together multiple image pre-processors.Chapter 10. Competing in Kaggle: Dogs vs. Cats
112
For example, our first pre-processor may resize the image to a fixed size via our SimplePreprocessor
class. From there we may perform mean subtraction via the MeanPreprocessor. And after that,
we’ll need to convert the image to a Keras-compatible array using the ImageToArrayPreprocessor.
At this point it should be clear why we defined all of our pre-processing classes with a preprocess
method – it allows us to chain our pre-processors together inside the data generator. The prepro-
cessed images are then converted back to a NumPy array on Line 58.
Provided we supplied an instance of aug, an ImageDataGenerator class used for data aug-
mentation, we’ll also want to apply data augmentation to the images as well:
60
61
62
63
# if the data augmenator exists, apply it
if self.aug is not None:
(images, labels) = next(self.aug.flow(images,
labels, batch_size=self.batchSize))
Finally, we can yield a 2-tuple of the batch of images and labels to the calling Keras generator:
65
66
# yield a tuple of images and labels
yield (images, labels)
67
68
69
# increment the total number of epochs
epochs += 1
70
71
72
73
def close(self):
# close the database
self.db.close()
Line 69 increments our total number of epochs after all mini-batches in the dataset have been
processed. The close method on Lines 71-73 is simply responsible for closing the pointer to the
HDF5 dataset.
Admittedly, implementing the HDF5DatasetGenerator may not “feel” like we’re doing any
deep learning. After all, isn’t this just a class responsible for yielding batches of data from a file?
Technically, yes, that is correct. However, keep in mind that practical deep learning is more than
just defining a model architecture, initializing an optimizer, and applying it to a dataset.
In reality, we need extra tools to help facilitate our ability to work with datasets, espe-
cially datasets that are too large to fit into memory. As we’ll see throughout the rest of this
book, our HDF5DatasetGenerator will come in handy a number of times – and when you start
creating your own deep learning applications/experiments, you’ll feel quite lucky to have it in your
repertoire.
10.3
Implementing AlexNet
Let’s now move on to implement the seminal AlexNet architecture by Krizhevsky et al. A table
summarizing the AlexNet architecture can be seen in Table 10.1.
Notice how our input images are assumed to be 227 × 227 × 3 pixels – this is actually the
correct input size for AlexNet. As mentioned in Chapter 9, in the original publication, Krizhevsky
et al. reported the input spatial dimensions to be 224 × 224 × 3; however, since we know 224 × 224
cannot possible by tiled with an 11 × 1 kernel, we assume there was likely a typo in the publication,
and 224 × 224 should actually be 227 × 227.
The first block of AlexNet applies 96, 11 × 11 kernels with a stride of 4 × 4, followed by a
RELU activation and max pooling with a pool size of 3 × 3 and strides of 2 × 2, resulting in an
output volume of size 55 × 55.10.3 Implementing AlexNet
Layer Type
INPUT IMAGE
CONV
ACT
BN
POOL
DROPOUT
CONV
ACT
BN
POOL
DROPOUT
CONV
ACT
BN
CONV
ACT
BN
CONV
ACT
BN
POOL
DROPOUT
FC
ACT
BN
DROPOUT
FC
ACT
BN
DROPOUT
FC
SOFTMAX
113
Output Size
227 × 227 × 3
57 × 57 × 96
57 × 57 × 96
57 × 57 × 96
16 × 16 × 96
28 × 28 × 96
28 × 28 × 256
28 × 28 × 256
28 × 28 × 256
13 × 13 × 256
13 × 13 × 256
13 × 13 × 384
13 × 13 × 384
13 × 13 × 384
13 × 13 × 384
13 × 13 × 384
13 × 13 × 384
13 × 13 × 256
13 × 13 × 256
13 × 13 × 256
13 × 13 × 256
6 × 6 × 256
4096
4096
4096
4096
4096
4096
4096
4096
1000
1000
Filter Size / Stride
11 × 11/4 × 4, K = 96
3 × 3/2 × 2
5 × 5, K = 256
3 × 3/2 × 2
3 × 3, K = 384
3 × 3, K = 384
3 × 3, K = 256
3 × 3/2 × 2
Table 10.1: A table summary of the AlexNet architecture. Output volume sizes are included for
each layer, along with convolutional filter size/pool size when relevant.114
Chapter 10. Competing in Kaggle: Dogs vs. Cats
We then apply a second CONV => RELU => POOL layer this, this time using 256, 5 × 5 filters
with 1 × 1 strides. After applying max pooling again with a pool size of 3 × 3 and strides of 2 × 2 we
are left with a 13 × 13 volume.
Next, we apply (CONV => RELU) * 3 => POOL. The first two CONV layers learn 384, 3 ×
3 filters while the final CONV learns 256, 3 × 3 filters.
After another max pooling operation, we reach our two FC layers, each with 4096 nodes and
RELU activations in between. The final layer in the network is our softmax classifier.
When AlexNet was first introduced we did not have techniques such as batch normalization – in
our implementation ,we are going to include batch normalization after the activation, as is standard
for the majority of image classification tasks using Convolutional Neural Networks. We’ll also
include a very small amount of dropout after each POOL operation to further help reduce overfitting.
To implement AlexNet, let’s create a new file named alexnet.py in the conv sub-module of
nn in pyimagesearch:
--- pyimagesearch
|
|--- __init__.py
|
|--- callbacks
|
|--- io
|
|--- nn
|
|
|--- __init__.py
|
|
|--- conv
|
|
|
|--- __init__.py
|
|
|
|--- alexnet.py
...
|
|--- preprocessing
|
|--- utils
From there, open up alexnet.py, and we’ll implement this seminal architecture:
1
2
3
4
5
6
7
8
9
10
11
# import the necessary packages
from keras.models import Sequential
from keras.layers.normalization import BatchNormalization
from keras.layers.convolutional import Conv2D
from keras.layers.convolutional import MaxPooling2D
from keras.layers.core import Activation
from keras.layers.core import Flatten
from keras.layers.core import Dropout
from keras.layers.core import Dense
from keras.regularizers import l2
from keras import backend as K
Lines 2-11 import our required Keras classes – we have used all of these layers before in previ-
ous chapters of this book so I’m going to skip explicitly describing each of them. The only import I
do want to draw your attention to is Line 10 where we import the l2 function – this method will be
responsible for applying L2 weight decay to the weight layers in the network.
Now that our imports are taken care of, let’s start the definition of AlexNet:
13
14
15
16
class AlexNet:
@staticmethod
def build(width, height, depth, classes, reg=0.0002):
# initialize the model along with the input shape to be10.3 Implementing AlexNet
17
18
19
20
115
# "channels last" and the channels dimension itself
model = Sequential()
inputShape = (height, width, depth)
chanDim = -1
21
22
23
24
25
26
# if we are using "channels first", update the input shape
# and channels dimension
if K.image_data_format() == "channels_first":
inputShape = (depth, height, width)
chanDim = 1
Line 15 defines the build method of AlexNet. Just like in all previous examples in this book,
the build method is required for constructing the actual network architecture and returning it to
the calling function. This method accepts four arguments: the width, height, and depth of the
input images, followed by the total number of class labels in the dataset. An optional parameter,
reg, controls the amount of L2 regularization we’ll be applying to the network. For larger, deeper
networks, applying regularization is critical to reducing overfitting while increasing accuracy on
the validation and testing sets.
Line 18 initializes the model itself along with the inputShape and channel dimension assum-
ing we are using “channels last” ordering. If we are instead using “channels first” ordering, we
update inputShape and chanDim (Lines 24-26).
Let’s now define the first CONV => RELU => POOL layer set in the network:
28
29
30
31
32
33
34
35
# Block #1: first CONV => RELU => POOL layer set
model.add(Conv2D(96, (11, 11), strides=(4, 4),
input_shape=inputShape, padding="same",
kernel_regularizer=l2(reg)))
model.add(Activation("relu"))
model.add(BatchNormalization(axis=chanDim))
model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))
model.add(Dropout(0.25))
Our first CONV layer will learn 96 filters, each of size 11 × 11 (Lines 28 and 29), using a stride
of 4 × 4. By applying the kernel_regularizer parameter to the Conv2D class, we can apply our
L2 weight regularization parameter – this regularization will be applied to all CONV and FC layers
in the network.
A ReLU activation is applied after our CONV, followed by a BatchNormalization (Lines 32
and 33). The MaxPooling2D is then applied to reduce our spatial dimensions (Line 34). We’ll
also apply dropout with a small probability (25 percent) to help reduce overfitting (Lines 35).
The following code block defines another CONV => RELU => POOL layer set, this time learning
256 filters, each of size 5 × 5:
37
38
39
40
41
42
43
# Block #2: second CONV => RELU => POOL layer set
model.add(Conv2D(256, (5, 5), padding="same",
kernel_regularizer=l2(reg)))
model.add(Activation("relu"))
model.add(BatchNormalization(axis=chanDim))
model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))
model.add(Dropout(0.25))
Deeper, richer features are learned in the third block of AlexNet where we stack multiple CONV
=> RELU together prior to applying a POOL operation:116
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
Chapter 10. Competing in Kaggle: Dogs vs. Cats
# Block #3: CONV => RELU => CONV => RELU => CONV => RELU
model.add(Conv2D(384, (3, 3), padding="same",
kernel_regularizer=l2(reg)))
model.add(Activation("relu"))
model.add(BatchNormalization(axis=chanDim))
model.add(Conv2D(384, (3, 3), padding="same",
kernel_regularizer=l2(reg)))
model.add(Activation("relu"))
model.add(BatchNormalization(axis=chanDim))
model.add(Conv2D(256, (3, 3), padding="same",
kernel_regularizer=l2(reg)))
model.add(Activation("relu"))
model.add(BatchNormalization(axis=chanDim))
model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))
model.add(Dropout(0.25))
The first two CONV filters learn 384, 3 × 3 filters while the third CONV learns 256, 3 × 3 filters.
Again, stacking multiple CONV =>RELU layers on to of each other prior to applying a destructive
POOL layer enables our network to learn richer, and potentially more discriminating features.
From there we collapse our multi-dimensional representation down into a standard feedforward
network using two fully-connected layers (4096 nodes each):
61
62
63
64
65
66
# Block #4: first set of FC => RELU layers
model.add(Flatten())
model.add(Dense(4096, kernel_regularizer=l2(reg)))
model.add(Activation("relu"))
model.add(BatchNormalization())
model.add(Dropout(0.5))
67
68
69
70
71
72
# Block #5: second set of FC => RELU layers
model.add(Dense(4096, kernel_regularizer=l2(reg)))
model.add(Activation("relu"))
model.add(BatchNormalization())
model.add(Dropout(0.5))
Batch normalization is applied after each activation in the FC layer sets, just as in the CONV
layers above. Dropout, with a larger probability of 50 percent, is applied after every FC layer set, as
is standard with the vast majority of CNNs.
Finally, we define the softmax classifier using the desired number of classes and return the
resulting model to the calling function:
74
75
76
# softmax classifier
model.add(Dense(classes, kernel_regularizer=l2(reg)))
model.add(Activation("softmax"))
77
78
79
# return the constructed network architecture
return model
As you can see, implementing AlexNet is a fairly straightforward process, especially when you
have the “blueprint” of the architecture presented in Table 10.1 above. Whenever implementing
architectures from publications, try to see if they provide such a table as it makes implementation10.4 Training AlexNet on Kaggle: Dogs vs. Cats
117
much easier. For your own network architectures, use Chapter 19 of the Starter Bundle on
visualizing network architectures to aid you in ensuring your input volume and output volume sizes
are what you expect.
10.4
Training AlexNet on Kaggle: Dogs vs. Cats
Now that the AlexNet architecture has been defined, let’s apply it to the Kaggle Dogs vs. Cats
challenge. Open up a new file, name it train_alexnet.py, and insert the following code:
1
2
3
4
# import the necessary packages
# set the matplotlib backend so figures can be saved in the background
import matplotlib
matplotlib.use("Agg")
5
6
7
8
9
10
11
12
13
14
15
16
17
18
# import the necessary packages
from config import dogs_vs_cats_config as config
from pyimagesearch.preprocessing import ImageToArrayPreprocessor
from pyimagesearch.preprocessing import SimplePreprocessor
from pyimagesearch.preprocessing import PatchPreprocessor
from pyimagesearch.preprocessing import MeanPreprocessor
from pyimagesearch.callbacks import TrainingMonitor
from pyimagesearch.io import HDF5DatasetGenerator
from pyimagesearch.nn.conv import AlexNet
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import Adam
import json
import os
Lines 3 and 4 import matplotlib, while ensuring the backend is set such that we can save figures
and plots to disk as our network trains. We then implement our pre-processors on Lines 8-11. The
HDF5DatasetGenerator is then imported on Line 13 so we can access batches of training data
from our serialized HDF5 dataset. AlexNet is also implement don Line 14.
Our next code block handles initializing our data augmentation generator via the ImageDataGenerator
class:
20
21
22
23
# construct the training image generator for data augmentation
aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15,
width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,
horizontal_flip=True, fill_mode="nearest")
Let’s take the time now to initialize each of our image pre-processors:
25
26
# load the RGB means for the training set
means = json.loads(open(config.DATASET_MEAN).read())
27
28
29
30
31
32
# initialize the image preprocessors
sp = SimplePreprocessor(227, 227)
pp = PatchPreprocessor(227, 227)
mp = MeanPreprocessor(means["R"], means["G"], means["B"])
iap = ImageToArrayPreprocessor()118
Chapter 10. Competing in Kaggle: Dogs vs. Cats
On Line 26 we load the serialized RGB means from disk – these are the means for each of the
respective Red, Green, and Blue channels across our training dataset. These values will later be
passed into a MeanPreprocessor for mean subtraction normalization.
Line 29 instantiates a SimplePreprocessor used to resize an input image down to 227 ×
227 pixels. This pre-processor will be used in the validation data generator as our input images are
256 × 256 pixels; however, AlexNet is intended to handle only 227 × 227 images (hence why we
need to resize the image during validation).
Line 30 instantiates a PatchPreprocessor – this pre-processor will randomly sample 227 ×
227 regions from the 256 × 256 input images during training time, serving as a second form of data
augmentation.
We then initialize the MeanPreprocessor on Line 31 using our respective, Red, Green, and
Blue averages. Finally, the ImageToArrayPreprocessor (Line 32) is used to convert images to
Keras-compatible arrays.
Given our pre-processors, let’s define the HDF5DatasteGenerator for both the training and
validation data:
34
35
36
37
38
# initialize the training and validation dataset generators
trainGen = HDF5DatasetGenerator(config.TRAIN_HDF5, 128, aug=aug,
preprocessors=[pp, mp, iap], classes=2)
valGen = HDF5DatasetGenerator(config.VAL_HDF5, 128,
preprocessors=[sp, mp, iap], classes=2)
Lines 35 and 36 create our training dataset generator. Here we supply the path to our training
HDF5 file, indicating that we should use batch sizes of 128 images, data augmentation, and three
pre-processors: patch, mean, and image to array, respectively.
Lines 37 and 38 are responsible for instantiating the testing generator. This time we’ll supply
the path to the validation HDF5 file, use a batch size of 128, no data augmentation, and a simple
pre-processor rather than a patch pre-processor (since data augmentation is not applied to validation
data).
Finally, we are ready to initialize the Adam optimizer and AlexNet architecture:
40
41
42
43
44
45
46
# initialize the optimizer
print("[INFO] compiling model...")
opt = Adam(lr=1e-3)
model = AlexNet.build(width=227, height=227, depth=3,
classes=2, reg=0.0002)
model.compile(loss="binary_crossentropy", optimizer=opt,
metrics=["accuracy"])
47
48
49
50
51
# construct the set of callbacks
path = os.path.sep.join([config.OUTPUT_PATH, "{}.png".format(
os.getpid())])
callbacks = [TrainingMonitor(path)]
On Line 42 we instantiate the Adam optimizer using the default learning rate of 0.001. The
reason I choose Adam for this experiment (rather than SGD) is two-fold:
1. I wanted to give you exposure to using the more advanced optimizers we covered in Chapter
7.
2. Adam performs better on this classification task than SGD (which I know from the multiple
previous experiments I ran before publishing this book).10.4 Training AlexNet on Kaggle: Dogs vs. Cats
119
We then initialize AlexNet on Lines 43 and 44, indicating that each input image will have a
width of 227 pixels, a height of 227 pixels, 3 channels, and the dataset itself will have two classes
(one for dogs, and another for cats). We’ll also apply a small regularization penalty of 0.0002 to
help combat overfitting and increase the ability of our model to generalize to the testing set.
We’ll use binary cross-entropy rather than categorical cross-entropy (Lines 45 and 46) as this
is only a two-class classification problem. We’ll also define a TrainingMonitor callback on Line
51 so we can monitor the performance of our network as it trains.
Speaking of training the network, let’s do that now:
53
54
55
56
57
58
59
60
61
# train the network
model.fit_generator(
trainGen.generator(),
steps_per_epoch=trainGen.numImages // 128,
validation_data=valGen.generator(),
validation_steps=valGen.numImages // 128,
epochs=75,
max_queue_size=128 * 2,
callbacks=callbacks, verbose=1)
To train AlexNet on the Kaggle Dogs vs. Cats dataset using our HDF5DatasetGenerator, we
need to use the fit_generator method of the model. First, we pass in trainGen.generator(),
the HDF5 generator used to construct mini-batches of training data (Line 55). To determine the
number of batches per epoch, we divide the total number of images in the training set by our batch
size (Line 56). We do the same on Lines 57 and 58 for the validation data. Finally, we’ll indicate
that AlexNet is to be trained for 75 epochs.
The last step is to simply serialize our model to file after training, along with closing each of
the training and testing HDF5 datasets, respectively:
63
64
65
# save the model to file
print("[INFO] serializing model...")
model.save(config.MODEL_PATH, overwrite=True)
66
67
68
69
# close the HDF5 datasets
trainGen.close()
valGen.close()
To train AlexNet on the Kaggle Dogs vs. Cats dataset, execute the following command:
$ python train_alexnet.py
Epoch 73/75
415s - loss: 0.4862 - acc: 0.9126 - val_loss: 0.6826 - val_acc: 0.8602
Epoch 74/75
408s - loss: 0.4865 - acc: 0.9166 - val_loss: 0.6894 - val_acc: 0.8721
Epoch 75/75
401s - loss: 0.4813 - acc: 0.9166 - val_loss: 0.4195 - val_acc: 0.9297
[INFO] serializing model...
A plot of the training and validation loss/accuracy over the 75 epochs can be seen in Figure 10.4.
Overall we can see that the training and accuracy plots correlate well with each other, although we
could help stabilize variations in validation loss towards the end of the 75 epoch cycle by applying120
Chapter 10. Competing in Kaggle: Dogs vs. Cats
Figure 10.4: Training AlexNet on the Kaggle Dogs vs. Cats competition where we obtain 92.97%
classification accuracy on our validation set. Our learning curve is stable with changes in training
accuracy/loss being reflected in the respective validation split.
a bit of learning rate decay. Examining the classification report of AlexNet on the Dogs vs. Cats
dataset, we see our obtained obtained 92.97% on the validation set.
In the next section, we’ll evaluate AlexNet on the testing set using both the standard method
and over-sampling method. As our results will demonstrate, using over-sampling can increase your
classification from 1-3% depending on your dataset and network architecture.
10.5
Evaluating AlexNet
To evaluate AlexNet on the testing set using both our standard method and over-sampling technique,
let’s create a new file named crop_accuracy.py:
--- dogs_vs_cats
|
|--- config
|
|--- build_dogs_vs_cats.py
|
|--- crop_accuracy.py
|
|--- extract_features.py
|
|--- train_alexnet.py
|
|--- train_model.py
|
|--- output
From there, open crop_accuracy.py and insert the following code:
1
2
# import the necessary packages
from config import dogs_vs_cats_config as config10.5 Evaluating AlexNet
3
4
5
6
7
8
9
10
11
12
121
from pyimagesearch.preprocessing import ImageToArrayPreprocessor
from pyimagesearch.preprocessing import SimplePreprocessor
from pyimagesearch.preprocessing import MeanPreprocessor
from pyimagesearch.preprocessing import CropPreprocessor
from pyimagesearch.io import HDF5DatasetGenerator
from pyimagesearch.utils.ranked import rank5_accuracy
from keras.models import load_model
import numpy as np
import progressbar
import json
Lines 2-12 import our required Python packages. Line 2 imports our Python configuration
file for the Dogs vs. Cats challenge. We’ll also import our image preprocessors on Lines 3-6,
including the ImageToArrayPreprocessor, SimplePreprocessor, MeanPreprocessor, and
CropPreprocessor. The HDF5DatasetGenerator is required so we can access the testing set of
our dataset and obtain predictions on this data using our pre-trained model.
Now that our imports are complete, let’s load the RGB means from disk, initialize our image
pre-preprocessors, and load the pre-trained AlexNet network:
14
15
# load the RGB means for the training set
means = json.loads(open(config.DATASET_MEAN).read())
16
17
18
19
20
21
# initialize the image preprocessors
sp = SimplePreprocessor(227, 227)
mp = MeanPreprocessor(means["R"], means["G"], means["B"])
cp = CropPreprocessor(227, 227)
iap = ImageToArrayPreprocessor()
22
23
24
25
# load the pretrained network
print("[INFO] loading model...")
model = load_model(config.MODEL_PATH)
Before we apply over-sampling and 10-cropping, let’s first obtain a baseline on the testing set
using only the original testing image as input to our network:
27
28
29
30
31
32
33
# initialize the testing dataset generator, then make predictions on
# the testing data
print("[INFO] predicting on test data (no crops)...")
testGen = HDF5DatasetGenerator(config.TEST_HDF5, 64,
preprocessors=[sp, mp, iap], classes=2)
predictions = model.predict_generator(testGen.generator(),
steps=testGen.numImages // 64, max_queue_size=64 * 2)
34
35
36
37
38
# compute the rank-1 and rank-5 accuracies
(rank1, _) = rank5_accuracy(predictions, testGen.db["labels"])
print("[INFO] rank-1: {:.2f}%".format(rank1 * 100))
testGen.close()
Lines 30 and 31 initialize the HDF5DatasetGenerator to access the testing dataset in batches
of 64 images. Since we are obtaining a baseline, we’ll use only the SimplePreprocessor to
resize the 256 × 256 input images down to 227 × 227 pixels, followed by mean normalization and122
Chapter 10. Competing in Kaggle: Dogs vs. Cats
converting the batch to a Keras-compatible array of images. Lines 32 and 33 then use the generator
to evaluate AlexNet on the dataset.
Given our predictions, we can compute our accuracy on the test set (Lines 36-38). Notice
here how we only care about the rank1 accuracy, which is because the Dogs vs. Cats is a 2-class
dataset – computing the rank-5 accuracy for a 2-class dataset would trivially report 100 percent
classification accuracy.
Now that we have a baseline for the standard evaluation technique, let’s move on to over-
sampling:
40
41
42
43
44
# re-initialize the testing set generator, this time excluding the
# ‘SimplePreprocessor‘
testGen = HDF5DatasetGenerator(config.TEST_HDF5, 64,
preprocessors=[mp], classes=2)
predictions = []
45
46
47
48
49
50
# initialize the progress bar
widgets = ["Evaluating: ", progressbar.Percentage(), " ",
progressbar.Bar(), " ", progressbar.ETA()]
pbar = progressbar.ProgressBar(maxval=testGen.numImages // 64,
widgets=widgets).start()
On Lines 42 and 43 we re-initialize the HDF5DatasetGenerator, this time instructing it to
use just the MeanPreprocessor – we’ll apply both over-sampling and Keras-array conversion later
in the pipeline. Lines 47-50 also initialize progressbar widgets to our screen if we are interested
in having the evaluating progress displayed to our screen.
Given the re-instantiated testGen, we are now ready to apply the 10-cropping technique:
52
53
54
55
56
57
58
59
60
# loop over a single pass of the test data
for (i, (images, labels)) in enumerate(testGen.generator(passes=1)):
# loop over each of the individual images
for image in images:
# apply the crop preprocessor to the image to generate 10
# separate crops, then convert them from images to arrays
crops = cp.preprocess(image)
crops = np.array([iap.preprocess(c) for c in crops],
dtype="float32")
61
62
63
64
65
# make predictions on the crops and then average them
# together to obtain the final prediction
pred = model.predict(crops)
predictions.append(pred.mean(axis=0))
66
67
68
# update the progress bar
pbar.update(i)
On Line 53 we start looping over every batch of images in the testing generator. Typically
an HDF5DatasetGenerator is set to loop forever until we explicitly tell it to stop (normally by
setting a maximum number of iterations via Keras when training); however, since we are now
evaluating, we can supply passes=1 to indicate the testing data only needs to be looped over once.
Then, for each image in the images batch (Line 55), we apply the 10-crop pre-processor on
Line 58, which converts the image into an array of ten 227 × 227 images. These 227 × 227 crops
were extracted from the original 256 × 256 batch based on the:10.6 Obtaining a Top-5 Spot on the Kaggle Leaderboard
123
• Top-left corner
• Top-right corner
• Bottom-right corner
• Bottom-left corner
• Corresponding horizontal flips
Once we have the crops, we pass them through the model on Line 64 for prediction. The final
prediction (Line 65) is the average of the probabilities across all ten crops.
Our final code block handles displaying the accuracy of the over-sampling method:
70
71
72
73
74
75
# compute the rank-1 accuracy
pbar.finish()
print("[INFO] predicting on test data (with crops)...")
(rank1, _) = rank5_accuracy(predictions, testGen.db["labels"])
print("[INFO] rank-1: {:.2f}%".format(rank1 * 100))
testGen.close()
To evaluate AlexNet on the Kaggle Dog vs. Cats dataset, just execute the following command:
$ python crop_accuracy.py
[INFO] loading model...
[INFO] predicting on test data (no crops)...
[INFO] rank-1: 92.60%
Evaluating: 100% |####################################| Time: 0:01:12
[INFO] predicting on test data (with crops)...
[INFO] rank-1: 94.00%
As our results demonstrate, we reach 92.60% accuracy on the testing set. However, by applying
the 10-crop over-sampling method, we are able to boost classification accuracy to 94.00%, an
increase of 1.4%, which this was all accomplished simply by taking multiple crops of the input
image and averaging the results. This straightforward, uncomplicated trick is an easy way to eke
out an extra few percentage points when evaluating your network.
10.6
Obtaining a Top-5 Spot on the Kaggle Leaderboard
Of course, if you were to look at the Kaggle Dogs vs. Cats leaderboard, you would notice that to
even break into the top-25 position we would need 96.69% accuracy, which our current method is
not capable of reaching. So, what’s the solution?
The answer is transfer learning, specifically transfer learning via feature extraction. While the
ImageNet dataset consists of 1,000 object categories, a good portion of those include both dog
species and cat species. Therefore, a network trained on ImageNet could not only tell you if an
image was of a dog or a cat, but what particular breed the animal is as well. Given that a network
trained on ImageNet must be able to discriminate between such fine-grained animals, it’s natural to
hypothesize that the features extracted from a pre-trained network would likely lend itself well to
claiming a top spot on the Kaggle Dogs vs. Cats leaderboard.
To test this hypothesis, let’s first extract features from the pre-trained ResNet architecture and
then train a Logistic Regression classifier on top of these features.
10.6.1
Extracting Features Using ResNet
The transfer learning via feature extraction technique we’ll be using in this section is heavily based
on Chapter 3. I’ll review the entire contents of extract_features.py as a matter of completeness;124
Chapter 10. Competing in Kaggle: Dogs vs. Cats
however, please refer to Chapter 3 if you require further knowledge on feature extraction using
CNNs.
To get started, open up a new file, name it extract_features.py, and insert the following
code:
1
2
3
4
5
6
7
8
9
10
11
12
13
# import the necessary packages
from keras.applications import ResNet50
from keras.applications import imagenet_utils
from keras.preprocessing.image import img_to_array
from keras.preprocessing.image import load_img
from sklearn.preprocessing import LabelEncoder
from pyimagesearch.io import HDF5DatasetWriter
from imutils import paths
import numpy as np
import progressbar
import argparse
import random
import os
Lines 2-13 import our required Python packages. We import the ResNet50 class on Line 2 so
we can access the pre-trained ResNet architecture. We’ll also use the HDF5DatasetWriter on
Line 7 so we can write the extracted features to an efficiently HDF5 file format.
From there, let’s parse our command line arguments:
15
16
17
18
19
20
21
22
23
24
25
# construct the argument parse and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-d", "--dataset", required=True,
help="path to input dataset")
ap.add_argument("-o", "--output", required=True,
help="path to output HDF5 file")
ap.add_argument("-b", "--batch-size", type=int, default=16,
help="batch size of images to be passed through network")
ap.add_argument("-s", "--buffer-size", type=int, default=1000,
help="size of feature extraction buffer")
args = vars(ap.parse_args())
26
27
28
# store the batch size in a convenience variable
bs = args["batch_size"]
We only need two required command line arguments here, --dataset, which is the path to
the input dataset of Dogs vs. Cats images, along with --output, the path to the output HDF5 file
containing the features extracted via ResNet.
Next, let’s grab the paths to the Dogs vs. Cats images residing on disk and then use the file
paths to extract the label names:
30
31
32
33
34
35
# grab the list of images that we’ll be describing then randomly
# shuffle them to allow for easy training and testing splits via
# array slicing during training time
print("[INFO] loading images...")
imagePaths = list(paths.list_images(args["dataset"]))
random.shuffle(imagePaths)10.6 Obtaining a Top-5 Spot on the Kaggle Leaderboard
125
36
37
38
39
40
41
# extract the class labels from the image paths then encode the
# labels
labels = [p.split(os.path.sep)[-1].split(".")[0] for p in imagePaths]
le = LabelEncoder()
labels = le.fit_transform(labels)
Now we can load our pre-trained ResNet50 weights from disk (excluding the FC layers):
43
44
45
# load the ResNet50 network
print("[INFO] loading network...")
model = ResNet50(weights="imagenet", include_top=False)
In order to store the features extracted from ResNet50 to disk, we need to instantiate a
HDF5DatasetWriter object:
47
48
49
50
51
# initialize the HDF5 dataset writer, then store the class label
# names in the dataset
dataset = HDF5DatasetWriter((len(imagePaths), 2048),
args["output"], dataKey="features", bufSize=args["buffer_size"])
dataset.storeClassLabels(le.classes_)
The final average pooling layer of ResNet50 is 2048-d, hence why we supply a value of 2048
as the dimensionality to our HDF5datasetWriter.
We’ll also initialize a progressbar so we can keep track of the feature extraction process:
53
54
55
56
57
# initialize the progress bar
widgets = ["Extracting Features: ", progressbar.Percentage(), " ",
progressbar.Bar(), " ", progressbar.ETA()]
pbar = progressbar.ProgressBar(maxval=len(imagePaths),
widgets=widgets).start()
Extracting features from a dataset using a CNN is the same as it was in Chapter 3. First, we
loop over the imagePaths in batches:
59
60
61
62
63
64
65
66
# loop over the images in batches
for i in np.arange(0, len(imagePaths), bs):
# extract the batch of images and labels, then initialize the
# list of actual images that will be passed through the network
# for feature extraction
batchPaths = imagePaths[i:i + bs]
batchLabels = labels[i:i + bs]
batchImages = []
Followed by pre-processing each image:
68
69
70
# loop over the images and labels in the current batch
for (j, imagePath) in enumerate(batchPaths):
# load the input image using the Keras helper utilityChapter 10. Competing in Kaggle: Dogs vs. Cats
126
71
72
73
# while ensuring the image is resized to 224x224 pixels
image = load_img(imagePath, target_size=(224, 224))
image = img_to_array(image)
74
75
76
77
78
79
# preprocess the image by (1) expanding the dimensions and
# (2) subtracting the mean RGB pixel intensity from the
# ImageNet dataset
image = np.expand_dims(image, axis=0)
image = imagenet_utils.preprocess_input(image)
80
81
82
# add the image to the batch
batchImages.append(image)
And then passing the batchImages through the network architecture, enabling us to extract
features from the final POOL layer of ResNet50:
84
85
86
87
# pass the images through the network and use the outputs as
# our actual features
batchImages = np.vstack(batchImages)
features = model.predict(batchImages, batch_size=bs)
88
89
90
91
# reshape the features so that each image is represented by
# a flattened feature vector of the ‘MaxPooling2D‘ outputs
features = features.reshape((features.shape[0], 2048))
These extracted features are then added to our dataset:
93
94
95
# add the features and labels to our HDF5 dataset
dataset.add(features, batchLabels)
pbar.update(i)
96
97
98
99
# close the dataset
dataset.close()
pbar.finish()
To utilize ResNet to extract features from the Dogs vs. Cats dataset, simply execute the
following command:
$ python extract_features.py --dataset ../datasets/kaggle_dogs_vs_cats/train \
--output ../datasets/kaggle_dogs_vs_cats/hdf5/features.hdf5
[INFO] loading images...
[INFO] loading network...
Extracting Features: 100% |####################################| Time: 0:06:18
After the command finishes executing, you should now have a file named dogs_vs_cats_features.hdf5
in your output directory:
$ ls -l output/dogs_vs_cats_features.hdf5
-rw-rw-r-- adrian 409806272 Jun 3 07:17 output/dogs_vs_cats_features.hdf5
Given these features, we can train a Logistic Regression classier on top of them to (ideally)
obtain a top-5 spot on the Kaggle Dogs vs. Cats leaderboard.10.6 Obtaining a Top-5 Spot on the Kaggle Leaderboard
10.6.2
127
Training a Logistic Regression Classifier
To train our Logistic Regression classifier, open up a new file and name it train_model.py. From
there, we can get started:
1
2
3
4
5
6
7
8
# import the necessary packages
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
import argparse
import pickle
import h5py
Lines 2-8 import our required Python packages. We’ll then parse our command line arguments:
10
11
12
13
14
15
16
17
18
# construct the argument parse and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-d", "--db", required=True,
help="path HDF5 database")
ap.add_argument("-m", "--model", required=True,
help="path to output model")
ap.add_argument("-j", "--jobs", type=int, default=-1,
help="# of jobs to run when tuning hyperparameters")
args = vars(ap.parse_args())
We only need two switches here, the path to the input HDF5 --db, along with the path to the
output Logistic Regression --model after training is complete.
Next, let’s open the HDF5 dataset for reading and determine the training and testing split – 75%
of the data for training and 25% for testing:
20
21
22
23
24
# open the HDF5 database for reading then determine the index of
# the training and testing split, provided that this data was
# already shuffled *prior* to writing it to disk
db = h5py.File(args["db"], "r")
i = int(db["labels"].shape[0] * 0.75)
Given this feature split, we’ll perform a grid search over the C hyperparameter of the LogisticRegression
classifier:
26
27
28
29
30
31
32
33
# define the set of parameters that we want to tune then start a
# grid search where we evaluate our model for each value of C
print("[INFO] tuning hyperparameters...")
params = {"C": [0.0001, 0.001, 0.01, 0.1, 1.0]}
model = GridSearchCV(LogisticRegression(), params, cv=3,
n_jobs=args["jobs"])
model.fit(db["features"][:i], db["labels"][:i])
print("[INFO] best hyperparameters: {}".format(model.best_params_))
Once we’ve found the best choice of C, we can generate a classification report for the testing
set:Chapter 10. Competing in Kaggle: Dogs vs. Cats
128
35
36
37
38
39
# generate a classification report for the model
print("[INFO] evaluating...")
preds = model.predict(db["features"][i:])
print(classification_report(db["labels"][i:], preds,
target_names=db["label_names"]))
40
41
42
43
# compute the raw accuracy with extra precision
acc = accuracy_score(db["labels"][i:], preds)
print("[INFO] score: {}".format(acc))
And finally, the trained model can be serialized to disk for later use, if we so wish:
45
46
47
48
49
# serialize the model to disk
print("[INFO] saving model...")
f = open(args["model"], "wb")
f.write(pickle.dumps(model.best_estimator_))
f.close()
50
51
52
# close the database
db.close()
To train our model on the ResNet50 features, simply execute the following command:
python train_model.py --db ../datasets/kaggle_dogs_vs_cats/hdf5/features.hdf5 \
--model dogs_vs_cats.pickle
[INFO] tuning hyperparameters...
[INFO] best hyperparameters: {’C’: 0.001}
[INFO] evaluating...
precision
recall f1-score
support
cat
dog 0.99
0.98 0.98
0.99 0.99
0.99 3160
3090
avg / total 0.99 0.99 0.99 6250
[INFO] score: 0.98688
[INFO] saving model...
As you can see from the output, our approach of using transfer learning via feature extraction
yields an impressive accuracy of 98.69%, enough for us to claim the #2 spot on the Kaggle Dogs
vs. Cats leaderboard.
10.7
Summary
In this chapter we took a deep dive into the Kaggle Dogs vs. Cats dataset and studied to methods to
obtain > 90% classification accuracy on it:
1. Training AlexNet from scratch.
2. Applying transfer learning via ResNet.
The AlexNet architecture is a seminal work first introduced by Krizhevsky et al. in 2012 [6].
Using our implementation of AlexNet, we reached 94 percent classification accuracy. This is a very
respectable accuracy, especially for a network trained from scratch. Further accuracy can likely be
obtained by:10.7 Summary
129
1. Obtaining more training data.
2. Applying more aggressive data augmentation.
3. Deepening the network.
However, the 94 percent we obtained is not even enough for us to break our way into the
top-25 leaderboard, let alone the top-5. Thus, to obtain our top-5 placement, we relied on transfer
learning via feature extraction, specifically, the ResNet50 architecture trained on the ImageNet
dataset. Since ImageNet contains many examples of both dog and cat breeds, applying a pre-trained
network to this task is a natural, easy method to ensure we obtain higher accuracy with less effort.
As our results demonstrated, we were able to obtain 98.69% classification accuracy, high enough
to claim the second position on the Kaggle Dogs vs. Cats leaderboard.11. GoogLeNet
In this chapter, we will study the GoogLeNet architecture, introduced by Szegedy et al. in their
2014 paper, Going Deeper With Convolutions [17]. This paper is important for two reasons.
First, the model architecture is tiny compared to AlexNet and VGGNet (≈ 28MB for the weights
themselves). The authors are able to obtain such a dramatic drop in network architecture size (while
still increasing the depth of the overall network) by removing fully-connected layers and instead
using global average pooling. Most of the weights in a CNN can be found in the dense FC layers –
if these layers can be removed, the memory savings are massive.
Secondly, the Szegedy et al. paper makes usage of a network in network or micro-architecture when
constructing the overall macro-architecture. Up to this point, we have seen only sequential neural
networks where the output of one network feeds directly into the next. We are now going to see
micro-architectures, small building blocks that are used inside the rest of the architecture, where
the output from one layer can split into a number of various paths and be rejoined later.
Specifically, Szegedy et al. contributed the Inception module to the deep learning community, a
building block that fits into a Convolutional Neural Network enabling it to learn CONV layers with
multiple filter sizes, turning the module into a multi-level feature extractor.
Micro-architectures such as Inception have inspired other important variants including the
Residual module in ResNet [24] and the Fire module in SqueezeNet [32]. We’ll be discussing
the Inception module (and its variants) later in this chapter. Once we’ve examined the Inception
module and ensure we know how it works, we’ll then implement a smaller version of GoogLeNet
called “MiniGoogLeNet” – we’ll train this architecture on the CIFAR-10 dataset and obtain higher
accuracy than in any of our previous chapters.
From there, we’ll move on to the more difficult cs231n Tiny ImageNet Challenge [4]. This
challenge is offered to students enrolled in Stanford’s cs231n Convolutional Neural Networks for
Visual Recognition class [39] as part of their final project. It means to give them a taste of the
challenges associated with large scale deep learning on modern architectures, without being as
time-consuming or taxing to work with as the entire ImageNet dataset.
By training GoogLeNet from scratch on Tiny ImageNet, we’ll demonstrate how to obtain a top
ranking position on the Tiny ImageNet leaderboard. And in our next chapter, we’ll utilize ResNetChapter 11. GoogLeNet
132
to claim the top position from models trained from scratch.
Let’s go ahead and get this chapter started by discussing the Inception module.
11.1
The Inception Module (and its Variants)
Modern state-of-the-art Convolutional Neural Networks utilize micro-architectures, also called
network-in-network modules, originally proposed by Lin et al. [40]. I personally prefer the term
micro-architecture as it better describes these modules as building blocks in context of the overall
macro-architecture (i.e., what you actually build and train).
Micro-architectures are small building blocks designed by deep learning practitioners to enable
networks to learn (1) faster and (2) more efficiently, all while increasing network depth. These
micro-architecture building blocks are stacked, along with conventional layer types such as CONV,
POOL, etc., to form the overall macro-architecture.
In 2014, Szegedy et al. introduced the Inception module. The general idea behind the Inception
module is two-fold:
1. It can be hard to decide the size of the filter you need to learn at a given CONV layers.
Should they be 5 × 5 filters? What about 3 × 3 filters? Should we learn local features
using 1 × 1 filters? Instead, why not learn them all and let the model decide? Inside the
Inception module, we learn all three 5 × 5, 3 × 3, and 1 × 1 filters (computing them in parallel)
concatenating the resulting feature maps along the channel dimension. The next layer in
the GoogLeNet architecture (which could be another Inception module) receives these
concatenated, mixed filters and performs the same process. Taken as a whole, this process
enables GoogLeNet to learn both local features via smaller convolutions and abstracted
features with larger convolutions – we don’t have to sacrifice our level of abstraction at the
expense of smaller features.
2. By learning multiple filter sizes, we can turn the module into a multi-level feature extractor.
The 5 × 5 filters have a larger receptive size and can learn more abstract features. The
1 × 1 filters are by definition local. The 3 × 3 filters sit as a balance in between.
11.1.1
Inception
Now that we’ve discussed the motivation behind the Inception module, let’s look at the actual
module itself in Figure 11.1.
R
An activation function (ReLU) is implicitly applied after every CONV layer. To save space,
this activation function was not included in the network diagram above. When we implement
GoogLeNet, you will see how this activation is used in the Inception module.
Specifically take note of how the Inception module branches into four distinct paths from the
input layer. The first branch in the Inception module simply learns a series of 1 × 1 local features
from the input.
The second batch first applies 1 × 1 convolution, not only as a form of learning local features,
but instead as dimensionality reduction. Larger convolutions (i.e., 3 × 3 and 5 × 5) by definition
take more computation to perform. Therefore, if we can reduce the dimensionality of the inputs
to these larger filters by applying 1 × 1 convolutions, we can reduce the amount of computation
required by our network. Therefore, the number of filters learned in the 1 × 1 CONV in the second
branch will always be smaller than the number of 3 × 3 filters learned directly afterward.
The third branch applies the same logic as the second branch, only this time with the goal of
learning 5 × 5 filters. We once again reduce dimensionality via 1 × 1 convolutions, then feed the
output into the 5 × 5 filters.11.1 The Inception Module (and its Variants)
133
Figure 11.1: The original Inception module used in GoogLeNet. The Inception module acts as
a “multi-level feature extractor” by computing 1 × 1, 3 × 3, and 5 × 5 convolutions within the
same module of the network. Figure from Szegedy et al., 2014 [17].
The fourth and final branch of the Inception module performs 3 × 3 max pooling with a stride
of 1 × 1 – this branch is commonly referred to as the pool projection branch. Historically, models
that perform pooling have demonstrated an ability to obtain higher accuracy, although we now
know through the work of Springenberg et al. in their 2014 paper, Striving for Simplicity: The All
Convolutional Net [41] that this isn’t necessarily true, and that POOL layers can be replaced with
CONV layers for reducing volume size.
In the case of Szegedy et al., this POOL layer was added simply due to the fact that it was
thought that they were needed for CNNs to perform reasonably. The output of the POOL is then fed
into another series of 1 × 1 convolutions to learn local features.
Finally, all four branches of the Inception module converge where they are concatenated together
along the channel dimension. Special care is taken during the implementation (via zero padding) to
ensure the output of each branch has the same volume size, thereby allowing the outputs to be
concatenated. The output of the Inception module is then fed into the next layer in the network.
In practice, we often stack multiple Inception modules on top of each other before performing a
pooling operation to reduce volume size.
11.1.2
Miniception
Of course, the original Inception module was designed for GoogLeNet such that it could be trained
on the ImageNet dataset (where each input image is assumed to be 224 × 224 × 3) and obtain
state-of-the-art accuracy. For smaller datasets (with smaller image spatial dimensions) where fewer
network parameters are required, we can simplify the Inception module.
I first became aware of the “Miniception” module from a tweet by @ericjang11 (https://twitter.com/ericjang11)
and @pluskid (https://twitter.com/pluskid) where they beautifully visualize a smaller variant of
Inception used when training the CIFAR-10 dataset (Figure 11.2; credit to @ericjang11 and
@pluskid).
After doing a bit of research, it turns out that this graphic was from Zhang et al.’s 2017
publication, Understanding Deep Learning Requires Re-Thinking Generalization [42]. The top row
of the figure describes three modules used in their MiniGoogLeNet implementation:
• Left: A convolution module responsible for performing convolution, batch normalization,
and activation.134
Chapter 11. GoogLeNet
Figure 11.2: The Miniception architecture consists of building blocks including a convolution
module, Inception module, and Downsample module. These modules are put together to form the
overall architecture.
• Middle: The Miniception module which performs two sets of convolutions, one for 1 ×
1 filters and the other for 3 × 3 filters, then concatenates the results. No dimensionality
reduction is performed before the 3 × 3 filter as (1) the input volumes will be smaller already
(since we’ll be using the CIFAR-10 dataset) and (2) to reduce the number of parameters in
the network.
• Right: A downsample module which applies both a convolution and max pooling to reduce
dimensionality, then concatenates across the filter dimension.
These building blocks are then used to build the MiniGoogLeNet architecture on the bottom row.
You’ll notice here that the authors placed the batch normalization before the activation (presumably
because this is what Szegedy et al. did as well), in contrast to what is now recommended when
implementing CNNs.
In this book I have stuck with the implementation of the original author’s work, placing the
batch normalization before activation in order to replicate results. In your own experiments, consider
swapping this order.
In our next section, we’ll implement the MiniGoogLeNet architecture and apply it to the
CIFAR-10 dataset. From there, we’ll be ready to implement the full Inception module and tackle
the cs231n Tiny ImageNet challenge.
11.2
MiniGoogLeNet on CIFAR-10
In this section, we are going to implement the MiniGoogLeNet architecture using the Miniception
module. We’ll then train MiniGoogLeNet on the CIFAR-10 dataset. As our results will demonstrate,11.2 MiniGoogLeNet on CIFAR-10
135
this architecture will obtain > 90% accuracy on CIFAR-10, far better than all of our previous
attempts.
11.2.1
Implementing MiniGoogLeNet
To get started, let’s first create a file named minigooglenet.py inside the conv module of
pyimagesearch.nn – this is where our implementation of the MiniGoogLeNet class will live:
--- pyimagesearch
|
|--- __init__.py
|
|--- callbacks
|
|--- io
|
|--- nn
|
|
|--- __init__.py
|
|
|--- conv
|
|
|
|--- __init__.py
|
|
|
|--- alexnet.py
|
|
|
|--- lenet.py
|
|
|
|--- minigooglenet.py
|
|
|
|--- minivggnet.py
|
|
|
|--- fcheadnet.py
|
|
|
|--- shallownet.py
|
|--- preprocessing
|
|--- utils
From there, open up minigooglenet.py and insert the following code:
1
2
3
4
5
6
7
8
9
10
11
12
13
# import the necessary packages
from keras.layers.normalization import BatchNormalization
from keras.layers.convolutional import Conv2D
from keras.layers.convolutional import AveragePooling2D
from keras.layers.convolutional import MaxPooling2D
from keras.layers.core import Activation
from keras.layers.core import Dropout
from keras.layers.core import Dense
from keras.layers import Flatten
from keras.layers import Input
from keras.models import Model
from keras.layers import concatenate
from keras import backend as K
Lines 2-13 import our required Python packages. Rather than importing the Sequential class
where the output of one layer feeds directly into the next, we’ll instead need to use the Model class
(Line 11). Using Model rather than Sequential will allow us to create a network graph with splits
and forks like in the Inception module. Another import you have not yet seen is the concatenate
function on Line 12. As the name suggests, this function takes a set of inputs and concatenates
them along a given axis, which in this case will be the channel dimension.
We’ll be implementing the exact version of MiniGoogLeNet as detailed in Figure 11.2 above,
so let’s start off with the conv_module:
15
16
class MiniGoogLeNet:
@staticmethodChapter 11. GoogLeNet
136
17
18
19
20
21
def conv_module(x, K, kX, kY, stride, chanDim, padding="same"):
# define a CONV => BN => RELU pattern
x = Conv2D(K, (kX, kY), strides=stride, padding=padding)(x)
x = BatchNormalization(axis=chanDim)(x)
x = Activation("relu")(x)
22
23
24
# return the block
return x
The conv_module function is responsible for applying a convolution, followed by a batch
normalization, and then finally an activation. The parameters to the method are detailed below:
• x: The input layer to the function.
• K: The number of filters our CONV layer is going to learn.
• kX and kY: The size of each of the K filters that will be learned.
• stride: The stride of the CONV layer.
• chanDim: The channel dimension, which is derived from either “channels last” or “channels
first” ordering.
• padding: The type of padding to be applied to the CONV layer.
On Line 19 we create the convolutional layer. The actual parameters to Conv2D are identical to
examples in previous architectures such as AlexNet and VGGNet, but what changes here is how we
supply the input to a given layer.
Since we are using a Model rather than a Sequential to define the network architecture, we
cannot call model.add as this would imply that the output from one layer follows sequentially into
the next layer. Instead, we supply the input layer in parenthesis at the end of the function call,
which is called a Functional API. Each layer instance in a Model is callable on a tensor and also
returns a tensor. Therefore, we can supply the inputs to a given layer by calling it as a function
once the object is instantiated.
A template for constructing layers in this manner can be seen below:
output = Layer(parameters)(input)
Take a second to familiarize yourself with this new style of adding layers to a network as we’ll
be using it whenever we define networks that are non-sequential.
The output of the Conv2D layer is then passed into the BatchNormalization layer on Line
20. The output of BatchNormalization then goes through a ReLU activation (Line 21). If we
were to construct a figure to help us visualize the conv_module it would look like Figure 11.3.
Figure 11.3: The conv_module of the MiniGoogLeNet architecture. This module includes no
branching and is a simple CONV => BN => ACT.
First the convolution is applied, then a batch normalization, followed by an activation. Note
that this module did not perform any branching. That is going to change with the definition of the
inception_module below:11.2 MiniGoogLeNet on CIFAR-10
26
27
28
29
30
31
32
33
34
137
@staticmethod
def inception_module(x, numK1x1, numK3x3, chanDim):
# define two CONV modules, then concatenate across the
# channel dimension
conv_1x1 = MiniGoogLeNet.conv_module(x, numK1x1, 1, 1,
(1, 1), chanDim)
conv_3x3 = MiniGoogLeNet.conv_module(x, numK3x3, 3, 3,
(1, 1), chanDim)
x = concatenate([conv_1x1, conv_3x3], axis=chanDim)
35
36
37
# return the block
return x
Our Mininception module will perform two sets of convolutions – a 1×1 CONV and a 3×3 CONV.
These two convolutions will be performed in parallel and the resulting features concatenated across
the channel dimension.
Lines 30 and 31 use the handy conv_module we just defined to learn numK1x1 filters (1 × 1).
Lines 32 and 33 then apply conv_module again to learn numK3x3 filters (3 × 3). By using the
conv_module function we are able to reuse code and not have to bloat our MiniGoogLeNet class
by inserting many blocks of CONV => BN => RELU blocks – this stacking is taken care of concisely
via conv_module.
Notice how both the input to the 1 × 1 and 3 × 3 Conv2D class is x, the input to the layer. When
using the Sequential class, this type of layer structure was not possible. But by using the Model
class, we can now have multiple layers accept the same input. Once we have both conv_1x1 and
conv_3x3, we concatenate them across the channel dimension.
Figure 11.4: The (mini)-inception_module consists of two branches. The first branch is a CONV
layer responsible for learning 1 × 1 filters. The second branch is another CONV layer that learns
3 × 3 filters. The filter responses are then concatenated along the channel dimension.
To visualize the “Mini"-Inception module, take a look at Figure 11.4. Our 1 × 1 and 3 × 3 CONV
layers take a given input and apply their respective convolutions. The output of both convolutions
is then concatenated (Line 33). We are allowed to concatenate the layer outputs because the output
volume size for both convolutions is identical due to padding="same".
Next comes the downsample_module, which as the name suggests, is responsible for reducing
the spatial dimensions of an input volume:
39
40
@staticmethod
def downsample_module(x, K, chanDim):Chapter 11. GoogLeNet
138
41
42
43
44
45
46
# define the CONV module and POOL, then concatenate
# across the channel dimensions
conv_3x3 = MiniGoogLeNet.conv_module(x, K, 3, 3, (2, 2),
chanDim, padding="valid")
pool = MaxPooling2D((3, 3), strides=(2, 2))(x)
x = concatenate([conv_3x3, pool], axis=chanDim)
47
48
49
# return the block
return x
This method requires us to pass in an input x, the number of filters K our convolutional layer
will learn, along with the chanDim for batch normalization and channel concatenation.
The first branch of the downsample_module learns a set of K, 3 × 3 filters using a stride of
2 × 2, thereby decreasing the output volume size (Lines 43 and 44). We apply max pooling on
Line 45 (the second branch), again with window size of 3 × 3 and stride of 2 × 2 to reduce volume
size. The conv_3x3 and pool outputs are then concatenated (Line 46) and returned to the calling
function.
Figure 11.5: The downsample_module is responsible for reducing the spatial dimensions of our
input volume. The first branch learns a set of filters with 2 × 2 stride to reduce the output volume.
The second branch also reduces the spatial dimensions, this time by applying max pooling. The
output of the downsample_module is concatenated along the channel dimension.
We can visualize the downsample_module in Figure 11.5. As the figure demonstrates, a con-
volution and max pooling operation are applied to the same input and then concatenated.
We are now ready to put all the pieces together:
51
52
53
54
55
56
@staticmethod
def build(width, height, depth, classes):
# initialize the input shape to be "channels last" and the
# channels dimension itself
inputShape = (height, width, depth)
chanDim = -1
57
58
59
60
61
62
# if we are using "channels first", update the input shape
# and channels dimension
if K.image_data_format() == "channels_first":
inputShape = (depth, height, width)
chanDim = 111.2 MiniGoogLeNet on CIFAR-10
139
Line 52 defines the build method to our network, as is standard for all other examples in this
book. Our build method accepts an input width, height, depth, and total number of classes
that will be learned. Lines 55 and 56 initialize our inputShape and chanDim assuming we are
using “channels last” ordering. If we are instead using “channels first” ordering, Lines 60-62 update
these variables, respectively.
Let’s define the model Input along with the first conv_module:
64
65
66
67
# define the model input and first CONV module
inputs = Input(shape=inputShape)
x = MiniGoogLeNet.conv_module(inputs, 96, 3, 3, (1, 1),
chanDim)
The call to Input on Line 65 initializes the architecture – all inputs to the network will start at
this layer which simply “holds” the input data (all networks need to have an input, after all). The
first CONV => BN => RELU is applied on Lines 66 and 67 where we learn 96, 3 × 3 filters.
From there, we stack two Inception modules followed by a downsample module:
69
70
71
72
#
x
x
x
two Inception modules followed by a downsample module
= MiniGoogLeNet.inception_module(x, 32, 32, chanDim)
= MiniGoogLeNet.inception_module(x, 32, 48, chanDim)
= MiniGoogLeNet.downsample_module(x, 80, chanDim)
The first Inception module (Line 70) learns 32 filters for both the 1 × 1 and 3 × 3 CONV layers.
When concatenated, this module outputs a volume with K = 32 + 32 = 64 filters.
The second Inception module (Line 71) learns 32, 1 × 1 filters and 48, 3 × 3 filters. Again,
when concatenated, we see that the output volume size is K = 32 + 48 = 80. The downsample
module reduces our input volume sizes but keeps the same number of filters learned at 80.
Next, let’s stack four Inception modules on top of each other before applying a downsample,
allowing GoogLeNet to learn deeper, richer features:
74
75
76
77
78
79
#
x
x
x
x
x
four Inception modules followed by a downsample module
= MiniGoogLeNet.inception_module(x, 112, 48, chanDim)
= MiniGoogLeNet.inception_module(x, 96, 64, chanDim)
= MiniGoogLeNet.inception_module(x, 80, 80, chanDim)
= MiniGoogLeNet.inception_module(x, 48, 96, chanDim)
= MiniGoogLeNet.downsample_module(x, 96, chanDim)
Notice how in some layers we learn more 1 × 1 filters than 3 × 3 filters, while other Inception
modules learn more 3 × 3 filters than 1 × 1. This type of alternating pattern is done on purpose and
was justified by Szegedy et al. after running many experiments. When we implement the deeper
variant of GoogLeNet later in this chapter, we’ll also see this pattern as well.
Continuing our implementation of Figure 11.2 by Zhang et al., we’ll now apply two more
inception modules followed by a global pool and dropout:
81
82
83
84
85
#
x
x
x
x
two Inception modules followed by global POOL and dropout
= MiniGoogLeNet.inception_module(x, 176, 160, chanDim)
= MiniGoogLeNet.inception_module(x, 176, 160, chanDim)
= AveragePooling2D((7, 7))(x)
= Dropout(0.5)(x)Chapter 11. GoogLeNet
140
The output volume size after Line 83 is 7 × 7 × 336. Applying an average pooling of 7 ×
7 reduces the volume size to 1 × 1 × 336 and thereby alleviates the need to apply many dense
fully-connected layers – instead, we simply average over the spatial outputs of the convolution.
Dropout is applied with a probability of 50 percent on Line 85 to help reduce overfitting.
Finally, we add in our softmax classifier based on the number of classes we wish to learn:
87
88
89
90
#
x
x
x
softmax classifier
= Flatten()(x)
= Dense(classes)(x)
= Activation("softmax")(x)
91
92
93
# create the model
model = Model(inputs, x, name="googlenet")
94
95
96
# return the constructed network architecture
return model
The actual Model is then instantiated on Line 93 where we pass in the inputs, the layers (x,
which includes the built-in branching), and optionally a name for the network. The constructed
architecture is returned to the calling function on Line 96.
11.2.2
Training and Evaluating MiniGoogLeNet on CIFAR-10
Now that MiniGoogLeNet is implemented, let’s train it on the CIFAR-10 dataset and see if we can
beat our previous best of 84 percent. Open up a new file, name it googlenet_cifar10.py, and
insert the following code:
1
2
3
# set the matplotlib backend so figures can be saved in the background
import matplotlib
matplotlib.use("Agg")
4
5
6
7
8
9
10
11
12
13
14
15
# import the necessary packages
from sklearn.preprocessing import LabelBinarizer
from pyimagesearch.nn.conv import MiniGoogLeNet
from pyimagesearch.callbacks import TrainingMonitor
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import LearningRateScheduler
from keras.optimizers import SGD
from keras.datasets import cifar10
import numpy as np
import argparse
import os
Lines 2 and 3 configure matplotlib so we can save figures and plots to disk in the background.
We then import the rest of our required packages on Lines 6-15. Line 7 imports our implementation
of MiniGoogLeNet.
Also notice how we are importing the LearningRateScheduler class on Line 10, which
implies that we’ll be defining a specific learning rate for our optimizer to follow when training the
network. Specifically, we’ll be defining a polynomial decay learning rate schedule. A polynomial
learning rate scheduler will follow the equation:
α = α 0 ∗ (1 − e/e max ) p
(11.1)11.2 MiniGoogLeNet on CIFAR-10
141
Where α 0 is the initial learning rate, e is the current epoch number, e max is the maximum number
of epochs we are going to perform, and p is the power of the polynomial. Applying this equation
yields the learning rate α for the current epoch.
Given the maximum number of epochs, the learning rate will decay to zero. This learning rate
scheduler can also be made linear by setting the power to 1.0 – which is often done – and, in fact,
what we are going to do in this example. I have included a number of example polynomial learning
rate schedules using a maximum of 70 epochs, an initial learning rate of 5e − 3, and varying powers
in Figure 11.6. Notice how as the power increases, the faster the learning rate drops. Using a power
of 1.0 turns the curve into a linear decay.
Figure 11.6: Plots of polynomial learning rate decays for varying values of the power, p. Notice
how as the power increases the sharper the decay. Setting p = 1.0 turns a polynomial decay into a
linear decay.
Let’s go ahead and implement this learning rate schedule function below:
17
18
19
20
# define the total number of epochs to train for along with the
# initial learning rate
NUM_EPOCHS = 70
INIT_LR = 5e-3
21
22
23
24
25
26
27
def poly_decay(epoch):
# initialize the maximum number of epochs, base learning rate,
# and power of the polynomial
maxEpochs = NUM_EPOCHS
baseLR = INIT_LR
power = 1.0
28
29
# compute the new learning rate based on polynomial decayChapter 11. GoogLeNet
142
30
alpha = baseLR * (1 - (epoch / float(maxEpochs))) ** power
31
32
33
# return the new learning rate
return alpha
Per our discussion of learning rate schedulers in Chapter 16 of the Starter Bundle, you know
that a learning rate scheduling function can only accept a single argument, the current epoch. We
then initialize the maxEpochs the network is allowed to train for (so we can decay the learning rate
to zero), the base learning rate, as well as the power of the polynomial.
Computing the new learning rate based on the polynomial decay is handled on Line 30 – the
output of this equation will match our graphs exactly based on the parameters supplied. The new
learning rate is returned to the calling function on Line 33 so that the optimizer can update its
internal learning rate. Again, for more information on learning rate schedulers, please see Chapter
16 of the Starter Bundle.
Now that we have defined our learning rate defined, we can parse our command line arguments:
35
36
37
38
39
40
41
# construct the argument parse and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-m", "--model", required=True,
help="path to output model")
ap.add_argument("-o", "--output", required=True,
help="path to output directory (logs, plots, etc.)")
args = vars(ap.parse_args())
Our script requires two arguments, --model, the path to the output file where MiniGoogLeNet
will be serialized after training, along with --output, where we will store any plots, logs, etc.
The next step is to load the CIFAR-10 data from disk, perform pixel-wise mean subtraction,
and then one-hot encode the labels:
43
44
45
46
47
48
# load the training and testing data, converting the images from
# integers to floats
print("[INFO] loading CIFAR-10 data...")
((trainX, trainY), (testX, testY)) = cifar10.load_data()
trainX = trainX.astype("float")
testX = testX.astype("float")
49
50
51
52
53
# apply mean subtraction to the data
mean = np.mean(trainX, axis=0)
trainX -= mean
testX -= mean
54
55
56
57
58
# convert the labels from integers to vectors
lb = LabelBinarizer()
trainY = lb.fit_transform(trainY)
testY = lb.transform(testY)
To help combat overfitting and enable our model to obtain higher classification accuracy, we’ll
apply data augmentation:
60
61
# construct the image generator for data augmentation
aug = ImageDataGenerator(width_shift_range=0.1,11.2 MiniGoogLeNet on CIFAR-10
62
63
143
height_shift_range=0.1, horizontal_flip=True,
fill_mode="nearest")
We’ll also construct a set of callbacks to monitor training progress as well as call our
LearningRateScheduler:
65
66
67
68
69
70
71
# construct the set of callbacks
figPath = os.path.sep.join([args["output"], "{}.png".format(
os.getpid())])
jsonPath = os.path.sep.join([args["output"], "{}.json".format(
os.getpid())])
callbacks = [TrainingMonitor(figPath, jsonPath=jsonPath),
LearningRateScheduler(poly_decay)]
Finally, we are ready to train our network:
73
74
75
76
77
78
# initialize the optimizer and model
print("[INFO] compiling model...")
opt = SGD(lr=INIT_LR, momentum=0.9)
model = MiniGoogLeNet.build(width=32, height=32, depth=3, classes=10)
model.compile(loss="categorical_crossentropy", optimizer=opt,
metrics=["accuracy"])
79
80
81
82
83
84
# train the network
print("[INFO] training network...")
model.fit_generator(aug.flow(trainX, trainY, batch_size=64),
validation_data=(testX, testY), steps_per_epoch=len(trainX) // 64,
epochs=NUM_EPOCHS, callbacks=callbacks, verbose=1)
85
86
87
88
# save the network to disk
print("[INFO] serializing network...")
model.save(args["model"])
Line 75 initializes the SGD optimizer with an INIT_LR initial learning rate. This learning
rate will be updated via the LearningRateScheduler once training starts. The MiniGoogLeNet
architecture itself will accept input images with a width of 32 pixels, height of 32 pixels, depth of 3
channels, and a total of 10 class labels. Lines 82-84 kick off the training process using mini-batch
sizes of 64, training for a total of NUM_EPOCHS. Once training is complete, Line 88 serializes our
model to disk.
11.2.3
MiniGoogLeNet: Experiment #1
At this point in the Practitioner Bundle, it becomes important for you to understand the actual
mindset, process, and set of experiments you’ll need to perform to obtain a high accuracy model on
a given dataset.
In previous chapters in both this bundle and the Starter Bundle, you were just getting your feet
wet – and it was enough to simply see a piece of code, understand what it does, execute it, and look
at the output. That was a great starting point to developing an understanding of deep learning.
However, now that we are working with more advanced architectures and challenging problems,
you need to understand the process behind performing experiments, examining the results, and then
updating the parameters. I provide a gentle introduction to this scientific method in this chapter144
Chapter 11. GoogLeNet
and the following chapter on ResNet. If you are interested in mastering this ability to perform
experiments, examine the results, and make intelligent postulations as to what the next best course
of action is, then please refer to the more advanced lessons in the ImageNet Bundle.
In my first experiment with GoogLeNet, I started with an initial learning rate of 1e − 3 with the
SGD optimizer. The learning rate was then decayed linearly over the course of 70 epochs. Why 70
epochs? Two reasons:
1. A priori knowledge. After reading hundreds of deep learning papers, blog posts, tutorials
and not to mention, performing your own experiments, you’ll start to notice a pattern for
some datasets. In my case, I knew from previous experiments in my career with the CIFAR-
10 dataset that anywhere between 50-100 epochs is typically all that is required to train
CIFAR-10. The deeper the network architecture is (with sufficient regularization), along with
a decreasing learning rate, will normally allow us to train our network for longer. Therefore,
I choose 70 epochs for my first experiment. After the experiment was finished running, I
could examine the learning plot and decide if more/less epochs should be used (as it turns
out, 70 epochs was spot on).
2. Inevitable overfitting. Secondly, we know from previous experiments in this book that
we will eventually overfit when working with CIFAR-10. It’s inevitable; even with strong
regularization and data augmentation, it’s still going to happen. Therefore, I decided on 70
epochs rather than risking 80-100 epochs where the effects of overfitting would become more
pronounced.
I then started training using the following command:
$ python googlenet_cifar10.py --output output \
--model output/minigooglenet_cifar10.hdf5
Using our initial learning rate of 1e − 3 with a linear decay over 70 epochs, we obtained
87.95% classification accuracy (Figure 11.7, top-left). Furthermore, looking at our plot, while
there is a gap between the training and validation loss, the gap stays (relatively) proportional past
epoch 40. The same can be said of the training and validation accuracy curves as well. Looking
at this graph, I became concerned that we did not train hard enough and that we might be able to
obtain higher classification accuracy with a 1e − 2 learning rate.
11.2.4
MiniGoogLeNet: Experiment #2
In our second MiniGoogLeNet experiment, I swapped out the initial SGD learning rate of 1e − 3
for a larger 1e − 2. This learning rate was linearly decayed over the course of 70 epochs. I once
again trained the network and gathered the results:
$ python googlenet_cifar10.py --output output \
--model output/minigooglenet_cifar10.hdf5
At the end of the 70th epoch, we are obtaining 91.79% accuracy on the validation set (Figure
11.7, top-right), certainly better than our previous experiment – but we shouldn’t get too excited
yet. Looking at the loss for the training curve, we can see that it falls entirely to zero past epoch 60.
Furthermore, the classification accuracy for the training curve is fully saturated at 100%.
While we have increased our validation accuracy, we have done so at the expense of overfitting
– the gap between validation loss and training loss is huge past epoch 20. Instead, we would do
better to re-train our network with a 5e − 3 learning rate, falling square in the middle of 1e − 2 and
1e − 3. We might obtain slightly less validation accuracy, but we’ll ideally be able to reduce the
effects of overfitting.11.2 MiniGoogLeNet on CIFAR-10
145
Figure 11.7: Top-left: Learning curves for Experiment #1. Top-right: Plots for Experiment #2.
Bottom: Learning curves for Experiment #3. Our final experiment is a good balance between the
first two, obtaining 90.81% accuracy, higher than all of our previous CIFAR-10 experiments.
R
11.2.5
For what it’s worth, if you find that your network has totally saturated loss (0.0) and accuracy
(100%) for the training set, be sure to pay close attention to your validation curves. If you see
larges of gaps between the validation and training plot, you have surely overfit. Go back to
your experiment and play with the parameters, introduce more regularization, and adjust the
learning rate. Saturations such as those displayed in this experiment are indicative of a model
that will not generalize well.
MiniGoogLeNet: Experiment #3
In this experiment, I adjusted my learning rate to 5e − 3. MiniGoogLeNet was trained for 70 epochs
using the SGD optimizer and a linear learning rate decay:
$ python googlenet_cifar10.py --output output \
--model output/minigooglenet_cifar10.hdf5
As the output shows, we obtained 90.81% classification accuracy on the validation set (Figure
11.7, bottom) – lower than the previous experiment, but higher than the first experiment. We
are definitely overfitting in this experiment (by approximately an order of magnitude), but we can
accept this as an inevitability when working with CIFAR-10.146
Chapter 11. GoogLeNet
What’s more important here is that we kept our training loss and accuracy saturation levels as
low as possible – the training loss did not fall completely to zero, and the training accuracy did
not reach 100%. We can also see a reasonable gap maintained between training and validation
accuracy, even in the later epochs.
At this point I would consider this experiment to be an initial success (with the caveat that more
experiments should be done to reduce overfitting) – we have successfully trained MiniGoogLeNet
on CIFAR-10, reaching our goal of > 90% classification, beating out all previous experiments on
CIFAR-10.
Future revisions of this experiment should consider being aggressive with regularization.
Specifically, we did not use any type of weight regularization here. Applying L2 weight decay
would help combat our overfitting (as our experiments in the next section will demonstrate).
Moving forward, use this experiment as a baseline. We know there is overfitting and we’d like
to reduce it while increasing classification accuracy. In our next chapter on ResNet, we’ll see how
we can accomplish both.
Now that we’ve explored MiniGoogLeNet applied to CIFAR-10, let’s move on to the more
difficult classification task of the cs231n Tiny ImageNet challenge where we’ll be implementing
a deeper variant of GoogLeNet, similar to the architecture used by Szgedy et al. in their original
paper.
11.3
The Tiny ImageNet Challenge
Figure 11.8: A sample of images from Stanford’s Tiny ImageNet classification challenge.
The Tiny ImageNet Visual Recognition Challenge (a sample of which can be seen in Figure
11.8) is part of the cs231n Stanford course on Convolutional Neural Networks for Visual Recognition
[39]. As part of their final project, students can compete in the classification by either training a
CNN from scratch or performing transfer learning via fine-tuning (transfer learning via feature
extraction is not allowed).
The Tiny ImageNet dataset is actually a subset of the full ImageNet dataset (hence why
feature extraction cannot be used, as it would give the network an unfair advantage), consisting
of 200 diverse classes, including everything from Egyptian cats to volleyballs to lemons. Given
that there are 200 classes, guessing at random we would expect to be correct 1/200 = 0.5%
of the time; therefore, our CNN needs to obtain at least 0.5% to demonstrate it has learned
discriminative underlying patterns in the respective classes.
Each class includes 500 training images, 50 validation images, and 50 testing images. Ground-
truth labels are only provided for the training and validation images. Since we do not have access
to the Tiny ImageNet evaluation server, we will use part of the training set to form our own testing
set so we can evaluate the performance of our classification algorithms.
As readers of the ImageNet Bundle will discover (where we discuss how to train deep Convolu-
tional Neural Networks on the full ImageNet dataset from scratch), the images in the ImageNet11.3 The Tiny ImageNet Challenge
147
Large Scale Visual Recognition Challenge (ILSVRC) have varying widths and heights. Therefore,
whenever we work with ILSVRC, we first need to resize all images in the dataset to a fixed width
and height before we can train our network. To help students focus strictly on the deep learning and
image classification component (and not get caught up in image processing details), all images in
the Tiny ImageNet dataset have been resized to 64 × 64 pixels and center cropped.
In some ways, having the images resized makes Tiny ImageNet a bit more challenging than
it’s bigger brother, ILSVRC. In ILSVRC we are free to apply any type of resizing, cropping, etc.
operations that we see fit. However, with Tiny ImageNet, much of the image has already been
discarded for us. As we’ll find out, obtaining a reasonable rank-1 and rank-5 accuracy on Tiny
ImageNet isn’t as easy as one might think, making it a great, insightful dataset for budding deep
learning practitioners to learn and practice on.
In the next few sections, you will learn how to obtain the Tiny ImageNet dataset, understand its
structure, and create HDF5 files for the training, validation, and testing images.
11.3.1 Downloading Tiny ImageNet
You can download the Tiny ImageNet dataset from official cs231n leaderboard page here:
https://tiny-imagenet.herokuapp.com/
Alternatively, I have created a mirror for the file here as well:
http://pyimg.co/h28e4
The .zip file is ≈ 237MB, so make sure you have a reasonable internet connection before
attempting the download.
11.3.2 The Tiny ImageNet Directory Structure
After downloading and unpacking your tiny-imagenet-200.zip file, unarchive it, and you’ll
find the following directory structure:
--- tiny-imagenet-200
|
|--- test
|
|--- train
|
|--- val
|
|--- wnids.txt
|
|--- words.txt
Inside the test directory are the testing images – we’ll be ignoring these images since we do
not have access to the cs231n evaluation server (the labels are purposely left out from the download
to ensure no one can “cheat” in the challenge).
We then have the train directory which contains subdirectories with strange names starting
with the letter n followed by a series of numbers. These subdirectories are the WordNet [43] IDs
called “synonym set” or “synsets” for short. Each WordNet ID maps to a specific word/object.
Every image inside a given WordNet subdirectory contains examples of that object.
We can lookup the human readable label for a WordNet ID by parsing the words.txt file,
which is simply a tab separated file with the WordNet ID in the first column and the human readable
word/object in the second column. The wnids.txt file lists out the 200 WordNet IDs (one per line)
in the ImageNet dataset.
Finally, the val directory stores our validation set. Inside the val directory, you’ll find an
images subdirectory and a file named val_annotations.txt. The val_annotations.txt
provides the WordNet IDs for every image in the val directory.
Therefore, before we can even get started training GoogLeNet on Tiny ImageNet, we first need
to write a script to parse these files and put them into HDF5 format. Keep in mind that being a deep148
Chapter 11. GoogLeNet
learning practitioner isn’t about implementing Convolutional Neural Networks and training them
from scratch. Part of being a deep learning practitioner involves using your programming skills to
build simple scripts that can parse data.
The more general purpose programming skills you have, the better deep learning practitioner
you can become – while other deep learning researchers are struggling to organize files on disk or
understand how a dataset is structured, you’ll have already converted your entire dataset to a format
suitable for training a CNN.
In the next section, I’ll teach you how to define your project configuration file and create a single,
simple Python script that will convert the Tiny ImageNet dataset into an HDF5 representation.
11.3.3
Building the Tiny ImageNet Dataset
Let’s go ahead and define the project structure for Tiny ImageNet + GoogLeNet:
--- deepergooglenet
|
|--- config
|
|
|--- __init__.py
|
|
|--- tiny_imagenet_config.py
|
|--- build_tiny_imagenet.py
|
|--- rank_accuracy.py
|
|--- train.py
|
|--- output/
|
|
|--- checkpoints/
|
|
|--- tiny-image-net-200-mean.json
We’ll create a config module where we’ll store any tiny_imagenet_config.py configura-
tions. We then have the build_tiny_imagenet.py script which is responsible for taking Tiny
ImageNet and converting it to HDF5. The train.py script will train GoogLeNet on the HDF5
version of Tiny ImageNet. Finally, we’ll use rank.py to compute the rank-1 and rank-5 accuracies
for the testing set.
Let’s go ahead and take a look at tiny_imagenet_config.py:
1
2
# import the necessary packages
from os import path
3
4
5
6
# define the paths to the training and validation directories
TRAIN_IMAGES = "../datasets/tiny-imagenet-200/train"
VAL_IMAGES = "../datasets/tiny-imagenet-200/val/images"
7
8
9
10
# define the path to the file that maps validation filenames to
# their corresponding class labels
VAL_MAPPINGS = "../datasets/tiny-imagenet-200/val/val_annotations.txt"
Lines 5 and 6 define the paths to the Tiny ImageNet training and validation images, respectively.
We then define the path to the validation file mappings which enables us to map the validation
filenames to actual class labels (i.e., the WordNet IDs).
Speaking of WordNet IDs and human readable labels, let’s define the paths to those as well:
12
13
14
15
# define the paths to the WordNet hierarchy files which are used
# to generate our class labels
WORDNET_IDS = "../datasets/tiny-imagenet-200/wnids.txt"
WORD_LABELS = "../datasets/tiny-imagenet-200/words.txt"11.3 The Tiny ImageNet Challenge
149
Given that we do not have access to the testing labels, we’ll need to take a portion of the training
data and use it for validation (since our training data does have labels associated with each image):
17
18
19
20
# since we do not have access to the testing data we need to
# take a number of images from the training data and use it instead
NUM_CLASSES = 200
NUM_TEST_IMAGES = 50 * NUM_CLASSES
The purpose of this section is to convert Tiny ImageNet to HDF5, therefore we need to supply
paths to the training, validation, and testing HDF5 files:
22
23
24
25
26
# define the path to the output training, validation, and testing
# HDF5 files
TRAIN_HDF5 = "../datasets/tiny-imagenet-200/hdf5/train.hdf5"
VAL_HDF5 = "../datasets/tiny-imagenet-200/hdf5/val.hdf5"
TEST_HDF5 = "../datasets/tiny-imagenet-200/hdf5/test.hdf5"
When writing the images to disk, we’ll want to compute the RGB means for the training
set, enabling us to perform mean normalization – after we have the means, they will need to be
serialized to disk as a JSON file:
28
29
# define the path to the dataset mean
DATASET_MEAN = "output/tiny-image-net-200-mean.json"
Finally, we’ll define the paths to our output model and training logs/plots:
31
32
33
34
35
36
37
38
39
# define the path to the output directory used for storing plots,
# classification reports, etc.
OUTPUT_PATH = "output"
MODEL_PATH = path.sep.join([OUTPUT_PATH,
"checkpoints/epoch_70.hdf5"])
FIG_PATH = path.sep.join([OUTPUT_PATH,
"deepergooglenet_tinyimagenet.png"])
JSON_PATH = path.sep.join([OUTPUT_PATH,
"deepergooglenet_tinyimagenet.json"])
As you can see, this configuration file is fairly straightforward. We are mainly just defining
paths to input directories of images/label mappings along with output files. However, taking
the time to create this configuration file makes our life much easier when actually building Tiny
ImageNet and converting it to HDF5.
To see how this is true, let’s go ahead and examine build_tiny_imagenet.py:
1
2
3
4
5
6
7
# import the necessary packages
from config import tiny_imagenet_config as config
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from pyimagesearch.io import HDF5DatasetWriter
from imutils import paths
import numpy as npChapter 11. GoogLeNet
150
8
9
10
11
import
import
import
import
progressbar
json
cv2
os
Lines 2-11 import our required Python packages. On Line 2 we import our newly coded
configuration file so we can have access to the variables inside it. We’ll be using the LabelEncoder
to encode the WordNet IDs as integers. The train_test_split function will be applied to
construct our training and testing split. We’ll the use the HDF5DatasetWriter class to actually
write the raw images to their respective HDF5 datasets.
Let’s go ahead and grab the paths to the training images, extract the class labels, and encode
them:
13
14
15
16
17
18
# grab the paths to the training images, then extract the training
# class labels and encode them
trainPaths = list(paths.list_images(config.TRAIN_IMAGES))
trainLabels = [p.split(os.path.sep)[-3] for p in trainPaths]
le = LabelEncoder()
trainLabels = le.fit_transform(trainLabels)
On Line 15 we grab a list of all image paths inside the TRAIN_IMAGES directory. Every path in
this list has the pattern:
tiny-imagenet-200/train/{wordnet_id}/{unique_filename}.JPG
Therefore, to extract the WordNet ID (i.e., class label), we simply need to split on the path
separator and grab the third entry (Python is zero-indexed, so we supply a value of 2 here). Once
we have all the trainLabels, we can initiate the LabelEncoder and convert all labels to unique
integers (Lines 17 and 18).
Since we do not have a testing split, we need to sample a set of images from the training set to
form one:
20
21
22
23
24
25
# perform stratified sampling from the training set to construct a
# a testing set
split = train_test_split(trainPaths, trainLabels,
test_size=config.NUM_TEST_IMAGES, stratify=trainLabels,
random_state=42)
(trainPaths, testPaths, trainLabels, testLabels) = split
Here we supply the trainPaths and trainLabels, along with the test_size of NUM_TEST_IMAGES,
which is 50 images per class (for a total of 10,000 images). Our testing set is sampled from our
training set, where we already have the class labels for the images, enabling us to evaluate the
performance of our neural network when we are ready; however, we have not parsed the validation
labels yet.
Parsing and encoding the validation labels is handled in the following code block:
27
28
29
# load the validation filename => class from file and then use these
# mappings to build the validation paths and label lists
M = open(config.VAL_MAPPINGS).read().strip().split("\n")11.3 The Tiny ImageNet Challenge
30
31
32
151
M = [r.split("\t")[:2] for r in M]
valPaths = [os.path.sep.join([config.VAL_IMAGES, m[0]]) for m in M]
valLabels = le.transform([m[1] for m in M])
On Line 29 we load the entire contents of the VAL_MAPPINGS file (i.e., the tab separated file
that maps validation image file names to their respective WordNet ID). For every line inside the M,
we split it into two columns – the image filename and the WordNet ID (Line 30). Based on the
path to the validation images (VAL_IMAGES) along with the filenames in M, we can then construct
the paths to the validation files (Line 31). Similarly, we can transform the WordNet ID string to a
unique class label integer on Line 32 by looping over the WordNet IDs in each row and applying
the label encoder.
For readers who struggle to understand this section of code, I would suggest stopping here to
spend a while executing each line and investigating the contents of every variable. We are making
heavy use of Python list comprehensions here, which are natural, succinct methods to build lists
with very little code. Again, this code block has nothing to do with deep learning – it’s simply
parsing a file which is a general purpose programming problem. Take a few minutes and ensure
you understand how we are able to parse the val_annotations.txt file using this code block.
Now that we have the paths to our training, validation, and testing images, we can define a
datasets tuple that we’ll loop over and write the images and associated class labels for each set to
HDF5, respectively:
34
35
36
37
38
39
40
# construct a list pairing the training, validation, and testing
# image paths along with their corresponding labels and output HDF5
# files
datasets = [
("train", trainPaths, trainLabels, config.TRAIN_HDF5),
("val", valPaths, valLabels, config.VAL_HDF5),
("test", testPaths, testLabels, config.TEST_HDF5)]
We’ll also initialize our RGB averages as well:
42
43
# initialize the lists of RGB channel averages
(R, G, B) = ([], [], [])
Finally, we are ready to build our HDF5 datasets for Tiny ImageNet:
45
46
47
48
49
# loop over the dataset tuples
for (dType, paths, labels, outputPath) in datasets:
# create HDF5 writer
print("[INFO] building {}...".format(outputPath))
writer = HDF5DatasetWriter((len(paths), 64, 64, 3), outputPath)
50
51
52
53
54
55
# initialize the progress bar
widgets = ["Building Dataset: ", progressbar.Percentage(), " ",
progressbar.Bar(), " ", progressbar.ETA()]
pbar = progressbar.ProgressBar(maxval=len(paths),
widgets=widgets).start()
On Line 46 we loop over the dataset type (dType), paths, labels, and outputPath in the
datasets list. For each of these output HDF5 files we’ll create an HDF5DatasetWriter which willChapter 11. GoogLeNet
152
store a total of len(paths) images, each of which is a 64 × 64 × 3 RGB image (Line 49). Lines
52-55 simply initialize a progressbar so we can easily visualize the dataset creation process.
We now need to loop over each path and label pair in the respective set:
57
58
59
60
# loop over the image paths
for (i, (path, label)) in enumerate(zip(paths, labels)):
# load the image from disk
image = cv2.imread(path)
61
62
63
64
65
66
67
68
69
# if we are building the training dataset, then compute the
# mean of each channel in the image, then update the
# respective lists
if dType == "train":
(b, g, r) = cv2.mean(image)[:3]
R.append(r)
G.append(g)
B.append(b)
70
71
72
73
# add the image and label to the HDF5 dataset
writer.add([image], [label])
pbar.update(i)
74
75
76
77
# close the HDF5 writer
pbar.finish()
writer.close()
For each image, we load it from disk on Line 60. If the image is a training image, we need to
compute the RGB average of the image and update the respective lists (Lines 66-69). Line 72 adds
the image (which is already 64 × 64 × 3) and label to the HDF5 dataset while Line 77 closes the
dataset.
The final step is to compute the RGB averages across the entire dataset and write them to disk:
79
80
81
82
83
84
85
# construct a dictionary of averages, then serialize the means to a
# JSON file
print("[INFO] serializing means...")
D = {"R": np.mean(R), "G": np.mean(G), "B": np.mean(B)}
f = open(config.DATASET_MEAN, "w")
f.write(json.dumps(D))
f.close()
To build the Tiny ImageNet dataset in HDF5 format, just execute the following command:
$ python build_tiny_imagenet.py
[INFO] building ../datasets/tiny-imagenet-200/hdf5/train.hdf5...
Building Dataset: 100% |####################################| Time: 0:00:36
[INFO] building ../datasets/tiny-imagenet-200/hdf5/val.hdf5...
Building Dataset: 100% |####################################| Time: 0:00:04
[INFO] building ../datasets/tiny-imagenet-200/hdf5/test.hdf5...
Building Dataset: 100% |####################################| Time: 0:00:05
[INFO] serializing means...11.4 DeeperGoogLeNet on Tiny ImageNet
153
After the script finishes executing, you’ll have three files in your hdf5 directory, train.hdf5,
val.hdf5, and test.hdf5. You can investigate each of these files with the h5py library if you
wish to validate that the datasets do indeed contain the images:
>>> import h5py
>>> filenames = ["train.hdf5", "val.hdf5", "test.hdf5"]
>>> for filename in filenames:
...
db = h5py.File(filename, "r")
...
print(db["images"].shape)
...
db.close()
...
(90000, 64, 64, 3)
(10000, 64, 64, 3)
(10000, 64, 64, 3)
We’ll be using these HDF5 dataset representations of ImageNet to train both GoogLeNet in
this chapter as well as ResNet in the following chapter.
11.4
DeeperGoogLeNet on Tiny ImageNet
Now that we have our HDF5 representation of the Tiny ImageNet dataset, we are ready to train
GoogLeNet on it – but instead of using MiniGoogLeNet as in the previous section, we are going to
use a deeper variant which more closely models the Szegedy et al. implementation. This deeper
variation will use the original Inception module as detailed in Figure 11.1 earlier in this chapter,
which will help you understand the original architecture and implement it on your own in the future.
To get started, we’ll first learn how to implement this deeper network architecture. We’ll then
train DeeperGoogLeNet on the Tiny ImageNet dataset and evaluate the results in terms of rank-1
and rank-5 accuracy.
11.4.1
Implementing DeeperGoogLeNet
I have provided a figure (replicated and modified from Szegedy et al.) detailing our Deeper-
GoogLeNet architecture in Figure 11.9. There are only two primary differences between our
implementation and the full GoogLeNet architecture used by Szegedy et al. when training the
network on the complete ImageNet dataset:
1. Instead of using 7 × 7 filters with a stride of 2 × 2 in the first CONV layer, we use 5 × 5 filters
with a 1 × 1 stride. We use these due to the fact that our implementation of GoogLeNet is only
able to accept 64 × 64 × 3 input images while the original implementation was constructed to
accept 224 × 224 × 3 images. If we applied 7 × 7 filters with a 2 × 2 stride, we would reduce
our input dimensions too quickly.
2. Our implementation is slightly shallower with two fewer Inception modules – in the original
Szegedy et al. paper, two more Inception modules were added prior to the average pooling
operation. This implementation of GoogLeNet will be more than enough for us to perform
well on Tiny ImageNet and claim a spot on the cs231n Tiny ImageNet leaderboard. For
readers who are interested in training the full GoogLeNet architecture from scratch on
the entire ImageNet dataset (thereby replicating the performance of the Szegedy et al.
experiments), please refer to Chapter 7 in the ImageNet Bundle.
To implement our DeeperGoogLeNet class, let’s create a file name deeergooglenet.py
inside the nn.conv sub-module of pyimagesearch:154
Chapter 11. GoogLeNet
Figure 11.9: Our modified GoogLeNet architecture which we will call “DeeperGoogLeNet”.
The DeeperGoogLNet architecture is identical to the original GoogLeNet architecture with two
modifications: (1) 5 × 5 filters with a stride of 1 × 1 are used in the first CONV layer and (2) the final
two inception modules (5a and 5b) are left out.
--- pyimagesearch
|
|--- __init__.py
|
|--- callbacks
|
|--- io
|
|--- nn
|
|
|--- __init__.py
|
|
|--- conv
|
|
|
|--- __init__.py
|
|
|
|--- alexnet.py
|
|
|
|--- deepergooglenet.py
|
|
|
|--- lenet.py
|
|
|
|--- minigooglenet.py
|
|
|
|--- minivggnet.py
|
|
|
|--- fcheadnet.py
|
|
|
|--- shallownet.py
|
|--- preprocessing
|
|--- utils
From there, we can start working on the implementation:
1
2
3
4
5
6
7
# import the necessary packages
from keras.layers.normalization import BatchNormalization
from keras.layers.convolutional import Conv2D
from keras.layers.convolutional import AveragePooling2D
from keras.layers.convolutional import MaxPooling2D
from keras.layers.core import Activation
from keras.layers.core import Dropout11.4 DeeperGoogLeNet on Tiny ImageNet
8
9
10
11
12
13
14
from
from
from
from
from
from
from
155
keras.layers.core import Dense
keras.layers import Flatten
keras.layers import Input
keras.models import Model
keras.layers import concatenate
keras.regularizers import l2
keras import backend as K
Lines 2-14 start by importing our required Python packages. Notice how we’ll be using
the Input and Model classes as in our MiniGoogLeNet implementation so we can construct a
graph rather than a sequential network – this graph construct is a requirement due to how the
Inception module branches. Also take note of Line 13 where we import the l2 class, implying that
we will allow L2 weight regularization in the network to help reduce overfitting.
As a matter of convenience (and to ensure our code doesn’t become bloated), let’s define a
conv_module function that will be responsible for accepting an input layer, performing a CONV
=> BN => RELU, and then returning the output. Typically I would prefer to place the BN after the
RELU, but since we are replicating the original work of Szegedy et al., let’s stick with the batch
normalization prior to the activation. The implementation of conv_module can be seen below:
16
17
18
19
20
21
class DeeperGoogLeNet:
@staticmethod
def conv_module(x, K, kX, kY, stride, chanDim,
padding="same", reg=0.0005, name=None):
# initialize the CONV, BN, and RELU layer names
(convName, bnName, actName) = (None, None, None)
22
23
24
25
26
27
# if a layer name was supplied, prepend it
if name is not None:
convName = name + "_conv"
bnName = name + "_bn"
actName = name + "_act"
28
29
30
31
32
33
# define a CONV => BN => RELU pattern
x = Conv2D(K, (kX, kY), strides=stride, padding=padding,
kernel_regularizer=l2(reg), name=convName)(x)
x = BatchNormalization(axis=chanDim, name=bnName)(x)
x = Activation("relu", name=actName)(x)
34
35
36
# return the block
return x
The conv_module method accepts a number of parameters, including:
• x: The input to the network.
• K: The number of filters the convolutional layer will learn.
• kX and kY: The filter size for the convolutional layer.
• stride: The stride (in pixels) for the convolution. Typically we’ll use a 1 × 1 stride, but we
could use a larger stride if we wished to reduce the output volume size.
• chanDim: This value controls the dimension (i.e., axis) of the image channel. It is automati-
cally set later in this class based on whether we are using “channels_last” or “channels_first”
ordering.
• padding: Here we can control the padding of the convolution layer.
• reg: The L2 weight decay strength.156
Chapter 11. GoogLeNet
• name: Since this network is deeper than all others we have worked with in this book, we may
wish to name the blocks of layers to help us (1) debug the network and (2) share/explain the
network to others.
Line 21 initializes the name of each of the convolution, batch normalization, and activation
layers, respectively. Provided that the name parameter to conv_module is not None, we then update
the names of our layers (Lines 24-27).
Defining the CONV => BN => RELU layer pattern is accomplished on Lines 30-33 – notice
how the name of each layer is also included. Again, the primary benefit of naming each layer is
that we can visualize the name in the output, just as we did in Chapter 19 in the Starter Bundle.
For example, using the plot_model on the conv_module would result in a chart similar to Figure
11.10.
Figure 11.10: A sample of the DeeperGoogLeNet architecture visualization which includes the
actual names for each layer. Naming layers makes it easier to keep track of them in larger, deeper
networks.
Notice how the names of each layer are included in the chart, which is especially helpful when
you are working with deep CNNs and can easily get “lost” examining the massive charts. The
output of the conv_module is then returned to the calling function on Line 36.
Next, let’s define the inception_module, as detailed by Szgedy et al. in their original
publication and displayed in Figure 11.1:
38
39
40
41
42
43
44
45
@staticmethod
def inception_module(x, num1x1, num3x3Reduce, num3x3,
num5x5Reduce, num5x5, num1x1Proj, chanDim, stage,
reg=0.0005):
# define the first branch of the Inception module which
# consists of 1x1 convolutions
first = DeeperGoogLeNet.conv_module(x, num1x1, 1, 1,
(1, 1), chanDim, reg=reg, name=stage + "_first")
The Inception module includes four branches, the outputs of which are concatenated along
the channel dimension. The first branch of in the Inception module simply performs a series of
1 × 1 convolutions – these enable the Inception module to learn local features.
The second branch of the Inception module first performs dimensionality reduction via 1 ×
1 convolution followed by expanding with a 3 × 3 convolution – we call these our num3x3Reduce11.4 DeeperGoogLeNet on Tiny ImageNet
157
and num3x3 variables, respectively:
47
48
49
50
51
52
# define the second branch of the Inception module which
# consists of 1x1 and 3x3 convolutions
second = DeeperGoogLeNet.conv_module(x, num3x3Reduce, 1, 1,
(1, 1), chanDim, reg=reg, name=stage + "_second1")
second = DeeperGoogLeNet.conv_module(second, num3x3, 3, 3,
(1, 1), chanDim, reg=reg, name=stage + "_second2")
Here we can see that the first conv_module applies the 1 × 1 convolutions to the input. The
output of these 1 × 1 convolutions are then passed into the second conv_module which performs a
series of 3 × 3 convolutions. The number of 1 × 1 convolutions is always smaller than the number
of 3 × 3 convolutions, thereby serving as a form of dimensionality reduction.
The third branch in Inception is identical to the second branch, only instead of performing a
1 × 1 reduce followed by a 3 × 3 expand, we are now going to use a 1 × 1 reduce and a 5 × 5 expand:
54
55
56
57
58
59
# define the third branch of the Inception module which
# are our 1x1 and 5x5 convolutions
third = DeeperGoogLeNet.conv_module(x, num5x5Reduce, 1, 1,
(1, 1), chanDim, reg=reg, name=stage + "_third1")
third = DeeperGoogLeNet.conv_module(third, num5x5, 5, 5,
(1, 1), chanDim, reg=reg, name=stage + "_third2")
On Lines 56 and 57 we learn num5x5Reduce kernels, each of size 1 × 1 based on the input
to the inception_module. The output of the 1 × 1 convolutions are then passed into a second
conv_module which then learns a num5x5 filters, each of size 5 × 5. Again, the number of
1 × 1 convolutions in this branch is always smaller than the number of 5 × 5 filters.
The fourth and final branch of the Inception module is commonly referred to as the pool
projection. Here we apply max pooling followed by a series of 1 × 1 convolutions:
61
62
63
64
65
66
# define the fourth branch of the Inception module which
# is the POOL projection
fourth = MaxPooling2D((3, 3), strides=(1, 1),
padding="same", name=stage + "_pool")(x)
fourth = DeeperGoogLeNet.conv_module(fourth, num1x1Proj,
1, 1, (1, 1), chanDim, reg=reg, name=stage + "_fourth")
The rationale for this branch is partially scientific and partially anecdotal. In 2014, most (if
not all) Convolutional Neural Networks that were obtaining state-of-the-art performance on the
ImageNet dataset were applying max pooling. Therefore, it was believed that a CNN should apply
max pooling. While GoogLeNet does apply max pooling outside of the Inception module, Szegedy
et al. decided to include the pool projection branch as another form of max pooling.
Now that we have all four branches computed, we can concatenate their output along the
channel dimension and return the output to the calling function:
68
69
70
# concatenate across the channel dimension
x = concatenate([first, second, third, fourth], axis=chanDim,
name=stage + "_mixed")
71
72
73
# return the block
return x158
Chapter 11. GoogLeNet
If we were to call the plot_model function on the Inception module using the following
parameters:
• num1x1=64
• num3x3Reduce=96
• num3x3=128
• num5x5Reduce=16
• num5x5=32
• num1x1Proj=32
The resulting graph would look like Figure 11.11. We can see in this visualization how the
Inception module constructs four branches. The first branch is responsible for learning local
1 × 1 features. The second branch performs dimensionality reduction via a 1 × 1 convolution,
followed by learning a larger filter size of 3 × 3. The third branch behaves similarly to the second
branch, only learning 5 × 5 filters rather than 3 × 3 filters. Finally, the fourth branch applies max
pooling.
Figure 11.11: The full Inception module proposed by Szegedy et al. (zoomed out to save space).
The key takeaway here is that there are four distinct branches in the Inception module.
By learning all three 1 × 1, 3 × 3, and 5 × 5 filters, the Inception module can learn both general
(5 × 5 and 3 × 3) along with local (1 × 1) features at the same time. The actual optimization process
will automatically determine how to value these branches and layers, essentially giving us a “general
purpose” module that will learn the best set of features (local, small convolutions) or higher-level
abstracted features (larger convolutions) at a given time. The output of the Inception module is thus
256, which is the concatenation of all the 64 + 128 + 32 + 32 = 256 filters from each branch.
Now that the inception_module has been defined, we can create the build method responsi-
ble for constructing the complete DeeperGoogLeNet architecture:
75
76
77
78
79
80
@staticmethod
def build(width, height, depth, classes, reg=0.0005):
# initialize the input shape to be "channels last" and the
# channels dimension itself
inputShape = (height, width, depth)
chanDim = -1
81
82
83
84
# if we are using "channels first", update the input shape
# and channels dimension
if K.image_data_format() == "channels_first":11.4 DeeperGoogLeNet on Tiny ImageNet
85
86
159
inputShape = (depth, height, width)
chanDim = 1
Our build method will accept the spatial input dimensions of our images, including the width,
height, and depth. We’ll also be able to supply the number of class labels the network is to learn,
along with an optional regularization term for L2 weight decay. Lines 77-86 then handle properly
setting the inputShape and chanDim based on the “channels last” or “channels first” configuration
in Keras.
Following Figure 11.9 above, our first block of layers will perform a sequence of CONV =>
POOL => (CONV * 2) => POOL:
88
89
90
91
92
93
94
95
96
97
98
99
100
# define the model input, followed by a sequence of CONV =>
# POOL => (CONV * 2) => POOL layers
inputs = Input(shape=inputShape)
x = DeeperGoogLeNet.conv_module(inputs, 64, 5, 5, (1, 1),
chanDim, reg=reg, name="block1")
x = MaxPooling2D((3, 3), strides=(2, 2), padding="same",
name="pool1")(x)
x = DeeperGoogLeNet.conv_module(x, 64, 1, 1, (1, 1),
chanDim, reg=reg, name="block2")
x = DeeperGoogLeNet.conv_module(x, 192, 3, 3, (1, 1),
chanDim, reg=reg, name="block3")
x = MaxPooling2D((3, 3), strides=(2, 2), padding="same",
name="pool2")(x)
The first CONV layer learns 64 5 × 5 filters with a stride of 1 × 1. We then apply max pooling
with a window size of 3 × 3 and stride of 2 × 2 to reduce the volume size of the input. Lines
95-98 are responsible for performing a reduce and expand. First, 64 1 × 1 filters are learned (Lines
95 and 96). Then, 192 3 × 3 filters are learned on Lines 97 and 98. This process is very similar
to the Inception module (which will be applied in later layers of the network), only without the
branching factor. Finally, another max pooling is performed on Lines 99 and 100.
Next, let’s apply two Inception modules (3a and 3b) followed by a max pooling:
102
103
104
105
106
107
108
# apply two Inception modules followed by a POOL
x = DeeperGoogLeNet.inception_module(x, 64, 96, 128, 16,
32, 32, chanDim, "3a", reg=reg)
x = DeeperGoogLeNet.inception_module(x, 128, 128, 192, 32,
96, 64, chanDim, "3b", reg=reg)
x = MaxPooling2D((3, 3), strides=(2, 2), padding="same",
name="pool3")(x)
Looking at this code, you might wonder how we decided on the number of filters for each CONV
layer. The answer is that all parameter values in this network were taken directly from the original
Szegedy et al. paper on GoogLeNet where the authors performed a number of experiments to tune
the parameters. In every case, you’ll notice a common pattern with all Inception modules:
1. The number of 1 × 1 filters we learn in the first branch of the Inception module will be less
than or equal to the 1 × 1 filters in the 3 × 3 (second) and 5 × 5 (third) branches.
2. The number of 1 × 1 filters will always be smaller than the 3 × 3 and 5 × 5 convolutions they
feed into.
3. The number of filters we learn in the 3 × 3 branch will be more than the 5 × 5 branch which
helps reduce the size of the network as well as improves the speed of training/evaluation.Chapter 11. GoogLeNet
160
4. The number of pool projection filters will always be smaller than the first branch of 1 × 1 local
features.
5. Regardless of branch type, the number of filters will increase (or at least remain the same) as
we go deeper in the network.
While there are certainly more parameters to keep track of when implementing this network,
we’re still following the same general rules of thumb as in previous CNNs – the deeper the network
gets, the smaller the volume size is; therefore, the more filters we learn to compensate.
The network continues to become deeper, learning richer features as we now stack five Inception
modules (4a-4e) on top of each other before applying a POOL:
110
111
112
113
114
115
116
117
118
119
120
121
122
# apply five Inception modules followed by POOL
x = DeeperGoogLeNet.inception_module(x, 192, 96, 208, 16,
48, 64, chanDim, "4a", reg=reg)
x = DeeperGoogLeNet.inception_module(x, 160, 112, 224, 24,
64, 64, chanDim, "4b", reg=reg)
x = DeeperGoogLeNet.inception_module(x, 128, 128, 256, 24,
64, 64, chanDim, "4c", reg=reg)
x = DeeperGoogLeNet.inception_module(x, 112, 144, 288, 32,
64, 64, chanDim, "4d", reg=reg)
x = DeeperGoogLeNet.inception_module(x, 256, 160, 320, 32,
128, 128, chanDim, "4e", reg=reg)
x = MaxPooling2D((3, 3), strides=(2, 2), padding="same",
name="pool4")(x)
After the final POOL on Lines 121 and 122, our volume size is 4 x 4 x classes. To avoid the
usage of computationally expensive fully-connected layers (not to mention, dramatically increased
network size), we apply average pooling with a 4 × 4 kernel to reduce the volume size to 1 x 1 x
classes:
124
125
126
# apply a POOL layer (average) followed by dropout
x = AveragePooling2D((4, 4), name="pool5")(x)
x = Dropout(0.4, name="do")(x)
127
128
129
130
131
132
# softmax classifier
x = Flatten(name="flatten")(x)
x = Dense(classes, kernel_regularizer=l2(reg),
name="labels")(x)
x = Activation("softmax", name="softmax")(x)
133
134
135
# create the model
model = Model(inputs, x, name="googlenet")
136
137
138
# return the constructed network architecture
return model
Dropout is then applied with a probability of 40%. Typically we would use a 50% dropout rate,
but again, we are simply following the original implementation.
Lines 130 and 131 create the Dense layer for the total number of classes we wish to learn.
A softmax classifier is then applied after the fully-connected layer on Line 132. Finally, the actual
Model is constructed based on the inputs and x, the actual computational network graph. This
model is returned to the calling function on Line 138.11.4 DeeperGoogLeNet on Tiny ImageNet
11.4.2
161
Training DeeperGoogLeNet on Tiny ImageNet
Now that our DeeperGoogLeNet architecture is implemented, we need to create a Python script
that will train the network on Tiny ImageNet. We’ll also need to create a second Python script that
will be responsible for evaluating our model on the testing set by computing rank-1 and rank-5
accuracies.
Once we have completed both of these tasks, I’ll share three experiments I ran when gathering
the results for this chapter. These experiments will form a “case study” and enable you to learn
how to run an experiment, investigate the results, and make an educated guess on how to tune your
hyperparameters to obtain a better performing network in your next experiment.
11.4.3
Creating the Training Script
Let’s go ahead and implement the training script – open up a new file, name it train.py, and insert
the following code:
1
2
3
# set the matplotlib backend so figures can be saved in the background
import matplotlib
matplotlib.use("Agg")
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
# import the necessary packages
from config import tiny_imagenet_config as config
from pyimagesearch.preprocessing import ImageToArrayPreprocessor
from pyimagesearch.preprocessing import SimplePreprocessor
from pyimagesearch.preprocessing import MeanPreprocessor
from pyimagesearch.callbacks import EpochCheckpoint
from pyimagesearch.callbacks import TrainingMonitor
from pyimagesearch.io import HDF5DatasetGenerator
from pyimagesearch.nn.conv import DeeperGoogLeNet
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import Adam
from keras.models import load_model
import keras.backend as K
import argparse
import json
Lines 2 and 3 instruct the matplotlib library to use a backend such that we can save our loss
and accuracy plots to disk. We then import the remainder of our required Python packages on
Lines 6-19. Take a look at Line 6 where we import the configuration file for the Tiny ImageNet
experiment. We also import our implementation of DeeperGoogLeNet on Line 13. All the imports
should feel relatively familiar to you now.
From there, we can parse our command line arguments:
21
22
23
24
25
26
27
28
29
# construct the argument parse and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-c", "--checkpoints", required=True,
help="path to output checkpoint directory")
ap.add_argument("-m", "--model", type=str,
help="path to *specific* model checkpoint to load")
ap.add_argument("-s", "--start-epoch", type=int, default=0,
help="epoch to restart training at")
args = vars(ap.parse_args())162
Chapter 11. GoogLeNet
We’ll be using the ctrl + c method to train our network, meaning that we’ll start the train-
ing process, monitor how the training is going, then stop script if overfitting/stagnation occurs,
adjust any hyperparameters, and restart training. To start, we’ll first need the --checkpoints
switch, which is the path to the output directory that will store individual checkpoints for the
DeeperGoogLeNet model. If we are restarting training, then we’ll need to supply the path to
a specific --model that we are restarting training from. Similarly, we’ll also need to supply
--start-epoch to obtain the integer value of the epoch we are restarting training from.
In order to obtain reasonable accuracy on the Tiny ImageNet dataset, we’ll need to apply data
augmentation to the training data:
31
32
33
34
# construct the training image generator for data augmentation
aug = ImageDataGenerator(rotation_range=18, zoom_range=0.15,
width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,
horizontal_flip=True, fill_mode="nearest")
35
36
37
# load the RGB means for the training set
means = json.loads(open(config.DATASET_MEAN).read())
We’ll also load our RGB means (Line 37) for mean subtraction and normalization.
Let’s move on to instantiating both our image pre-processors as well as the training and
validation HDF5 dataset generators:
39
40
41
42
# initialize the image preprocessors
sp = SimplePreprocessor(64, 64)
mp = MeanPreprocessor(means["R"], means["G"], means["B"])
iap = ImageToArrayPreprocessor()
43
44
45
46
47
48
# initialize the training and validation dataset generators
trainGen = HDF5DatasetGenerator(config.TRAIN_HDF5, 64, aug=aug,
preprocessors=[sp, mp, iap], classes=config.NUM_CLASSES)
valGen = HDF5DatasetGenerator(config.VAL_HDF5, 64,
preprocessors=[sp, mp, iap], classes=config.NUM_CLASSES)
Both the training and validation generator will apply:
1. A simple preprocessor to ensure the image is resized to 64 × 64 pixels (which it already
should be, but we’ll include it here as a matter of completeness).
2. Mean subtraction to normalize the data.
3. An image to Keras-compatible array converter.
We’ll be training our network in mini-batch sizes of 64. In the case that we are training
DeeperGoogLeNet from the first epoch, wet must instantiate the network and optimizer:
50
51
52
53
54
55
56
57
58
# if there is no specific model checkpoint supplied, then initialize
# the network and compile the model
if args["model"] is None:
print("[INFO] compiling model...")
model = DeeperGoogLeNet.build(width=64, height=64, depth=3,
classes=config.NUM_CLASSES, reg=0.0002)
opt = Adam(1e-3)
model.compile(loss="categorical_crossentropy", optimizer=opt,
metrics=["accuracy"])11.4 DeeperGoogLeNet on Tiny ImageNet
163
Notice here how we are applying a L2 regularization strength of 0.0002 as well as the Adam
optimizer – we’ll find out why in Section 11.4.5 below.
Otherwise, we must be restarting training from a specific epoch, so we’ll need to load the model
and adjust the learning rate:
60
61
62
63
# otherwise, load the checkpoint from disk
else:
print("[INFO] loading {}...".format(args["model"]))
model = load_model(args["model"])
64
65
66
67
68
69
70
# update the learning rate
print("[INFO] old learning rate: {}".format(
K.get_value(model.optimizer.lr)))
K.set_value(model.optimizer.lr, 1e-5)
print("[INFO] new learning rate: {}".format(
K.get_value(model.optimizer.lr)))
We’ll create two callbacks, one to serialize the model weights to disk every five epochs and
another to create our loss/accuracy plot over time:
72
73
74
75
76
77
# construct the set of callbacks
callbacks = [
EpochCheckpoint(args["checkpoints"], every=5,
startAt=args["start_epoch"]),
TrainingMonitor(config.FIG_PATH, jsonPath=config.JSON_PATH,
startAt=args["start_epoch"])]
Finally, we can train our network:
79
80
81
82
83
84
85
86
87
# train the network
model.fit_generator(
trainGen.generator(),
steps_per_epoch=trainGen.numImages // 64,
validation_data=valGen.generator(),
validation_steps=valGen.numImages // 64,
epochs=10,
max_queue_size=64 * 2,
callbacks=callbacks, verbose=1)
88
89
90
91
# close the databases
trainGen.close()
valGen.close()
The exact number of epochs we choose to train our network for will depend on how our
loss/accuracy plots look. We will make an informed decision regarding updating learning rates or
applying early stopping based on model performance.
11.4.4
Creating the Evaluation Script
Once we are satisfied with our model performance on the training and validation set, we can
move on to evaluating the network on the testing set. To do so, let’s create a new file named
rank_accuracy.py:164
1
2
3
4
5
6
7
8
9
Chapter 11. GoogLeNet
# import the necessary packages
from config import tiny_imagenet_config as config
from pyimagesearch.preprocessing import ImageToArrayPreprocessor
from pyimagesearch.preprocessing import SimplePreprocessor
from pyimagesearch.preprocessing import MeanPreprocessor
from pyimagesearch.utils.ranked import rank5_accuracy
from pyimagesearch.io import HDF5DatasetGenerator
from keras.models import load_model
import json
To start, we’ll import our required Python packages. The rank5_accuracy function is imported
on Line 6 so we can compute both the rank-1 and rank-5 accuracy on the dataset.
From there, we load our RGB means, initialize our image pre-processors (in the same manner
as we did for testing), and then initialize the testing dataset generator:
11
12
# load the RGB means for the training set
means = json.loads(open(config.DATASET_MEAN).read())
13
14
15
16
17
# initialize the image preprocessors
sp = SimplePreprocessor(64, 64)
mp = MeanPreprocessor(means["R"], means["G"], means["B"])
iap = ImageToArrayPreprocessor()
18
19
20
21
# initialize the testing dataset generator
testGen = HDF5DatasetGenerator(config.TEST_HDF5, 64,
preprocessors=[sp, mp, iap], classes=config.NUM_CLASSES)
The following code block handles loading the pre-trained model from disk via the MODEL_PATH
we supply in our configuration file:
23
24
25
# load the pre-trained network
print("[INFO] loading model...")
model = load_model(config.MODEL_PATH)
You should set MODEL_PATH to be the final epoch checkpoint after training is complete. Al-
ternatively, you can set this variable to earlier epochs to obtain an understanding on how testing
accuracy increases in later epochs.
Once the model is loaded we can make predictions on the testing data and display both the
rank-1 and rank-5 accuracies:
27
28
29
30
# make predictions on the testing data
print("[INFO] predicting on test data...")
predictions = model.predict_generator(testGen.generator(),
steps=testGen.numImages // 64, max_queue_size=64 * 2)
31
32
33
34
35
# compute the rank-1 and rank-5 accuracies
(rank1, rank5) = rank5_accuracy(predictions, testGen.db["labels"])
print("[INFO] rank-1: {:.2f}%".format(rank1 * 100))
print("[INFO] rank-5: {:.2f}%".format(rank5 * 100))
36
37
38
# close the database
testGen.close()11.4 DeeperGoogLeNet on Tiny ImageNet
Epoch
1 − 25
26 − 35
36 − 65
165
Learning Rate
1e − 2
1e − 3
1e − 4
Table 11.1: Learning rate schedule used when training DeeperGoogLeNet on Tiny ImageNet for
Experiment #1.
11.4.5
DeeperGoogLeNet Experiments
In the following sections I have included the results of four separate experiments I ran when training
DeeperGoogLeNet on Tiny ImageNet. After each experiment I evaluated the results and then made
an educated decision on how the hyperparameters and network architecture should be updated to
increase accuracy.
Case studies like these are especially helpful to you as a budding deep learning practitioner.
Not only do they demonstrate that deep learning is an iterative process requiring many experiments,
but they also show which parameters you should be paying attention to and how to update them.
Finally, it’s worth noting that some of these experiments required changes to the code. Both
the implementations of deepergooglenet.py and train.py are my final implementations that
obtained the best accuracy. I’ll note the changes I made in earlier experiments in case you want to
replicate my (less accurate) results.
DeeperGoogLeNet: Experiment #1
Given that this was my first time training a network on the Tiny ImageNet challenge, I wasn’t
sure what the optimal depth should be for a given architecture on this dataset. While I knew Tiny
ImageNet would be a challenging classification task, I didn’t think Inception modules 4a-4e were
required, so I removed them from our DeeperGoogLeNet implementation above, leading to a
substantially more shallow network architecture.
I decided to train DeeperGoogLenet using SGD with an initial learning rate of 1e − 2 and mo-
mentum term of 0.9 (no Nesterov acceleration was applied). I always use SGD in my first
experiment. Per my guidelines and rules of thumb in Chapter 7, you should first try SGD to obtain
a baseline, and then if need be, use more advanced optimization methods.
I started training using the following command:
$ python train.py --checkpoints output/checkpoints
The learning rate schedule detailed in Table 11.1 was then used. This table implies that after
epoch 25 I stopped training, lowered the learning rate to 1e − 3, then resumed training for another
10 epochs:
$ python train.py --checkpoints output/checkpoints \
--model output/checkpoints/epoch_25.hdf5 --start-epoch 25
After epoch 35 I again stopped training, lowered the learning rate to 1e − 4, and then resumed
training for thirty more epochs:
$ python train.py --checkpoints output/checkpoints \
--model output/checkpoints/epoch_35.hdf5 --start-epoch 35166
Chapter 11. GoogLeNet
Training for an extra thirty epochs was excessive, to say the least; however, I wanted to get a
feel for the level of overfitting to expect for a large number of epochs after the original learning rate
had been dropped (as this was the first time I had worked with GoogLeNet + Tiny ImageNet). You
can see a plot of the loss/accuracy over time for both the training and validation in Figure 11.12
(top-left).
Figure 11.12: Top-left: Plots for Experiment #1. Top-right: Learning curves for Experiment
#2. Bottom: Training/validation plots for Experiment #3. The final experiment obtains the best
validation accuracy at 55.77%.
Starting at approximately epoch 15, there is a divergence in training and validation loss. By the
time we get to epoch 25, the divergence is getting more significant, so I lowered the learning rate
by an order of magnitude; the result is a nice jump in accuracy and decrease in loss. The problem is
that after this point, both training and validation learning essentially stagnate. Even lowering the
learning rate to 1e − 4 at epoch 35 does not introduce an extra boost in accuracy.
At the end of epoch 40 learning has stagnated completely. Had I not wanted to see the effects
of a low learning rate for a long period of time, I would have stopped training after epoch 45. In
this case, I let the network train until epoch 65 (with no change in loss/accuracy) where I stopped
training and examined the results, noting that the network was obtaining 52.25% rank-1 accuracy on
the validation set. However, given how our network performance plateaued quickly after dropping
the learning rate, I determined that there is clearly more work to be done.11.4 DeeperGoogLeNet on Tiny ImageNet
Epoch
1 − 20
21 − 30
31 − 40
Learning Rate
1e − 3
1e − 4
1e − 5
Epoch
1 − 40
41 − 60
61 − 70
167
Learning Rate
1e − 3
1e − 4
1e − 5
Table 11.2: Left: The learning rate schedule used when training DeeperGoogLeNet on Tiny
ImageNet in Experiment #2. Right: The learning rate schedule for Experiment #3.
DeeperGoogLeNet: Experiment #2
In my second experiment with DeeperGoogLeNet + Tiny ImageNet, I decided to switch out the
SGD optimizer for Adam. This decision was made strictly because I wasn’t convinced that the
network architecture needed to be deeper (yet). The Adam optimizer was used with the default
initial learning rate of 1e − 3. I then used the learning rate schedule in Table 11.2 (left) to lower the
learning rate.
A plot of the learning curves can be seen below in Figure 11.12 (top-right). This plot looks
very similar to the top-left above. We initially start off strong, but validation loss diverges quickly
past epoch 10, forcing me to lower the learning rate at epoch 20 (or otherwise risk overfitting). As
soon as the learning rate is reduced, learning plateaus and I am unable to increase accuracy, even
reducing learning rate a second time. However, at the end of the 40th epoch, I noticed that my
validation loss was lower than the previous experiment and my accuracy was higher.
By swapping out SGD for Adam, I was able to boost validation accuracy to 54.20% rank-1, an
increase of nearly 2%. However, I still had the issue of learning stagnation as soon as the initial
learning rate was lowered.
DeeperGoogLeNet: Experiment #3
Given the learning stagnation, I postulated that the network was not deep enough to model the
underlying patterns in the Tiny ImageNet dataset. Therefore, I decided to enable the Inception
modules 4a-4e, creating a much deeper network architecture capable of learning deeper, more
discriminative features. The Adam optimizer with an initial learning rate of 1e − 3 was used to
train the network. I left the L2 weight decay term at 0.0002. DeeperGoogLeNet was then trained
according to Table 11.2 (right).
You can see the plot in Figure 11.12 (bottom). Immediately you’ll notice that using the deeper
network architecture enabled me to train for longer without risking stagnation or severe overfitting.
At the end of the 70th epoch, I was obtaining 55.77% rank-1 accuracy on the validation set.
At this point, I decided it was time to evaluate DeeperGoogLeNet on the testing set:
$ python rank_accuracy.py
[INFO] loading model...
[INFO] predicting on test data...
[INFO] rank-1: 54.38%
[INFO] rank-5: 78.96%
The evaluation script reported a rank-1 accuracy of 54.38%, or an error rate of 1 − 0.5438 =
0.4562. It’s also interesting to note that our rank-5 accuracy is 78.96%, which is quite impressive
for this challenge. Looking at the Tiny ImageNet leaderboard (http://pyimg.co/h5q0o) below, we
can see this error rate is enough to claim the #7 position, a great start to journey to climb to the top
of the leaderboard (Figure 11.13).Chapter 11. GoogLeNet
168
Figure 11.13: Our first successful attempt at training DeeperGoogLeNet on the Tiny ImageNet
dataset allows us to claim the #7 position on the leaderboard. We’ll be able to reach higher positions
in our next chapter on ResNet.
R
Positions 1-4 on the Tiny ImageNet leaderboard were achieved by transfer learning via
fine-tuning on networks already trained on the full ImageNet dataset. Since we are training
our networks from scratch, we are more concerned with claiming the #5 position, the highest
position achieved without transfer learning. As we’ll find out in the next chapter on ResNet,
we’ll easily be able to claim this position. For more information on the techniques the cs231n
students used to achieve their error rates, please see the Stanford cs231n project page [4].
For readers interested in trying to boost the accuracy of DeeperGoogLeNet further, I would
suggest the following experiments:
1. Change the conv_module to use CONV => RELU => BN instead of the original CONV => BN
=> RELU ordering.
2. Attempt using ELUs instead of ReLUs, which will likely lead to a small 0.5 − 1% gain in
accuracy.
11.5
Summary
In this chapter, we reviewed the work of Szgedy et al. [17] which introduced the now famous
Inception module. The Inception module is an example of a micro-architecture, a building block
that fits into the overall macro-architecture of the network. Current state-of-the-art Convolutional
Neural Networks tend to use some form of micro-architecture.
We then applied the Inception module to create two variants of GoogLeNet:
1. One for CIFAR-10.
2. And another for the more challenging Tiny ImageNet.
When training on CIFAR-10, we obtained our best accuracy thus far of 90.81% (and improve-
ment from the previous best of 84%).
On the challenging Tiny ImageNet dataset we reached 54.38% rank-1 and 78.96% rank-5
accuracy on the testing set, enabling us to claim position #7 on the Tiny ImageNet leaderboard.
Our goal is to climb the leaderboard to position #5 (the highest position obtained when training
a network from scratch, all higher positions applied fine-tuning on networks pre-trained on the11.5 Summary
169
ImageNet dataset, giving them an unfair advantage). To reach our goal of position #5, we’ll need to
use the ResNet architecture detailed in the following chapter.12. ResNet
In our previous chapter, we discussed the GoogLeNet architecture and the Inception module, a
micro-architecture that acts as a building block in the overall macro-architecture. We are now going
to discuss another network architecture that relies on micro-architectures – ResNet.
ResNet uses what’s called a residual module to train Convolutional Neural Networks to depths
previously thought impossible. For example, in 2014, the VGG16 and VGG19 architectures were
considered very deep [11]. However, with ResNet, we have successfully trained networks with
> 100 layers on the challenging ImageNet dataset and over 1,000 layers on CIFAR-10 [24].
These depths are only made possible by using “smarter” weight initialization algorithms (such
as Xavier/Glorot [44] and MSRA/He et al. [45]) along with identity mapping, a concept we’ll
discuss later in this chapter. Given the depths of ResNet networks, perhaps it comes as no surprise
that in ResNet took first place in all three ILSVRC 2015 challenges (classification, detection, and
localization).
In this chapter, we are going to discuss the ResNet architecture, the residual module, along with
updates to the residual module that have made it capable of obtaining higher classification accuracy.
From there we’ll implement and train variants of ResNet on the CIFAR-10 dataset and the Tiny
ImageNet challenge – in each case, our ResNet implementations will outperform every experiment
we have executed in this book.
12.1
ResNet and the Residual Module
First introduced by He et al. in their 2015 paper, Deep Residual Learning for Image Recognition [24],
the ResNet architecture has become a seminal work, demonstrating that extremely deep networks
can be trained using standard SGD and a reasonable initialization function. In order to train
networks at depths greater than 50-100 (and in some cases, 1,000) layers, ResNet relies on a
micro-architecture called the residual module.
Another interesting component of ResNet is that pooling layers are used extremely sparingly.
Building on the work of Springenberg et al. [41], ResNet does not strictly rely on max pooling
operations to reduce volume size. Instead, convolutions with strides > 1 are used to not only learn172
Chapter 12. ResNet
weights, but reduce the output volume spatial dimensions. In fact, there are only two occurrences
of pooling being applied in the full implementation of the architecture:
1. The first (and only) currency of max pooling happens early in the network to help reduce
spatial dimensions.
2. The second pooling operation is actually an average pooling layer used in place of fully-
connected layers, like in GoogLeNet.
Strictly speaking, there is only one max pooling layer – all other reductions in spatial dimensions
are handled by convolutional layers.
In this section, we’ll review the original residual module, along with the bottleneck residual
module used to train deeper networks. From there, we’ll discuss extensions and updates to
the original residual module by He et al. in their 2016 publication, Identity Mappings in Deep
Residual Networks [33], that allow us to further increase classification accuracy. Later in this
chapter, we’ll implement ResNet from scratch using Keras.
12.1.1
Going Deeper: Residual Modules and Bottlenecks
The original residual module introduced by He et al. in 2015 relies on identity mappings, the process
of taking the original input to the module and adding it to the output of a series of operations. A
graphical depiction of this module can be seen in Figure 12.1 (left). Notice how this module only
has two branches, unlike the four branches in the Inception module of GoogLeNet. Furthermore,
this module is highly simplistic.
Figure 12.1: Left: The original Residual module proposed by He et al. Right: The more commonly
used bottleneck variant of the Residual module.
At the top of the module, we accept an input to the module (i.e., the previous layer in the
network). The right branch is a “linear shortcut" – it connects the input to an addition operation at
the bottom of the module. Then, on the left branch of the residual module, we apply a series of
convolutions (all of which are 3 × 3), activations, and batch normalizations. This is a fairly standard
pattern to follow when constructing Convolutional Neural Networks.
But what makes ResNet interesting is that He et al. suggested adding the original input to the
output of the CONV, RELU and BN layers. We call this addition an identity mapping since the input
(the identity) is added to the output of series of operations. It is also why the term “residual” is
used. The “residual” input is added to the output of a series of layer operations. The connection
between the input and the addition node is called the shortcut. Note that we are not referring to
concatenation along the channel dimension as we have done in previous chapters. Instead, we are
performing simple 1 + 1 = 2 addition at the bottom of the module between the two branches.
While traditional neural network layers can be seen as learning a function y = f (x), a residual
layer attempts to approximate y via f (x) + id(x) = f (x) + x where id(x) is the identity function.12.1 ResNet and the Residual Module
173
These residual layers start at the identity function and evolve to become more complex as the
network learns. This type of residual learning framework allows us to train networks that are sub-
stantially deeper than previously proposed network architectures.
Furthermore, since the input is included in every residual module, it turns out the network can
learn faster and with larger learning rates. It is very common to see the base learning rates for
ResNet implementations start at 1e − 1. For most architectures such as AlexNet or VGGNet, this
high of a learning rate would almost guarantee the network would not converge. But since ResNet
relies on residual modules via identity mappings, this higher learning rate is completely possible.
In the same 2015 work, He et al. also included an extension to the original residual module
called bottlenecks (Figure 12.1, right). Here we can see that the same identity mapping is taking
place, only now the CONV layers in the left branch of the residual module have been updated:
1. We are utilizing three CONV layers rather than just two.
2. The first and last CONV layers are 1 × 1 convolutions.
3. The number of filters learned in the first two CONV layers are 1/4 the number of filters learned
in the final CONV.
To understand why we call this a “bottleneck”, consider the following figure where two residual
modules are stacked on top of each other, with one residual feeding into the next (Figure 12.2).
Figure 12.2: An example of two stacked residual modules where one feeds into next. Both modules
learn K = 32, 32, and 128 filters, respectively. Notice how the dimensionality is reduced during the
first two CONV layers then increased during the final CONV layer.
The first residual module accepts an input volume of size M × N × 64 (the actual width and
height are arbitrary for this example). The three CONV layers in the first residual module learn
K = 32, 32, and 128 filters, respectively. After applying the first residual module our output volume
size is M × N × 128 which is then fed into the second residual module.
In the second residual module, our number of filters learned by each of the three CONV layers
stays the same at K = 32, 32, and 128, respectively. However, notice that 32 < 128, implying that
we are actually reducing the volume size during the 1 × 1 and 3 × 3 CONV layers. This result has the
benefit of leaving the 3 × 3 bottleneck layer with smaller input and output dimensions.
The final 1 × 1 CONV then applies 4x the number of filters than the first two CONV layers, thereby174
Chapter 12. ResNet
increasing dimensionality once again, which is why we call this update to the residual module
the “bottleneck” technique. When building our own residual modules, it’s common to supply
pseudocode such as residual_module(K=128) which implies that the final CONV layer will learn
128 filters, while the first two will learn 128/4 = 32 filters. This notation is often easier to work
with as it’s understood that the bottleneck CONV layers will learn 1/4th the number of filters as the
final CONV layer.
When it comes to training ResNet, we typically use the bottleneck variant of the residual
module rather than the original version, especially for ResNet implementations with > 50 layers.
12.1.2
Rethinking the Residual Module
In 2016, He et al. published a second paper on the residual module entitled Identity Mappings
in Deep Residual Networks [33]. This publication described a comprehensive study, both theo-
retically and empirically, on the ordering of convolutional, activation, and batch normalization
layers within the residual module itself. Originally, the residual module (with bottleneck) looked
like Figure 12.3 (left).
Figure 12.3: Left: The original residual module with bottleneck. Right: Adapting the bottleneck
module to use pre-activations.
The original residual module with bottleneck accepts an input (a ReLU activation map) and
then applies a series of (CONV => BN => RELU) * 2 => CONV => BN before adding this output
to the original input and applying a final ReLU activation (which is then fed into the next residual
module in the network). However, the He et al. 2016 study, it was found there was a more optimal
layer ordering capable of obtaining higher accuracy – this method is called pre-activation.12.2 Implementing ResNet
175
In the pre-activation version of the residual module, we remove the ReLU at the bottom of
the module and re-order the batch normalization and activation such that they come before the
convolution (Figure 12.3, right).
Now, instead of starting with a convolution, we apply a series of (BN => RELU => CONV) *
3 (assuming the bottleneck is being used, of course). The output of the residual module is now the
addition operation which is subsequently fed into the next residual module in the network (since
residual modules are stacked on top of each other).
We call this layer ordering pre-activation as our ReLUs and batch normalization are placed
before the convolutions, which is in contrast to the typical approach of applying ReLUs and batch
normalizations after the convolutions. In our next section, we’ll implement ResNet from scratch
using both bottlenecks and pre-activations.
12.2
Implementing ResNet
Now that we have reviewed the ResNet architecture, let’s go ahead and implement in Keras. For
this specific implementation, we’ll be using the most recent incarnation of the residual module,
including bottlenecks and pre-activations. To update your project structure, create a new file
named resnet.py inside the nn.conv sub-module of pyimagesearch – that is where our ResNet
implementation will live:
--- pyimagesearch
|
|--- __init__.py
|
|--- callbacks
|
|--- io
|
|--- nn
|
|
|--- __init__.py
|
|
|--- conv
|
|
|
|--- __init__.py
|
|
|
|--- alexnet.py
|
|
|
|--- deepergooglenet.py
|
|
|
|--- lenet.py
|
|
|
|--- minigooglenet.py
|
|
|
|--- minivggnet.py
|
|
|
|--- fcheadnet.py
|
|
|
|--- resnet.py
|
|
|
|--- shallownet.py
|
|--- preprocessing
|
|--- utils
From there, open up resnet.py and insert the following code:
1
2
3
4
5
6
7
8
9
10
11
# import the necessary packages
from keras.layers.normalization import BatchNormalization
from keras.layers.convolutional import Conv2D
from keras.layers.convolutional import AveragePooling2D
from keras.layers.convolutional import MaxPooling2D
from keras.layers.convolutional import ZeroPadding2D
from keras.layers.core import Activation
from keras.layers.core import Dense
from keras.layers import Flatten
from keras.layers import Input
from keras.models import ModelChapter 12. ResNet
176
12
13
14
from keras.layers import add
from keras.regularizers import l2
from keras import backend as K
We start off by importing our fairly standard set of classes and functions when building
Convolutional Neural Networks. However, I would like to draw your attention to Line 12 where we
import the add function. Inside the residual module, we’ll need to add together the outputs of two
branches, which will be accomplished via this add method. We’ll also import the l2 function on
Line 13 so that we can perform L2 weight decay. Regularization is extremely important when
training ResNet since, due to the network’s depth, it is prone to overfitting.
Next, let’s move on to our residual_module:
16
17
18
19
class ResNet:
@staticmethod
def residual_module(data, K, stride, chanDim, red=False,
reg=0.0001, bnEps=2e-5, bnMom=0.9):
This specific implementation of ResNet was inspired by both He et al. in their Caffe distribu-
tion [46] as well as the mxnet implementation from Wei Wu [47], therefore we will follow their
parameter choices as closely as possible. Looking at the residual_module we can see that the
function accepts more parameters than any of our previous functions – let’s review each of them in
detail.
The data parameter is simply the input to the residual module. The value K defines the number
of filters that will be learned by the final CONV in the bottleneck. The first two CONV layers will
learn K / 4 filters, as per the He et al. paper. The stride controls the stride of the convolution.
We’ll use this parameter to help us reduce the spatial dimensions of our volume without resorting to
max pooling.
We then have the chanDim parameter which defines the axis which will perform batch normal-
ization – this value is specified later in the build function based on whether we are using “channels
last” or “channels first” ordering.
Not all residual modules will be responsible for reducing the dimensions of our spatial volume –
the red (i.e., “reduce”) boolean will control whether we are reducing spatial dimensions (True) or
not (False).
We can then supply a regularization strength to all CONV layers in the residual module via
reg. The bnEps parameter controls the ε responsible for avoiding “division by zero” errors when
normalizing inputs. In Keras, ε defaults to 0.001; however, for our particular implementation, we’ll
allow this value to be reduced significantly. The bnMom controls the momentum for the moving
average. This value normally defaults to 0.99 inside Keras, but He et al. as well as Wei Wu
recommend decreasing the value to 0.9.
Now that the parameters of residual_module are defined, let’s move on to the body of the
function:
20
21
22
# the shortcut branch of the ResNet module should be
# initialize as the input (identity) data
shortcut = data
23
24
25
26
27
# the first block of the ResNet module are the 1x1 CONVs
bn1 = BatchNormalization(axis=chanDim, epsilon=bnEps,
momentum=bnMom)(data)
act1 = Activation("relu")(bn1)12.2 Implementing ResNet
28
29
177
conv1 = Conv2D(int(K * 0.25), (1, 1), use_bias=False,
kernel_regularizer=l2(reg))(act1)
On Line 22 we initialize the shortcut in the residual module, which is simply a reference to the
input data. We will later add the shortcut to the output of our bottleneck + pre-activation branch.
The first pre-activation of the bottleneck branch can be seen in Lines 25-29. Here we apply
a batch normalization layer, followed by ReLU activation, and then a 1 × 1 convolution, using
K/4 total filters. You’ll also notice that we are excluding the bias term from our CONV layers via
use_bias=False. Why might we wish to purposely leave out the bias term? According to He et
al., the biases are in the BN layers that immediately follow the convolutions [48], so there is no need
to introduce a second bias term.
Next, we have our second CONV layer in the bottleneck, this one responsible for learning a total
of K/4, 3 × 3 filters:
31
32
33
34
35
36
37
# the second block of the ResNet module are the 3x3 CONVs
bn2 = BatchNormalization(axis=chanDim, epsilon=bnEps,
momentum=bnMom)(conv1)
act2 = Activation("relu")(bn2)
conv2 = Conv2D(int(K * 0.25), (3, 3), strides=stride,
padding="same", use_bias=False,
kernel_regularizer=l2(reg))(act2)
The final block in the bottleneck learns K filters, each of which are 1 × 1:
39
40
41
42
43
44
45
# the third block of the ResNet module is another set of 1x1
# CONVs
bn3 = BatchNormalization(axis=chanDim, epsilon=bnEps,
momentum=bnMom)(conv2)
act3 = Activation("relu")(bn3)
conv3 = Conv2D(K, (1, 1), use_bias=False,
kernel_regularizer=l2(reg))(act3)
For more details on why we call this a “bottleneck” with “pre-activation”, please see Section
12.1 above.
The next step is to see if we need to reduce spatial dimensions, thereby alleviating the need to
apply max pooling:
47
48
49
50
51
# if we are to reduce the spatial size, apply a CONV layer to
# the shortcut
if red:
shortcut = Conv2D(K, (1, 1), strides=stride,
use_bias=False, kernel_regularizer=l2(reg))(act1)
If we are instructed to reduce spatial dimensions, we’ll do so with a convolutional layer (applied
to the shortcut) with a stride > 1.
The output of the final conv3 in the bottleneck is the added together with the shortcut, thus
serving as the output of the residual_module:Chapter 12. ResNet
178
53
54
# add together the shortcut and the final CONV
x = add([conv3, shortcut])
55
56
57
# return the addition as the output of the ResNet module
return x
The residual_module will serve as our building block when creating deep residual networks.
Let’s move on to using this building block inside the build method:
59
60
61
@staticmethod
def build(width, height, depth, classes, stages, filters,
reg=0.0001, bnEps=2e-5, bnMom=0.9, dataset="cifar"):
Just as our residual_module requires more parameters than previous micro-architecture
implementations, the same is true for our build function. The width, height, and depth classes
all control the input spatial dimensions of the images in our dataset. The classes variable dictates
how many overall classes our network should learn – these variables you have already seen.
What is interesting are the stages and filters parameters, both of which are lists. When
constructing the ResNet architecture, we’ll be stacking a number of residual modules on top of each
other (using the same number of filters for each stack), followed by reducing the spatial dimensions
of the volume – this process is then continued until we are ready to apply our average pooling and
softmax classifier.
To make this point clear, let’s suppose that stages=(3, 4, 6) and filters=(64, 128,
256, 512). The first filter value, 64, will be applied to the only CONV layer not part of the residual
module (i.e., first convolutional layer in the network). We’ll then stack three residual modules on
top of each other – each of these residual modules will learn K = 128 filters. The spatial dimensions
of the volume will be reduced, and then we’ll move on to the second entry in stages where we’ll
stack four residual modules on top of each other, each responsible for learning K = 256 filters.
After these four residual modules, we’ll again reduce dimensionality and move on to the final entry
in the stages list, instructing us to stack six residual modules on top of each other, where each
residual module will learn K = 512.
The benefit of specifying both stages and filters in a list (rather than hardcoding them)
is that we can easily leverage for loops to build the very deep network architectures without
introducing code bloat – this point will become more clear later in our implementation.
Finally, we have the dataset parameter which is assumed to be a string. Depending on
the dataset we are building ResNet for, we may want to apply more/less convolutions and batch
normalizations before we start stacking our residual modules. We’ll see why we might want to vary
the number of convolutional layers in Section 12.5 below, but for the time being, you can safely
ignore this parameter.
Next, let’s initialize our inputShape and chanDim based on whether we are using “channels
last” (Lines 64 and 65) or “channels first” (Lines 69-71) ordering.
62
63
64
65
# initialize the input shape to be "channels last" and the
# channels dimension itself
inputShape = (height, width, depth)
chanDim = -1
66
67
68
# if we are using "channels first", update the input shape
# and channels dimension12.2 Implementing ResNet
69
70
71
179
if K.image_data_format() == "channels_first":
inputShape = (depth, height, width)
chanDim = 1
We are now ready to define the Input to our ResNet implementation:
73
74
75
76
# set the input and apply BN
inputs = Input(shape=inputShape)
x = BatchNormalization(axis=chanDim, epsilon=bnEps,
momentum=bnMom)(inputs)
77
78
79
80
81
82
# check if we are utilizing the CIFAR dataset
if dataset == "cifar":
# apply a single CONV layer
x = Conv2D(filters[0], (3, 3), use_bias=False,
padding="same", kernel_regularizer=l2(reg))(x)
Unlike previous network architectures we have seen in this book (where the first layer is
typically a CONV), we see that ResNet uses a BN as the first layer. The reasoning behind applying
batch normalization to your input is an added level of normalization. In fact, performing batch
normalization on the input itself can sometimes remove the need to apply mean normalization to
the inputs. In either case, the BN on Lines 75 and 76 acts as an added level of normalization.
From there, we apply a single CONV layer on Lines 81 and 82. This CONV layer will learn a
total of filters[0], 3 × 3 filters (keep in mind that filters is a list, so this value is specified via
the build method when constructing the architecture).
You’ll also notice that I’ve made a check to see if we are using the CIFAR-10 dataset (Line 79).
Later in this chapter, we’ll be updating this if block to include an elif statement for Tiny ImageNet.
Since the input dimensions to Tiny ImageNet are larger, we’ll apply a series of convolutions, batch
normalizations, and max pooling (the only max pooling in the ResNet architecture) before we start
stacking residual modules. However, for the time being, we are only using the CIFAR-10 dataset.
Let’s go ahead and start stacking residual layers on top of each other, the cornerstone of the
ResNet architecture:
84
85
86
87
88
89
90
# loop over the number of stages
for i in range(0, len(stages)):
# initialize the stride, then apply a residual module
# used to reduce the spatial size of the input volume
stride = (1, 1) if i == 0 else (2, 2)
x = ResNet.residual_module(x, filters[i + 1], stride,
chanDim, red=True, bnEps=bnEps, bnMom=bnMom)
91
92
93
94
95
96
# loop over the number of layers in the stage
for j in range(0, stages[i] - 1):
# apply a ResNet module
x = ResNet.residual_module(x, filters[i + 1],
(1, 1), chanDim, bnEps=bnEps, bnMom=bnMom)
On Line 85 we start looping over the list of stages. Keep in mind that every entry in the
stages list is an integer, indicating how many residual modules will be stacked on top of each
other. Following the work of Springenberg et al., ResNet tries to reduce the usage of pooling as
much as possible, relying on CONV layers to reduce the spatial dimensions of a volume.Chapter 12. ResNet
180
To reduce volume size without pooling layers, we must set the stride of the convolution on
Line 88. If this is the first entry in the stage, we’ll set the stride to (1, 1), indicating that
no downsampling should be performed. However, for every subsequent stage we’ll apply a residual
module with a stride of (2, 2), which will allow us to decrease the volume size.
From there, we’ll loop over the number of layers in the current stage on Line 93 (i.e., the
number of residual modules that will be stacked on top of each other). The number of filters each
residual module will learn is controlled by the corresponding entry in the filters list. The reason
we use i + 1 as the index into filters is because the first filter value was used on Lines 81 and
82. The rest of the filter values correspond to the number of filters in each stage. Once we have
stacked stages[i] residual modules on top of each other, our for loop brings us back up to Lines
88-90 where we decrease the spatial dimensions of the volume and repeat the process.
At this point, our volume size has been reduced to 8 x 8 x classes (you can verify this for
yourself by computing the input/output volume sizes for each layer, or better yet, simply using the
plot_model function from Chapter 19 of the Starter Bundle).
In order to avoid using dense fully-connected layers, we’ll instead apply average pooling to
reduce the volume size to 1 x 1 x classes:
98
99
100
101
102
# apply BN => ACT => POOL
x = BatchNormalization(axis=chanDim, epsilon=bnEps,
momentum=bnMom)(x)
x = Activation("relu")(x)
x = AveragePooling2D((8, 8))(x)
From there, we create a dense layer for the total number of classes we are going to learn,
followed by applying a softmax activation to obtain our final output probabilities:
104
105
106
107
#
x
x
x
softmax classifier
= Flatten()(x)
= Dense(classes, kernel_regularizer=l2(reg))(x)
= Activation("softmax")(x)
108
109
110
# create the model
model = Model(inputs, x, name="resnet")
111
112
113
# return the constructed network architecture
return model
The fully constructed ResNet model is then returned to the calling function on Line 113.
12.3
ResNet on CIFAR-10
Outside of training smaller variants on ResNet on the full ImageNet dataset, I had never attempted
to train ResNet on CIFAR-10 (or Stanford’s Tiny ImageNet challenge, as we’ll see in this section).
Because of this fact, I have decided to treat this section and the next as candid case studies where I
reveal my personal rules of thumb and best practices I have developed over years of training neural
networks.
These best practices allow me to approach a new problem with an initial plan, iterate on it, and
eventually arrive at a solution that obtains good accuracy. In the case of CIFAR-10, we’ll be able to
replicate the performance of He et al. and claim a spot amongst other state-of-the-art approaches
[49].12.3 ResNet on CIFAR-10
12.3.1
181
Training ResNet on CIFAR-10 With the ctrl + c Method
Whenever I start a new set of experiments with either a network architecture I am unfamiliar
with, a dataset I have never worked with, or both, I always begin with the ctrl + c method of
training. Using this method, I can start training with an initial learning rate (and associated set of
hyperparameters), monitor training, and quickly adjust the learning rate based the results as they
come in. This method is especially helpful when I am totally unsure on the approximate number of
epochs it will take for a given architecture to obtain reasonable accuracy or a specific dataset.
In the case of CIFAR-10, I have previous experience (as do you, after reading all the other
chapters in this book), so I’m quite confident that it will take 60-100 epochs, but I’m not exactly
sure since I’ve never trained ResNet on the CIFAR-10 before.
Therefore, our first few experiments will rely on the ctrl + c method of training to narrow in
on what hyperparameters we should be using. Once we are comfortable with our set of hyperpa-
rameters, we’ll switch over to a specific learning rate decay schedule in hopes of milking every last
bit of accuracy out of the training process.
To get started, open up a new file, name it resnet_cifar10.py, and insert the following code:
1
2
3
# set the matplotlib backend so figures can be saved in the background
import matplotlib
matplotlib.use("Agg")
4
5
6
7
8
9
10
11
12
13
14
15
16
17
# import the necessary packages
from sklearn.preprocessing import LabelBinarizer
from pyimagesearch.nn.conv import ResNet
from pyimagesearch.callbacks import EpochCheckpoint
from pyimagesearch.callbacks import TrainingMonitor
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import SGD
from keras.datasets import cifar10
from keras.models import load_model
import keras.backend as K
import numpy as np
import argparse
import sys
18
19
20
# set a high recursion limit so Theano doesn’t complain
sys.setrecursionlimit(5000)
We start by importing our required Python packages on Lines 6-17. Since we’ll be using the
ctrl + c method to training, we’ll make sure to import the EpochCheckpoint class (Line 8) to
serialize ResNet weights to disk during the training process, allowing us to stop and restart training
from a specific checkpoint. Since this is our first experiment with ResNet, we’ll be using the SGD
optimizer (Line 11) – time will tell if we decide to switch and use a different optimizer (we’ll let
our results dictate that).
On Line 20 I update the recursion limit for the Python programming language. I wrote this
book with both TensorFlow and Theano in mind, so if you are using TensorFlow, you don’t have
to worry about this line. However, if you are using Theano, you may encounter an error when
instantiating the ResNet architecture that a maximum recursion level has been reached. This is
a known “bug” with Theano and can be resolved simply by increasing the recursion limit of the
Python programming language [50].
Next, let’s parse our command line arguments:182
22
23
24
25
26
27
28
29
30
Chapter 12. ResNet
# construct the argument parse and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-c", "--checkpoints", required=True,
help="path to output checkpoint directory")
ap.add_argument("-m", "--model", type=str,
help="path to *specific* model checkpoint to load")
ap.add_argument("-s", "--start-epoch", type=int, default=0,
help="epoch to restart training at")
args = vars(ap.parse_args())
Our script will require only the --checkpoints switch, the path to the directory where we
will store the ResNet weights every N epochs. In the case that we need to restart training from a
particular epoch, we can supply the --model path along with an integer indicating the specific epoch
number.
The next step is to load the CIFAR-10 dataset from disk (pre-split into training and testing),
perform mean subtraction, and one-hot encode the integer labels as vectors:
32
33
34
35
36
37
# load the training and testing data, converting the images from
# integers to floats
print("[INFO] loading CIFAR-10 data...")
((trainX, trainY), (testX, testY)) = cifar10.load_data()
trainX = trainX.astype("float")
testX = testX.astype("float")
38
39
40
41
42
# apply mean subtraction to the data
mean = np.mean(trainX, axis=0)
trainX -= mean
testX -= mean
43
44
45
46
47
# convert the labels from integers to vectors
lb = LabelBinarizer()
trainY = lb.fit_transform(trainY)
testY = lb.transform(testY)
While we’re at it, let’s also initialize an ImageDataGenerator so we can apply data augmen-
tation to CIFAR-10:
49
50
51
52
# construct the image generator for data augmentation
aug = ImageDataGenerator(width_shift_range=0.1,
height_shift_range=0.1, horizontal_flip=True,
fill_mode="nearest")
In the case we are training ResNet from the very first epoch, we need to instantiate the network
architecture:
54
55
56
57
58
59
# if there is no specific model checkpoint supplied, then initialize
# the network (ResNet-56) and compile the model
if args["model"] is None:
print("[INFO] compiling model...")
opt = SGD(lr=1e-1)
model = ResNet.build(32, 32, 3, 10, (9, 9, 9),12.3 ResNet on CIFAR-10
60
61
62
183
(64, 64, 128, 256), reg=0.0005)
model.compile(loss="categorical_crossentropy", optimizer=opt,
metrics=["accuracy"])
To start, take a look at the learning rate for our SGD optimizer on Line 58 – at 1e − 1 this
learning rate is by far the largest we have used in this book (by an order of magnitude). The reason
we are able to get away with such a high learning rate is due to the identity mappings built into the
residual module. Learning rates this high would not (typically) work for networks such as AlexNet,
VGG, etc.
We then instantiate our ResNet model on Lines 59 and 60. Here we can see that the network
will accept input images with a width of 32 pixels, height of 32 pixels, and depth of 3 (one for each
of the RGB channels in the CIFAR-10 dataset). Since the CIFAR-10 dataset has ten classes, we’ll
learn ten output labels.
The next parameter we need to supply is (9, 9, 9), or the number of stages in our architecture.
This tuple indicates that we will be learning three stages with each stage containing nine residual
modules stacked on top of each other. In between each stage, we will apply an additional residual
module to decrease the volume size.
The next parameter, (64, 64, 128, 256) is the number of filters that the CONV layers will
learn. The first CONV layer (before any residual model is applied) will learn K = 64 filters. The
remaining entries, 64, 128, and 256 correspond to the number of filters each of the residual module
stages will learn. For example, the first nine residual modules will learn K = 64 filters. The
second set of nine residual modules will learn K = 128 filters. And finally, the last set of nine
residual modules will learn K = 256 filters. The last argument we’ll supply to ResNet is reg, or
our L2 regularization strength for weight decay – this value is crucial as it will enable us to prevent
overfitting.
In the case we start restarting training from a specific epoch, we need to load the network
weights from disk and update the learning rate:
64
65
66
67
# otherwise, load the checkpoint from disk
else:
print("[INFO] loading {}...".format(args["model"]))
model = load_model(args["model"])
68
69
70
71
72
73
74
# update the learning rate
print("[INFO] old learning rate: {}".format(
K.get_value(model.optimizer.lr)))
K.set_value(model.optimizer.lr, 1e-5)
print("[INFO] new learning rate: {}".format(
K.get_value(model.optimizer.lr)))
Let’s also construct a set of callbacks so we can both (1) checkpoint ResNet weights every
five epochs and (2) monitor training:
76
77
78
79
80
81
82
# construct the set of callbacks
callbacks = [
EpochCheckpoint(args["checkpoints"], every=5,
startAt=args["start_epoch"]),
TrainingMonitor("output/resnet56_cifar10.png",
jsonPath="output/resnet56_cifar10.json",
startAt=args["start_epoch"])]184
Chapter 12. ResNet
Finally, we’ll train our network in batch sizes of 128:
84
85
86
87
88
89
90
# train the network
print("[INFO] training network...")
model.fit_generator(
aug.flow(trainX, trainY, batch_size=128),
validation_data=(testX, testY),
steps_per_epoch=len(trainX) // 128, epochs=10,
callbacks=callbacks, verbose=1)
Now that our resnet_cifar10.py script is coded up, let’s move on to running experiments
with it.
ResNet on CIFAR-10: Experiment #1
Figure 12.4: Top-left: First 50 epochs when training ResNet on CIFAR-10 in Experiment #1.
Top-right: Next 25 epochs. Bottom: Final 10 epochs.12.3 ResNet on CIFAR-10
185
In my very first experiment with CIFAR-10, I was worried about the number of filters in the
network, especially regarding overfitting. Because of this concern, my initial filter list consisted
of (16, 16, 32, 64) along with (9, 9, 9) stages of residual modules. I also applied a very
small amount of L2 regularization with reg=0.0001 – I knew regularization would be needed, but
I wasn’t sure on the correct amount (yet). ResNet was trained using SGD with a base learning rate
of 1e − 1 and a momentum term of 0.9.
I started training using the following command:
$ python resnet_cifar10.py --checkpoints output/checkpoints
Past epoch 50 I noticed training loss starting to slow as well as some volatility in the validation
loss (and a growing gap between the two) (Figure 12.4, top-left). I stopped training, lowered the
learning rate to 1e − 2, and then continued training:
$ python resnet_cifar10.py --checkpoints output/checkpoints \
--model output/checkpoints/epoch_50.hdf5 --start-epoch 50
The drop in learning rate proved very effective, stabilizing validation loss, but also overfitting
on the training set start to creep in (in inevitability when working with CIFAR-10) around epoch 75
(Figure 12.4, top-right). After epoch 75 I once again stopped training, lowered the learning rate to
1e − 3, and allowed ResNet to continue training for another 10 epochs:
$ python resnet_cifar10.py --checkpoints output/checkpoints \
--model output/checkpoints/epoch_75.hdf5 --start-epoch 75
The final plot is shown in Figure 12.4 (bottom), where we reach 89.06% accuracy on the
validation set. For our very first experiment 89.06% is a good start; however, it’s not as high as the
90.81% achieved by GoogLeNet in Chapter 11. Furthermore, He et al. reported an accuracy of
93% with ResNet on CIFAR-10, so we clearly have some work to do.
12.3.2
ResNet on CIFAR-10: Experiment #2
Our previous experiment achieved a reasonable accuracy of 89.06% accuracy – but we need higher
accuracy. Instead of increasing the depth of the network (by adding more stages), I decided to add
more filters to each of the CONV layers. Thus, my filters list was updated to be (16, 64, 128,
256).
Notice how the number of filters in all residual modules have doubled from the previous
experiment (the number of filters in the first CONV layer was left the same). SGD was once again
used to train the network with a momentum term of 0.9. I also kept the regularization term at
0.0001.
In Figure 12.5 (top-left) you can find a plot of my first 40 epochs: We can clearly see a gap
between training loss and validation loss, but overall, validation accuracy is still keeping up with
training accuracy. In an effort to improve the accuracy, I decided to lower the learning rate from
1e − 1 to 1e − 2 and train for another five epochs – the result was that learning stagnated entirely
(top-right). Lowering learning rate again from 1e − 2 to 1e − 3 even caused overfitting through a
slight rise in validation loss (bottom)
Interestingly, the loss stagnated for both the training set and the validation set, not unlike
previous experiments we’ve run with GoogLeNet. After the initial drop in learning rate, it appears
that our network could not learn any more underlying patterns in the dataset. All that said, after
the 50th epoch validation accuracy had increased to 90.10%, an improvement from our first
experiment.186
Chapter 12. ResNet
Figure 12.5: Top-left: First 40 epochs when training ResNet on CIFAR-10 in Experiment #2.
Top-right: Next 5 epochs. Bottom: Final 5 epochs.
ResNet on CIFAR-10: Experiment #3
At this point, I was starting to become more comfortable training ResNet on CIFAR-10. Clearly
the increase of filters helped, but the stagnation in learning after the first learning rate drop was
still troubling. I was confident that a slow, linear decrease in learning rate would help combat
this problem, but I wasn’t convinced that I had obtained a good set of hyperparameters to warrant
switching over to learning rate decay.
Instead, I decided to increase the number of filters learned in the first CONV layer to 64 (up
from 16), turning the filters list into (64, 64, 128, 256). The increase in filters helped in
the second experiment, and there is no reason the first CONV layer should miss out on these benefits
as well. The SGD optimizer was left alone with an initial learning rate of 1e − 1 and momentum of
0.9.
Furthermore, I also decided to dramatically increase regularization from 0.0001 to 0.0005. I
had a suspicion that allowing the network to train for longer would result in higher validation
accuracy – using a larger regularization term would likely enable me to train for longer.
I was also considering lowering my learning rate, but given that the network was making12.3 ResNet on CIFAR-10
187
Figure 12.6: Top-left: First 80 epochs when training ResNet on CIFAR-10 in Experiment #3.
Top-right: Next 10 epochs. Bottom: Final 10 epochs.
traction without a problem at 1e − 1, it hardly seemed worth it to lower to 1e − 2. Doing so might
have stabilized training (i.e., less fluctuation in validation loss/accuracy), but would have ultimately
led to lower accuracy after training completed. If my larger learning rate + larger regularization
term suspicion turned out to be correct, then it would make sense to switch over to a learning rate
decay to avoid stagnation after order of magnitude drops. But before I could make this switch, I
first needed to prove my hunch.
As my plot of the first 80 epochs demonstrates (Figure 12.6, top-left), there certainly is
overfitting as validation quickly diverges from training. However, what’s interesting here is that
while overfitting is undoubtedly occurring (as training loss drops much faster than validation loss),
we are able to train the network for longer without validation loss starting to increase.
After the 80th epoch, I stopped training, lowered the learning rate to 1e − 2, then trained for
another 10 epochs (Figure 12.6, top-right). We see an initial drop and loss and increase in accuracy,
but from there validation loss/accuracy plateaus. Furthermore, we can start to see the validation
loss increase, a sure sign of overfitting.188
Chapter 12. ResNet
To validate that overfitting was indeed happening, I stopped training at epoch 90, lowered the
learning rate to 1e-3, then trained for another 10 epochs (Figure 12.6, bottom). Sure enough, this
is the telltale sign of overfitting: validation loss increasing while training loss decreases/remains
constant.
However, what’s very interesting is that after the 100th epoch we obtained 91.83% validation
accuracy, higher than our third experiment. The downside is that we are overfit – we need a way to
maintain this level of accuracy (and increase it) without overfitting. To do so, I decided to switch
from ctrl + c training to learning rate decay.
12.4
Training ResNet on CIFAR-10 with Learning Rate Decay
At this point, it seems that we have gotten as far as we can using standard ctrl + c training. We’ve
also been able to see that our most successful experiments occur when we can train for longer, in
the range of 80-100 epochs. However, there are two major problems we need to overcome:
1. Whenever we drop the learning rate by an order of magnitude and restart training, we obtain
a nice bump in accuracy, but then we quickly plateau.
2. We are overfitting.
To solve these problems, and boost accuracy further, a good experiment to try is linearly
decreasing the learning rate over a large number of epochs, typically about the same as your
longest ctrl + c experiments (if not slightly longer). To start this, let’s open up a new file, name
it resnet_cifar10_decay.py, and insert the following code:
1
2
3
# set the matplotlib backend so figures can be saved in the background
import matplotlib
matplotlib.use("Agg")
4
5
6
7
8
9
10
11
12
13
14
15
16
# import the necessary packages
from sklearn.preprocessing import LabelBinarizer
from pyimagesearch.nn.conv import ResNet
from pyimagesearch.callbacks import TrainingMonitor
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import LearningRateScheduler
from keras.optimizers import SGD
from keras.datasets import cifar10
import numpy as np
import argparse
import sys
import os
17
18
19
# set a high recursion limit so Theano doesn’t complain
sys.setrecursionlimit(5000)
Line 2 configures matplotlib so we can save plots in the background. We then import the
remainder of our Python packages on Lines 6-16. Take a look at Line 10 where we import our
LearningRateScheduler so we can define a custom learning rate decay for the training process.
We then set a high system recursion limit in order to avoid any issues with the Theano backend
(just in case you are using it).
The next step is to define the learning rate decay schedule:
21
22
# define the total number of epochs to train for along with the
# initial learning rate12.4 Training ResNet on CIFAR-10 with Learning Rate Decay
23
24
189
NUM_EPOCHS = 100
INIT_LR = 1e-1
25
26
27
28
29
30
31
def poly_decay(epoch):
# initialize the maximum number of epochs, base learning rate,
# and power of the polynomial
maxEpochs = NUM_EPOCHS
baseLR = INIT_LR
power = 1.0
32
33
34
# compute the new learning rate based on polynomial decay
alpha = baseLR * (1 - (epoch / float(maxEpochs))) ** power
35
36
37
# return the new learning rate
return alpha
We’ll train our network for a total of 100 epochs with a base learning rate of 1e − 1. The
poly_decay function will decay our 1e − 1 learning rate linearly over the course of 100 epochs.
This is a linear decay due to the fact that we set power=1. For more information on learning rate
schedules, see Chapter 16 of the Starter Bundle along with Chapter 11 of the Practitioner Bundle.
We then need to supply two command line arguments:
39
40
41
42
43
44
45
# construct the argument parse and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-m", "--model", required=True,
help="path to output model")
ap.add_argument("-o", "--output", required=True,
help="path to output directory (logs, plots, etc.)")
args = vars(ap.parse_args())
The --model switch controls the path to our final serialized model after training, while
--output is the base directory to where we will store any logs, plots, etc.
We can now load the CIFAR-10 dataset and mean normalize it:
47
48
49
50
51
52
# load the training and testing data, converting the images from
# integers to floats
print("[INFO] loading CIFAR-10 data...")
((trainX, trainY), (testX, testY)) = cifar10.load_data()
trainX = trainX.astype("float")
testX = testX.astype("float")
53
54
55
56
57
# apply mean subtraction to the data
mean = np.mean(trainX, axis=0)
trainX -= mean
testX -= mean
Encode the integer labels as vectors:
59
60
61
62
# convert the labels from integers to vectors
lb = LabelBinarizer()
trainY = lb.fit_transform(trainY)
testY = lb.transform(testY)190
Chapter 12. ResNet
As well as initialize our ImageDataGenerator for data argumentation:
64
65
66
67
# construct the image generator for data augmentation
aug = ImageDataGenerator(width_shift_range=0.1,
height_shift_range=0.1, horizontal_flip=True,
fill_mode="nearest")
Our callbacks list will consist of both a TrainingMonitor along with a LearningRateScheduler
with the poly_decay function supplied as the only argument – this class will allow us to decay our
learning rate as we train.
69
70
71
72
73
74
75
# construct the set of callbacks
figPath = os.path.sep.join([args["output"], "{}.png".format(
os.getpid())])
jsonPath = os.path.sep.join([args["output"], "{}.json".format(
os.getpid())])
callbacks = [TrainingMonitor(figPath, jsonPath=jsonPath),
LearningRateScheduler(poly_decay)]
We’ll then instantiate ResNet with the best parameters we found from Section 12.6 (three
stacks of nine residual modules: 64 filters in the first CONV layer before the residual modules, and
64, 128, and 256 filters for each stack of respective residual modules):
77
78
79
80
81
82
83
# initialize the optimizer and model (ResNet-56)
print("[INFO] compiling model...")
opt = SGD(lr=INIT_LR, momentum=0.9)
model = ResNet.build(32, 32, 3, 10, (9, 9, 9),
(64, 64, 128, 256), reg=0.0005)
model.compile(loss="categorical_crossentropy", optimizer=opt,
metrics=["accuracy"])
We’ll then train our network using learning rate decay:
85
86
87
88
89
90
91
# train the network
print("[INFO] training network...")
model.fit_generator(
aug.flow(trainX, trainY, batch_size=128),
validation_data=(testX, testY),
steps_per_epoch=len(trainX) // 128, epochs=10,
callbacks=callbacks, verbose=1)
92
93
94
95
# save the network to disk
print("[INFO] serializing network...")
model.save(args["model"])
The big question is – will our learning rate decay pay off? To find out, proceed to the next
section.12.4 Training ResNet on CIFAR-10 with Learning Rate Decay
191
ResNet on CIFAR-10: Experiment #4
As the code in the previous section indicates, we are going to use the SGD optimizer with a base
learning rate of 1e − 1 and a momentum term of 0.9. We’ll train ResNet for a total of 100 epochs,
linearly decreasing the linear rate from 1e − 1 down to zero. To train ResNet on CIFAR-10 with
learning rate decay, I executed the following command:
$ python resnet_cifar10_decay.py --output output \
--model output/resnet_cifar10.hdf5
Figure 12.7: Training ResNet on CIFAR-10 using learning rate decay beats out all previous
experiments.
After training was complete, I took a look at the plot (Figure 12.7). As in previous experiments,
training and validation loss start to diverge early on, but more importantly, the gap remains
approximately constant after the initial divergence. This result is important as it indicates that our
overfitting is controlled. We have to accept that we will overfit when training on CIFAR-10, but we
need to control this overfitting. By applying learning rate decay, we were able to successfully do so.
The question is, did we obtain higher classification accuracy? To answer that,
take a look at the output of the last few epochs:
...
Epoch 98/100
247s - loss: 0.1563 - acc: 0.9985 - val_loss: 0.3987 - val_acc: 0.9351
Epoch 99/100
245s - loss: 0.1548 - acc: 0.9987 - val_loss: 0.3973 - val_acc: 0.9358
Epoch 100/100192
Chapter 12. ResNet
244s - loss: 0.1538 - acc: 0.9990 - val_loss: 0.3978 - val_acc: 0.9358
[INFO] serializing network...
After the 100th epoch, ResNet is reaching 93.58% accuracy on our testing set. This result
is substantially higher than our previous two experiments, and more importantly, it has allowed us
to replicate the results from He et al. when training ResNet on CIFAR-10.
Taking a look at the CIFAR-10 leaderboard [49], we see that He et al. reached 93.57% accuracy
[24], near identical to our result (Figure 12.8). The red arrow indicates our accuracy, safely landing
us in the top-10 leaderboard.
Figure 12.8: We have successfully replicated the work of He et al. when applying ResNet to
CIFAR-10 down to 0.01%.
12.5
ResNet on Tiny ImageNet
In this section, we will train the ResNet architecture (with bottleneck and pre-activation) on
Stanford’s cs231n Tiny ImageNet challenge. Similar to the ResNet + CIFAR-10 experiments earlier
in this chapter, I have never trained ResNet on Tiny ImageNet before, so I’m going to apply my
same exact experiment process:
1. Start with ctrl + c-based training to obtain a baseline.
2. If stagnation/plateauing occurs after order of magnitude learning rate drops, then switch over
to learning rate decay.
Given that we’ve already applied a similar technique to CIFAR-10, we should be able to save
ourselves some time noticing signs of overfitting and plateauing earlier. That said, let’s get started
by reviewing directory structure for this project, which is near identical to the GoogLeNet and Tiny
ImageNet challenge from the previous chapter:
--- resnet_tinyimagenet.py
|
|--- config
|
|
|--- __init__.py12.5 ResNet on Tiny ImageNet
|
|
|
|
|
|
|
|
|---
|---
|---
|---
|
|
193
|--- tiny_imagenet_config.py
rank_accuracy.py
train.py
train_decay.py
output/
|--- checkpoints/
|--- tiny-image-net-200-mean.json
Here you can see we have created config Python module where we have stored a file named
tiny_imagenet_config.py.
This file was copied directly from the GoogLeNet chapter. I then updated Lines 31-39 to point
to the output ResNet MODEL_PATH, FIG_PATH (learning plot), and JSON_PATH (serialized log of
training history):
31
32
33
34
35
36
37
38
39
# define the path to the output directory used for storing plots,
# classification reports, etc.
OUTPUT_PATH = "output"
MODEL_PATH = path.sep.join([OUTPUT_PATH,
"resnet_tinyimagenet.hdf5"])
FIG_PATH = path.sep.join([OUTPUT_PATH,
"resnet56_tinyimagenet.png"])
JSON_PATH = path.sep.join([OUTPUT_PATH,
"resnet56_tinyimagenet.json"])
From there we have train.py which will be responsible for training ResNet using the standard
ctrl + c method. In the case that we wish to apply learning rate decay, we’ll be able to use
train_decay.py. Finally, rank_accuracy.py will be used to compute the rank-1 and rank-5
accuracy of ResNet on Tiny ImageNet.
12.5.1
Updating the ResNet Architecture
Earlier in this chapter we reviewed our implementation of the ResNet architecture in detail.
Specifically, we noted the dataset parameter supplied to the build method, like so:
59
60
61
@staticmethod
def build(width, height, depth, classes, stages, filters,
reg=0.0001, bnEps=2e-5, bnMom=0.9, dataset="cifar"):
This value defaulted to cifar; however, since we are now working with Tiny ImageNet, we
need to update our ResNet implementation to include an if/elif block. Go ahead and open up
your resnet.py file in the nn.conv sub-module of PyImageSearch and insert the following code:
78
79
80
81
82
# check if we are utilizing the CIFAR dataset
if dataset == "cifar":
# apply a single CONV layer
x = Conv2D(filters[0], (3, 3), use_bias=False,
padding="same", kernel_regularizer=l2(reg))(x)
83
84
85
86
# check to see if we are using the Tiny ImageNet dataset
elif dataset == "tiny_imagenet":
# apply CONV => BN => ACT => POOL to reduce spatial sizeChapter 12. ResNet
194
87
88
89
90
91
92
93
x = Conv2D(filters[0], (5, 5), use_bias=False,
padding="same", kernel_regularizer=l2(reg))(x)
x = BatchNormalization(axis=chanDim, epsilon=bnEps,
momentum=bnMom)(x)
x = Activation("relu")(x)
x = ZeroPadding2D((1, 1))(x)
x = MaxPooling2D((3, 3), strides=(2, 2))(x)
Lines 79-82 we have already reviewed before – these lines are where we apply a single 3 × 3
CONV layer for CIFAR-10. However, we are now updating the architecture to check for Tiny
ImageNet (Line 85).
Provided we are instantiating ResNet for Tiny ImageNet, we need to add in some additional
layers. To start, we apply 5 × 5 CONV layer to learn larger feature maps (in the full ImageNet dataset
implementation, we’ll actually be learning 7 × 7 filters).
Next, we apply a batch normalization followed a ReLU activation. Max pooling, the only max
pooling layer in the ResNet architecture, is applied on Line 93 using a size of 3 × 3 and stride
of 2 × 2. Combined with the previous zero padding layer (Line 92), pooling ensures that our
output spatial volume size is 32 × 32, the exact same spatial dimensions as the input images from
CIFAR-10. Validating that the output volume size is 32 × 32 ensures we can easily reuse the rest of
the ResNet implementation without having to make any additional changes.
12.5.2
Training ResNet on Tiny ImageNet With the ctrl + c Method
Now that our ResNet implementation has been updated, let’s code up a Python script responsible
for the actual training process. Open up a new file, name it train.py, and insert the following
code:
1
2
3
# set the matplotlib backend so figures can be saved in the background
import matplotlib
matplotlib.use("Agg")
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
# import the necessary packages
from config import tiny_imagenet_config as config
from pyimagesearch.preprocessing import ImageToArrayPreprocessor
from pyimagesearch.preprocessing import SimplePreprocessor
from pyimagesearch.preprocessing import MeanPreprocessor
from pyimagesearch.callbacks import EpochCheckpoint
from pyimagesearch.callbacks import TrainingMonitor
from pyimagesearch.io import HDF5DatasetGenerator
from pyimagesearch.nn.conv import ResNet
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import SGD
from keras.models import load_model
import keras.backend as K
import argparse
import json
import sys
21
22
23
# set a high recursion limit so Theano doesn’t complain
sys.setrecursionlimit(5000)
Lines 2 and 3 configure matplotlib so we can save our figures and plots to disk during the
training process. We then import the remainder of our Python packages on Lines 6-20. We have12.5 ResNet on Tiny ImageNet
195
seen all of these imports before from Chapter 11 on GoogLeNet + Tiny ImageNet, only now we are
importing ResNet (Line 13) rather than GoogLeNet. We’ll also update the maximum recursion
limit just in case you are using the Theano backend.
Next comes our command line arguments:
25
26
27
28
29
30
31
32
33
# construct the argument parse and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-c", "--checkpoints", required=True,
help="path to output checkpoint directory")
ap.add_argument("-m", "--model", type=str,
help="path to *specific* model checkpoint to load")
ap.add_argument("-s", "--start-epoch", type=int, default=0,
help="epoch to restart training at")
args = vars(ap.parse_args())
These command line switches can be used to start training from scratch or restart training from
a specific epoch. For a more detailed review of each command line argument, please see Section
12.3.1.
Let’s also initialize our ImageDataGenerator that will be applied to the training set for data
augmentation:
35
36
37
38
# construct the training image generator for data augmentation
aug = ImageDataGenerator(rotation_range=18, zoom_range=0.15,
width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,
horizontal_flip=True, fill_mode="nearest")
39
40
41
# load the RGB means for the training set
means = json.loads(open(config.DATASET_MEAN).read())
Line 41 then loads our RGB means computed over the training set of mean subtraction.
Our image pre-processors are fairly standard here, consisting of ensuring the image is resized
to 64 × 64 pixels, performing mean normalization, and then converting the image to a Keras-
compatible array:
43
44
45
46
# initialize the image preprocessors
sp = SimplePreprocessor(64, 64)
mp = MeanPreprocessor(means["R"], means["G"], means["B"])
iap = ImageToArrayPreprocessor()
47
48
49
50
51
52
# initialize the training and validation dataset generators
trainGen = HDF5DatasetGenerator(config.TRAIN_HDF5, 64, aug=aug,
preprocessors=[sp, mp, iap], classes=config.NUM_CLASSES)
valGen = HDF5DatasetGenerator(config.VAL_HDF5, 64,
preprocessors=[sp, mp, iap], classes=config.NUM_CLASSES)
Based on the image pre-processors and data augmentation, we can then construct an HDF5DatasetGenerator
for both the training and validation datasets (Lines 49-52). Batches of 64 images will be polled at a
time for these generators and passed through the network.
If we are training ResNet from the very first epoch, we need to instantiate the model and
compile it:Chapter 12. ResNet
196
54
55
56
57
58
59
60
61
62
# if there is no specific model checkpoint supplied, then initialize
# the network and compile the model
if args["model"] is None:
print("[INFO] compiling model...")
model = ResNet.build(64, 64, 3, config.NUM_CLASSES, (3, 4, 6),
(64, 128, 256, 512), reg=0.0005, dataset="tiny_imagenet")
opt = SGD(lr=1e-1, momentum=0.9)
model.compile(loss="categorical_crossentropy", optimizer=opt,
metrics=["accuracy"])
Lines 58 and 59 initialize the ResNet model itself. The model will accept input images with
64 × 64 × 3 spatial dimensions and will learn a total of NUM_CLASSES, which in the case of Tiny
ImageNet is 200. My choice for stages=(3, 4, 6) was inspired by the Deep Residual Learning
for Image Recognition [24] paper where a similar version of residual module layer stacking was
used for the full ImageNet dataset.
Given that Tiny ImageNet will require more discriminative filters than CIFAR-10, I also updated
the filters list to learn more filters for each of the CONV layers: (64, 128, 256, 512). This
list of filters implies that the first CONV layer (before any of the residual modules) will learn a total
of 64 5 × 5 filters. From there, three residual modules will be stacked on top of each other, each
responsible for learning K = 128 filters. Dimensionality is then reduced, then four residual modules
are stacked, this time learning K = 256 filters. Once again the spatial dimensions of the volume are
reduced, then six residual modules are stacked, each module learning K = 512 filters.
We’ll also apply a regularization strength of 0.0005 as regularization seemed to aide us in
training ResNet on CIFAR-10. To train the network, SGD will be used with a base learning rate of
1e − 1 and a momentum term of 0.9.
If we have stopped training, updated any hyperparameters (such as the learning rate), and wish
to restart training, the following code block will handle that process for us:
64
65
66
67
# otherwise, load the checkpoint from disk
else:
print("[INFO] loading {}...".format(args["model"]))
model = load_model(args["model"])
68
69
70
71
72
73
74
# update the learning rate
print("[INFO] old learning rate: {}".format(
K.get_value(model.optimizer.lr)))
K.set_value(model.optimizer.lr, 1e-5)
print("[INFO] new learning rate: {}".format(
K.get_value(model.optimizer.lr)))
Our callbacks list will consist of checkpointing ResNet weights to disk every five epochs,
followed by plotting the training history:
76
77
78
79
80
81
# construct the set of callbacks
callbacks = [
EpochCheckpoint(args["checkpoints"], every=5,
startAt=args["start_epoch"]),
TrainingMonitor(config.FIG_PATH, jsonPath=config.JSON_PATH,
startAt=args["start_epoch"])]12.5 ResNet on Tiny ImageNet
197
Finally, we’ll kick off the training process and train our network using mini-batches of size 64:
83
84
85
86
87
88
89
90
91
# train the network
model.fit_generator(
trainGen.generator(),
steps_per_epoch=trainGen.numImages // 64,
validation_data=valGen.generator(),
validation_steps=valGen.numImages // 64,
epochs=50,
max_queue_size=64 * 2,
callbacks=callbacks, verbose=1)
92
93
94
95
# close the databases
trainGen.close()
valGen.close()
The exact number of epochs we’ll need to train ResNet is unknown at this point, so we’ll set
epochs to a large number and adjust as needed.
ResNet on Tiny ImageNet: Experiment #1
To start training, I executed the following command:
$ python train.py --checkpoints output/checkpoints
After monitoring training for the first 25 epochs, it became clear that training loss was starting
to stagnate a bit (Figure 12.9, top-left). To combat this stagnation, I stopped training, reduced my
learning rate from 1e − 1 to 1e − 2, and resumed training:
$ python train.py --checkpoints output/checkpoints \
--model output/checkpoints/epoch_25.hdf5 --start-epoch 25
Training continued from epochs 25-35 at this lower learning rate. We can immediately see
the benefit of lowering the learning rate by an order of magnitude – loss drops dramatically, and
accuracy enjoys a nice bump. (Figure 12.9, top-right).
However, after this initial bump, the training loss continued to drop at a much faster rate than
the validation loss. I once again stopped training at epoch 35, lowered the learning rate from 1e − 2
to 1e − 3, and resumed training:
$ python train.py --checkpoints output/checkpoints \
--model output/checkpoints/epoch_35.hdf5 --start-epoch 35
I only allowed ResNet to train for another 5 epochs as I started to notice clear signs of overfitting
(Figure 12.9, bottom). Loss dips slightly upon the 1e − 3 change, then starts to increase, all the
while training loss decreases at a faster rate – this is a telltale sign of overfitting. I stopped training
altogether at this point, and noted the validation accuracy to be 53.14%.
To determine the accuracy on the testing set, I executed the following command (keeping in
mind that the rank_accuracy.py script is identical to the one from Chapter 11 on GoogLeNet:198
Chapter 12. ResNet
Figure 12.9: Top-left: First 25 epochs when training ResNet on Tiny ImageNet in Experiment
#1. Top-right: Next 10 epochs. Bottom: Final 5 epochs. Notice the telltale sign of overfitting as
validation loss starts to increase during the final epochs.
$ python rank_accuracy.py
[INFO] loading model...
[INFO] predicting on test data...
[INFO] rank-1: 53.10%
[INFO] rank-5: 75.43%
As the output demonstrates, we are obtaining 53.10% rank-1 accuracy on the testing set. This
first experiment was not a bad one as we are already closing in on the GoogLeNet + Tiny ImageNet
accuracy. Given the success of applying learning rate decay to Tiny ImageNet, I immediately
decided to apply the same process to ResNet.
12.5.3
Training ResNet on Tiny ImageNet with Learning Rate Decay
To train ResNet using learning rate decay, open up a new file, name it train_decay.py, and insert
the following code:12.5 ResNet on Tiny ImageNet
1
2
3
199
# set the matplotlib backend so figures can be saved in the background
import matplotlib
matplotlib.use("Agg")
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
# import the necessary packages
from config import tiny_imagenet_config as config
from pyimagesearch.preprocessing import ImageToArrayPreprocessor
from pyimagesearch.preprocessing import SimplePreprocessor
from pyimagesearch.preprocessing import MeanPreprocessor
from pyimagesearch.callbacks import TrainingMonitor
from pyimagesearch.io import HDF5DatasetGenerator
from pyimagesearch.nn.conv import ResNet
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import LearningRateScheduler
from keras.optimizers import SGD
import argparse
import json
import sys
import os
20
21
22
# set a high recursion limit so Theano doesn’t complain
sys.setrecursionlimit(5000)
Lines 6-19 import our required Python packages. We’ll need to import the LearningRateScheduler
class on Line 14 so we can apply a learning rate schedule to the training process. The system
recursion limit is then set on Line 22 just in case you are using the Theano backend.
Next, we define the actual function responsible for applying the learning rate decay:
24
25
26
27
# define the total number of epochs to train for along with the
# initial learning rate
NUM_EPOCHS = 75
INIT_LR = 1e-1
28
29
30
31
32
33
34
def poly_decay(epoch):
# initialize the maximum number of epochs, base learning rate,
# and power of the polynomial
maxEpochs = NUM_EPOCHS
baseLR = INIT_LR
power = 1.0
35
36
37
# compute the new learning rate based on polynomial decay
alpha = baseLR * (1 - (epoch / float(maxEpochs))) ** power
38
39
40
# return the new learning rate
return alpha
Here we indicate that we’ll train for a maximum of 75 epochs (Line 26) with a base learning
rate of 1e − 1 (Line 27). The poly_decay function is defined on Line 29 which accepts a single
parameter, the current epoch number. We set power=1.0 on Line 34 to turn the polynomial decay
into a linear one (see Chapter 11 for more details). The new learning rate (based on the current
epoch) is computed on Line 37 and then returned to the calling function on Line 40.
Let’s move on to the command line arguments:200
42
43
44
45
46
47
48
Chapter 12. ResNet
# construct the argument parse and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-m", "--model", required=True,
help="path to output model")
ap.add_argument("-o", "--output", required=True,
help="path to output directory (logs, plots, etc.)")
args = vars(ap.parse_args())
Here we simply need to provide a path to our output serialized --model after training is
complete along with an --output path to store any plots/logs.
We can now initialize our data augmentation class and load the RGB means from disk:
50
51
52
53
# construct the training image generator for data augmentation
aug = ImageDataGenerator(rotation_range=18, zoom_range=0.15,
width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,
horizontal_flip=True, fill_mode="nearest")
54
55
56
# load the RGB means for the training set
means = json.loads(open(config.DATASET_MEAN).read())
As well as create our training and validation HDF5DatasetGenerators:
58
59
60
61
# initialize the image preprocessors
sp = SimplePreprocessor(64, 64)
mp = MeanPreprocessor(means["R"], means["G"], means["B"])
iap = ImageToArrayPreprocessor()
62
63
64
65
66
67
# initialize the training and validation dataset generators
trainGen = HDF5DatasetGenerator(config.TRAIN_HDF5, 64, aug=aug,
preprocessors=[sp, mp, iap], classes=config.NUM_CLASSES)
valGen = HDF5DatasetGenerator(config.VAL_HDF5, 64,
preprocessors=[sp, mp, iap], classes=config.NUM_CLASSES)
Our callbacks list will consist of a TrainingMonitor and a LearningRateScheduler:
69
70
71
72
73
74
75
# construct the set of callbacks
figPath = os.path.sep.join([args["output"], "{}.png".format(
os.getpid())])
jsonPath = os.path.sep.join([args["output"], "{}.json".format(
os.getpid())])
callbacks = [TrainingMonitor(figPath, jsonPath=jsonPath),
LearningRateScheduler(poly_decay)]
Below we instantiate our ResNet architecture and SGD optimizer using the same parameters
as our first experiment:
77
78
79
80
# initialize the optimizer and model (ResNet-56)
print("[INFO] compiling model...")
model = ResNet.build(64, 64, 3, config.NUM_CLASSES, (3, 4, 6),
(64, 128, 256, 512), reg=0.0005, dataset="tiny_imagenet")12.5 ResNet on Tiny ImageNet
81
82
83
201
opt = SGD(lr=INIT_LR, momentum=0.9)
model.compile(loss="categorical_crossentropy", optimizer=opt,
metrics=["accuracy"])
Finally, we can train our network in mini-batches of 64:
85
86
87
88
89
90
91
92
93
94
# train the network
print("[INFO] training network...")
model.fit_generator(
trainGen.generator(),
steps_per_epoch=trainGen.numImages // 64,
validation_data=valGen.generator(),
validation_steps=valGen.numImages // 64,
epochs=NUM_EPOCHS,
max_queue_size=64 * 2,
callbacks=callbacks, verbose=1)
The number of epochs we are going to train for is controlled by NUM_EPOCHS defined earlier in
this script. We’ll be linearly decreasing our learning rate from 1e-1 down to zero over the course of
NUM_EPOCHS. And finally serialize the model to disk once training is complete:
96
97
98
# save the network to disk
print("[INFO] serializing network...")
model.save(args["model"])
99
100
101
102
# close the databases
trainGen.close()
valGen.close()
ResNet on Tiny ImageNet: Experiment #2
In this experiment, I trained the ResNet architecture detailed above on the Tiny ImageNet dataset
using the SGD optimizer, a base learning rate of 1e − 1, and a momentum term of 0.9. The learning
rate was decayed linearly over the course of 75 epochs. To perform this experiment, I executed the
following command:
$ python train_decay.py --model output/resnet_tinyimagenet_decay.hdf5 \
--output output
The resulting learning plot can be found in Figure 12.11. Here we can see the dramatic effect
that learning rate decay can have on a training process. While both training loss and validation
loss diverge from each other, it’s not until epoch 60 where the gap widens past previous epochs.
Furthermore, our validation loss continues to decrease as well. At the end of the 75th epoch, I
obtained 58.32% rank-1 accuracy.
I then evaluated the network on the testing set using the following command:
$ python rank_accuracy.py
[INFO] loading model...
[INFO] predicting on test data...
[INFO] rank-1: 58.03%
[INFO] rank-5: 80.46%202
Chapter 12. ResNet
Figure 12.10: Using ResNet we are able to reach the #5 position on the Tiny ImageNet leaderboard,
beating out all other approaches that attempted to train a network from scratch. All results with
lower error than our approach applied fine-tuning/feature extraction.
As the results demonstrated, we have reached 58.03% rank-1 and 80.46% rank-5 accuracy on
the testing set, a substantial improvement over our previous chapter on GoogLeNet. This result
leads to a test error of 1 − 0.5803 = 0.4197, which easily takes position #5 on the Tiny ImageNet
leaderboard (Figure 12.10).
This is the best accuracy obtained on the Tiny ImageNet dataset that does not perform some
form of transfer learning or fine-tuning. Given that the accuracies for positions 1-4 were obtained
from fine-tuning a network that was already trained on the fully ImageNet dataset, it’s hard to
compare a fine-tuned network to one that was trained entirely from scratch. If we (fairly) compare
our network that was trained from scratch to the other networks trained from scratch on the
leaderboard, we can see that our ResNet has obtained the highest accuracy amongst the group.
12.6
Summary
In this chapter, we discussed the ResNet architecture in detail, including the residual module
micro-architecture. The original residual module proposed by He et al. in their 2015 paper, Deep
Residual Learning for Image Recognition [24] has gone through many revisions. Originally the
module consisted of two CONV layers and an identity mapping “shortcut”. In the same paper, it
was found that adding the “bottleneck” sequence of 1 × 1, 3 × 3, and 1 × 1 CONV layers improved
accuracy.
Then, in their 2016 study, Identity Mappings in Deep Residual Networks [33], the pre-
activation residual module was introduced. We call this update “pre-activation” because we apply
the activation and batch normalization before the convolution, going against the “conventional
wisdom” when building Convolutional Neural Networks.
From there, we implemented the ResNet architecture using both bottleneck and pre-activation
using the Keras framework. This implementation was then used to train ResNet on both the
CIFAR-10 and Tiny ImageNet datasets. In CIFAR-10, we were able to replicate the results of
He et al., obtaining 93.58% accuracy. Then, on Tiny ImageNet, reached 58.03% accuracy, the
highest accuracy thus far from a network trained from scratch on Stanford’s cs231n Tiny ImageNet12.6 Summary
203
Figure 12.11: Applying learning rate to ResNet when trained on Tiny ImageNet leads to 58.32%
validation and 58.03% testing accuracy – significantly higher than our previous experiment using
ctrl + c training.
challenge.
Later in the ImageNet Bundle, we’ll investigate ResNet and train it on the complete ImageNet
dataset, once again replicating the work of He et al.Bibliography
[1] Diederik P. Kingma and Jimmy Ba. “Adam: A Method for Stochastic Optimization”. In:
CoRR abs/1412.6980 (2014). URL : http : / / arxiv . org / abs / 1412 . 6980 (cited on
pages 11, 85, 86).
[2] Geoffrey Hinton. Neural Networks for Machine Learning. http://www.cs.toronto.edu/
~tijmen/csc321/slides/lecture_slides_lec6.pdf (cited on pages 11, 85).
[3] Kaggle Team. Kaggle: Dogs vs. Cats. https://www.kaggle.com/c/dogs- vs- cats
(cited on pages 12, 95).
[4] Andrej Karpathy. Tiny ImageNet Challenge. http://cs231n.stanford.edu/project.
html (cited on pages 12, 131, 168).
[5] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. http : / / www .
deeplearningbook.org. MIT Press, 2016 (cited on pages 13, 85, 86).
[6] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. “ImageNet Classification with
Deep Convolutional Neural Networks”. In: Advances in Neural Information Processing
Systems 25. Edited by F. Pereira et al. Curran Associates, Inc., 2012, pages 1097–1105. URL :
http://papers.nips.cc/paper/4824- imagenet- classification- with- deep-
convolutional-neural-networks.pdf (cited on pages 14, 86, 95, 103, 128).
[7]
Yann Lecun et al. “Gradient-based learning applied to document recognition”. In: Proceed-
ings of the IEEE. 1998, pages 2278–2324 (cited on page 14).
[8] Adrian Rosebrock. PyImageSearch Gurus. https://www.pyimagesearch.com/pyimagesearch-
gurus/. 2016 (cited on page 16).
[9] Richard Szeliski. Computer Vision: Algorithms and Applications. 1st. New York, NY,
USA: Springer-Verlag New York, Inc., 2010. ISBN : 1848829345, 9781848829343 (cited on
page 16).206 BIBLIOGRAPHY
[10] Maria-Elena Nilsback and Andrew Zisserman. “A Visual Vocabulary for Flower Classi-
fication.” In: CVPR (2). IEEE Computer Society, 2006, pages 1447–1454. URL : http :
//dblp.uni- trier.de/db/conf/cvpr/cvpr2006- 2.html#NilsbackZ06 (cited on
page 17).
[11] Karen Simonyan and Andrew Zisserman. “Very Deep Convolutional Networks for Large-
Scale Image Recognition”. In: CoRR abs/1409.1556 (2014). URL : http://arxiv.org/
abs/1409.1556 (cited on pages 32, 86, 171).
[12] The HDF Group. Hierarchical data format version 5. http://www.hdfgroup.org/HDF5
(cited on page 33).
[13] Andrew Ng. Machine Learning. https : / / www . coursera . org / learn / machine -
learning (cited on page 44).
[14] Navneet Dalal and Bill Triggs. “Histograms of Oriented Gradients for Human Detection”.
In: Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and
Pattern Recognition (CVPR’05) - Volume 1 - Volume 01. CVPR ’05. Washington, DC, USA:
IEEE Computer Society, 2005, pages 886–893. ISBN : 0-7695-2372-2. DOI : 10.1109/CVPR.
2005.177. URL : http://dx.doi.org/10.1109/CVPR.2005.177 (cited on pages 47,
57).
[15] David G. Lowe. “Object Recognition from Local Scale-Invariant Features”. In: Proceedings
of the International Conference on Computer Vision-Volume 2 - Volume 2. ICCV ’99.
Washington, DC, USA: IEEE Computer Society, 1999, pages 1150–. ISBN : 0-7695-0164-8.
URL : http://dl.acm.org/citation.cfm?id=850924.851523 (cited on pages 47, 57).
[16] T. Ojala, M. Pietikainen, and T. Maenpaa. “Multiresolution gray-scale and rotation invari-
ant texture classification with local binary patterns”. In: Pattern Analysis and Machine
Intelligence, IEEE Transactions on 24.7 (2002), pages 971–987 (cited on pages 47, 57).
[17] Christian Szegedy et al. “Going Deeper with Convolutions”. In: Computer Vision and Pattern
Recognition (CVPR). 2015. URL : http://arxiv.org/abs/1409.4842 (cited on pages 51,
81, 86, 131, 133, 168).
[18] Yoav Freund and Robert E Schapire. “A Decision-Theoretic Generalization of On-Line
Learning and an Application to Boosting”. In: J. Comput. Syst. Sci. 55.1 (Aug. 1997),
pages 119–139. ISSN : 0022-0000. DOI : 10.1006/jcss.1997.1504. URL : http://dx.
doi.org/10.1006/jcss.1997.1504 (cited on page 71).
[19] Leo Breiman. “Random Forests”. In: Mach. Learn. 45.1 (Oct. 2001), pages 5–32. ISSN :
0885-6125. DOI : 10.1023/A:1010933404324. URL : http://dx.doi.org/10.1023/A:
1010933404324 (cited on page 71).
[20] L. Breiman et al. Classification and Regression Trees. Monterey, CA: Wadsworth and Brooks,
1984 (cited on page 71).
[21] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning.
Springer Series in Statistics. New York, NY, USA: Springer New York Inc., 2001 (cited on
page 71).
[22] Cuong Nguyen, Yong Wang, and Ha Nam Nguyen. “Random forest classifier combined
with feature selection for breast cancer diagnosis and prognostic”. In: Journal of Biomedical
Science and Engineering (2013). URL : http://file.scirp.org/Html/6- 9101686_
31887.htm (cited on page 72).BIBLIOGRAPHY
207
[23] Thomas G. Dietterich. “Ensemble Methods in Machine Learning”. In: Proceedings of the
First International Workshop on Multiple Classifier Systems. MCS ’00. London, UK, UK:
Springer-Verlag, 2000, pages 1–15. ISBN : 3-540-67704-6. URL : http://dl.acm.org/
citation.cfm?id=648054.743935 (cited on page 72).
[24] Kaiming He et al. “Deep Residual Learning for Image Recognition”. In: CoRR abs/1512.03385
(2015). URL : http://arxiv.org/abs/1512.03385 (cited on pages 81, 131, 171, 192,
196, 202).
[25] Gao Huang et al. “Snapshot Ensembles: Train 1, get M for free”. In: CoRR abs/1704.00109
(2017). URL : http://arxiv.org/abs/1704.00109 (cited on page 81).
[26] Andrej Karpathy. Neural Networks (Part III). http : / / cs231n . github . io / neural -
networks-3/ (cited on pages 83, 85).
[27] Sebastian Ruder. “An overview of gradient descent optimization algorithms”. In: CoRR
abs/1609.04747 (2016). URL : http://arxiv.org/abs/1609.04747 (cited on page 83).
[28] John Duchi, Elad Hazan, and Yoram Singer. “Adaptive Subgradient Methods for Online
Learning and Stochastic Optimization”. In: J. Mach. Learn. Res. 12 (July 2011), pages 2121–
2159. ISSN : 1532-4435. URL : http : / / dl . acm . org / citation . cfm ? id = 1953048 .
2021068 (cited on page 84).
[29] Matthew D. Zeiler. “ADADELTA: An Adaptive Learning Rate Method”. In: CoRR abs/1212.5701
(2012). URL : http://arxiv.org/abs/1212.5701 (cited on page 84).
[30] Timothy Dozat. Incorporating Nesterov Momentum into Adam. http://cs229.stanford.
edu/proj2015/054_report.pdf (cited on page 86).
[31] Tom Schaul, Ioannis Antonoglou, and David Silver. “Unit Tests for Stochastic Optimization”.
In: CoRR abs/1312.6055 (2013). URL : http://arxiv.org/abs/1312.6055 (cited on
page 86).
[32] Forrest N. Iandola et al. “SqueezeNet: AlexNet-level accuracy with 50x fewer parameters
and <1MB model size”. In: CoRR abs/1602.07360 (2016). URL : http://arxiv.org/abs/
1602.07360 (cited on pages 86, 131).
[33] Kaiming He et al. “Identity Mappings in Deep Residual Networks”. In: CoRR abs/1603.05027
(2016). URL : http://arxiv.org/abs/1603.05027 (cited on pages 86, 172, 174, 202).
[34] Andrew Ng. Nuts and Bolts of Building Applications using Deep Learning. https://nips.
cc/Conferences/2016/Schedule?showEvent=6203. 2016 (cited on pages 89, 91).
[35] Tomasz Malisiewicz. Nuts and Bolts of Building Deep Learning Applications: Ng at
NIPS2016. http : / / www . computervisionblog . com / 2016 / 12 / nuts - and - bolts -
of-building-deep.html (cited on page 89).
[36] Andrej Karpathy. Transfer Learning. http://cs231n.github.io/transfer-learning/
(cited on page 93).
[37] Greg Chu. How to use transfer learning and fine-tuning in Keras and Tensorflow to build an
image recognition system and classify (almost) any object. https://deeplearningsandbox.
com / how - to - use - transfer - learning - and - fine - tuning - in - keras - and -
tensorflow-to-build-an-image-recognition-94b0b02444f2 (cited on page 93).
[38] Adrian Rosebrock. Practical Python and OpenCV + Case Studies. PyImageSearch.com,
2016. URL : https://www.pyimagesearch.com/practical-python-opencv/ (cited
on page 105).
[39] Andrej Karpathy. CS231n: Convolutional Neural Networks for Visual Recognition. http:
//cs231n.stanford.edu/. 2016 (cited on pages 131, 146).208 BIBLIOGRAPHY
[40] Min Lin, Qiang Chen, and Shuicheng Yan. “Network In Network”. In: CoRR abs/1312.4400
(2013). URL : http://arxiv.org/abs/1312.4400 (cited on page 132).
[41] Jost Tobias Springenberg et al. “Striving for Simplicity: The All Convolutional Net”. In:
CoRR abs/1412.6806 (2014). URL : http : / / arxiv . org / abs / 1412 . 6806 (cited on
pages 133, 171).
[42] Chiyuan Zhang et al. “Understanding deep learning requires rethinking generalization”. In:
CoRR abs/1611.03530 (2016). URL : http://arxiv.org/abs/1611.03530 (cited on
page 133).
[43] WordNet. About WordNet. http://wordnet.princeton.edu. 2010 (cited on page 147).
[44] Xavier Glorot and Yoshua Bengio. “Understanding the difficulty of training deep feedforward
neural networks”. In: In Proceedings of the International Conference on Artificial Intelligence
and Statistics (AISTATS’10). Society for Artificial Intelligence and Statistics. 2010 (cited on
page 171).
[45] Kaiming He et al. “Delving Deep into Rectifiers: Surpassing Human-Level Performance on
ImageNet Classification”. In: CoRR abs/1502.01852 (2015). URL : http://arxiv.org/
abs/1502.01852 (cited on page 171).
[46] Kaiming He. Deep Residual Networks. https : / / github . com / KaimingHe / deep -
residual-networks (cited on page 176).
[47] Wei Wu. ResNet. https://github.com/tornadomeet/ResNet (cited on page 176).
[48] Kaiming He. ResNet: Should the convolution layers have biases? https://github.com/
KaimingHe/deep-residual-networks/issues/10#issuecomment-194037195 (cited
on page 177).
[49] Rodrigo Benenson. CIFAR-10: Who is the best in CIFAR-10? http://rodrigob.github.
io / are _ we _ there _ yet / build / classification _ datasets _ results . html #
43494641522d3130 (cited on pages 180, 192).
[50] Theano Community. Theano: Max Recursion Limit. https : / / github . com / Theano /
Theano/issues/689 (cited on page 181).
